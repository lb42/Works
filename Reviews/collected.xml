<?xml version="1.0"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xi="http://www.w3.org/2001/XInclude">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>The Collected Reviews of Slasher Burnard</title>
      </titleStmt>
      <publicationStmt>
        <publisher>Egotistical Solipsisms PLC</publisher>
       </publicationStmt>
      <sourceDesc>
        <p>Various files, and original newsprint, as documented in the bibliography.</p>
      </sourceDesc>
    </fileDesc>
    <revisionDesc>
       <list>
         <item><date>16 feb 2019</date>review for consistency before checkin</item>        
        <item><date>20 mar 2018</date>pretty much complete</item>
        <item><date>4 Apr 98</date>started to compile, added header</item>
      </list>
    </revisionDesc>
  </teiHeader>
  <text>
    <body>
      <div xmlns="http://www.tei-c.org/ns/1.0" xml:id="gimpel">
<head>J. F. Gimpel, <title>Algorithms in SNOBOL4</title>, 
Wiley, 1976, price &#xA3;12.90,487pp.</head>

<p>Books on specific programming languages tend to come in one of two
flavours &#x2014; the exhaustive or the evangelical. At one extreme, we
have the formal language definition which expresses with rigorous
abstraction a syntax and morphology as if so vulgar a thing as
compiler or program were beneath consideration
(<foreign>vide</foreign> Revised Report on Algol 68), and at the other
we have the persuasive primer, demonstrating in embarrassing detail
yet another way of discovering how many beans make five. Fortunately
for those of us who actually have to do the job, there are a few books
with a third flavour all of their own, which one can only call
exemplary, and Gimpel's book is a leader amongst them. It is not
enough to whet the appetite, nor yet hardly to state the facilities of
a language like SNOBOL 4. For this language, perhaps more than others,
a text providing a full range of concrete examples of 'how to do it'
has been long overdue. The topics covered range widely over the
spectrum of (basically) non-numeric computing, embracing with equal
thoroughness compiler writing, pattern matching, sorting, text
formatting, random sentence generation &#x2014; not to mention a fully
interactive poker game simulation.  The diversity of the applications
which Gimpel so effortlessly demonstrates makes a powerful case for
SNOBOL 4 as not just another cumbersome Markov language.  Because it
is probably easier to write bad &#x2014; by which I mean both
inefficient and inelegant &#x2014; code in SNOBOL than in almost any
other language (except FORTRAN), it has often been assumed that
efficient and elegant code is impossible in SNOBOL.  In this very
journal<note n="1" place="foot"><bibl>A. M. Addyman <title>A Language
for Linguistic data Processing - 1</title>. <title level="j">ALLC
Bulletin</title>, 4 (1976), No. 2</bibl></note> it has been stated
that SNOBOL is unsuited to linguistic data processing on grounds of
lack of <q>efficiency, type- checking, and file-handling
facilities</q>. This is not the place to go into the argument in
detail, but probably anyone glancing through this book, certainly
anyone who has actually used the language, will find Addyman's
judgement a little sweeping. To complain of the lack of type-checking,
for example, is simply to misunderstand the nature of the language. As
tor 'efficiency' &#x2014; the use of a language as rich in facilities
as SNOBOL must surely increase it if we include 'person-time' as well
as 'machine- time' in our calculations. Or must every application
redesign such basic elements of linguistic data processing as an
associative table facility, built-in functions to analyse strings at
machine code speeds, and an efficient dynamic storage mechanism? Ihose
attracted to SNOBOL by its inclusion, not just of these but of a whole
range of rich and complex string handling facilities, have often found
themselves floundering because of that very richness. Gimpel's book is
a valuable corrective, inculcating good programming practice by solid
example backed up by cogent explication. The hook is divided into
eighteen sections, each of which contains an informal but thorough
discussion of the relevant points of theory, code for SNOBOL 4
functions to implement them, and a wide range of exercises to test the
reader's understanding. The SNOBOL 4 code has been written with a
conscientiously applied system of naming conventions which allows the
functions to be simply 'plugged in' to existing programs with the
minimum of upheaval. The book is more than simply a crib for the
overworked programmer, however (but certainly not less); one is
encouraged to understand the problem as well as its recommended
solution. Since only a basic knowledge of SNOBOL is assumed, the book
commends itself equally to the expert and to the novice
programmer. Much of the information on the various implementations of
SNOBOL is unavailable elsewhere, and the thoroughness of the two
sections discussing pattern matching is particularly notable. In
Gimpel's phrase, this has always been <q>one of the murkier aspects of
the language</q>, and, if his discussion is at first blush a little
forbidding, it is none the less indispensable. Another useful section
provides efficient code for various common mathematical functions, the
lack of which has often been considered a deficiency in the language,
and the light-hearted programmer will derive much profit and
entertainment from such functions as PHRASE (which delivers a random
insult, or compliment, to enliven execution of any interactive game
simulation) or RSTORY (which produces endless variants on a basic
story syntax). The success of the section on text formatting may be
gauged by the book itself, which was produced in its entirely on the
computer. It says much for Gimpel's honesty, and his faith in his
work, that he has left the occasional wart in the layout unretouched!
No SNOBOL programmer's library is complete without a copy of this
book, and even the most hardened FORTRAN user may learn something from
the algorithmic sections of it.
</p>
<trailer>
L. D. Burnard
University of Oxford Computing Laboratory
</trailer>
</div>
      <div xmlns="http://www.tei-c.org/ns/1.0" xml:id="boyle">
<head>Camp Conflicts </head>
<bibl>Descent of Man by T. Coraghessan Boyle. (Gollancz, 1980)</bibl>
<p>
There is a facile nastiness about many stories in this small collection which
can be intended only hypothetically to outrage. In truth, of course, we are all
far too urbane to be moved by such hackneyed situations as the anthropologist's
wife having it away with an ape, the skies raining first blood and then shit on
n houseful of happy hippies, an astronaut first farting and then masturbating in
zero gravity, to say nothing of the various spiders ("as big as a two egg
omelette") which turn up with surprising regularity within these pages. Faced
with the high (low?) camp of an eating contest described in a sickening amalgam
of American menu-ese and Harry Carpenter at his most poetic, the correct
reaction is all too predictable: my, my, Mr Boyle is indeed a "master of black
humour" and "genially disillusioned about the human condition" &#x2014; it says so,
right here on the blurb. (Even for a blurb writer, even in these godless days,
that juxtaposition of disillusionment and geniality might perhaps have given
some occasion for hesitation, but let it pass). Such stuff is best dealt with in
a rigorously scientific manner, as it is evidently concocted. What oppositions
then are mediated by these tales? Hell, of course, we have those good old bogies
of modern American writing - dirt, blood, vomit etc. versus plastic forks,
deodorants etc (Descent of man, Blood fall, Green Hell, Earth&#x2014;Moon); we have
-curse them for their intractable otherness- Women versus Us Chaps (A women&#x2019;s
restaurant, John Barleycorn Lives, Drowning); we have The Media versus Reality &#x2014;
a very one&#x2014;sided affair with Reality hardly getting a look in (Heart of a
champion, Dada, De rerum natura, Second swimming). In this last category
particularly mock bathos (deflation) slides with ironic frequency into the
genuine article (sinking) so that the response is a genuine groan. The
juxtaposition of General Idi Amin Dada with the posturing of contemporary New
York dadaists might have been funny as a passing thought, but I do not think
anyone now finds General Amin amusing. Equally, The Second Swimming might have
been bearable as a Monty Python one-liner; as it is, a Chairman Mao who plans to
have "wieners with Grey Poupon mustard for breakfast" and exhorts his barber to
"buff the pate" is merely silly. The same sub-Perelman smartness mars an
otherwise exemplary evocation of comic book Norsemen; when their leader sums up
the newly&#x2014;discovered Newfoundland with the crisp and novel aphorism "That
place'll never amount to a hill of beans" this reader at least is not as
convulsed as might have been desired. Stylistically I would place Mr Boyle
amongst the Creative Writing school of modern prose masters. His text
corruscates equally with verbless sentences, flat similes and brand names, set
in a timeless continuum of present indicatives. Two stories deserve exception
from this blanket condemnation however, both (I think) at least partially
intended to parody greater masters of the form. Quetzacoatl Lite (though marred
by the same sort of silliness as Horsemen) recalls Borges or Nabokov in its
gallant attempt at conveying the inner workings of an obsessive collector, while
John Barleycorn is dead is oddly reminiscent of the later Kipling in its hearty
celebration of the great practical joke. By and large however, the closest Mr
Boyle comes to expressing an opinion is in the pseudo-Vonnegutese of The
extinction tales. "What's a species here, a species there? This is where
extinction becomes sublime." Sublime extinction? What is the world coming to?

[3 june]
</p></div>
      <div xmlns="http://www.tei-c.org/ns/1.0" source="#r8008TLS" facs="pages:1980-cochrane.jpg" xml:id="cochrane">
<head>Growing up in Ulster</head>
<byline>By Louis Burnard </byline>
<bibl>IAN COCHRANE: <title>F for Ferg</title> 117 pp.
Gollancz. &#xA3;5.95. 0 575 02862 9 </bibl>
<p> Readers of Ian Cochrane's last novel, Ladybird in a Loony Bin (1978), may be a little
disappointed to find that in this, his latest, he has returned as if obsessivelv to the subject
matter and milieu of his first three successes. Has the man but one tale in him (they may testily
enquire)&#x2014;are we to get nothing from him but the joys and miseries of growing pains in Ulster,
shaded by poverty and madness? A novelist is allowed to retell his unspeakable youth in his first
novel, and maybe even his second, but surely by his fifth he should have found something new to
say? </p>
<p> Even such readers will, however, concede that Mr Cochrane's skill has not changed but rather
strengthened over the years; it is (if possible) more energetic in its apprehension of living
detail, more exact in its mirroring of the cadencies of dialogue, more witty in its unexpected
juxtaposition of tense and mood, more affecting in its presentation of innocence. To capture the
distinctive &#xFB02;avour of a culturally deprived vernacular is no mean feat; to give it a cutting edge
and vitality as Cochrane does is something else again. It is a power which more than makes up for a
certain perfunctoriness in the plot. </p>
<p> Ferg is for Fergus who is new to the village, and the manager's son. As if that were not enough
he wears a red waistcoat with gold buttons and it suspected of being a Norwegian fruit merchant.
Doggedly attempting to integrate himself with the peasantry, he is prone to such remarks as &#x201C;There
is something natural about you. You don&#x2019;t have to think. You use instincts. You&#x2019;re real." This novel
charts with remorseless precision Fergus's painful induction to the youthful sports of chasing
girls, rioting in the dole queue and hanging around street corners. It also charts without
mawkishness the maturing of its narrator, Johnny, who differs from other Cochranesque narrators
chiefly in that he achieves an understanding of compassion for himself rather than learning through
the example of others. There is no deus ex machina comparable to the policeman at the end of
<title>Ladybird in a Loony Bin</title>; there are no painfully nice next-door neighbours as in <title>Gone in the Head</title>;
there is no tower of strength like the teacher in <title>A Streak of Madness</title>. </p>
<p>This gives this novel a seriousness lacking in earlier ones, a seriousness demonstrated also by
the comparative lack of incidental comic detail. Characters such as the village Poet, and incidents
such as the village dance at which Fergus is tricked into undressing himself, are more sparingly
described than might have been anticipated. It is as if the author were deliberately restraining his
imagination, particularly in the novel's closing pages. There is nothing of the solemn social
documentary in it, thank heavens, yet this novel gives a more honest account of that Northern
Ireland we hear of occasionally &#x2014; the one where 15 per cent of the population subsists on the dole &#x2014;
than many others more ostensibly serious or less ostensibly comic. </p>
</div>
      <div xmlns="http://www.tei-c.org/ns/1.0" xml:id="conway">
<head>[Review published in ALLC Bulletin 1980]</head>
<bibl>Richard Conway and James Archer, <title>Programming for Poets: a Gentle
Introduction using FORTRAN with WATFIV</title> (Cambridge, Mass:
Winthrop Publishers, 1978)</bibl>


<p>One has the impression reading this book that the authors
would really rather have written quite a different one. Their
goal, set out explicitly in the preface, is not to help the
reader to write his own programs, but 'to achieve an
understanding of the programming process'. No one would quarrel
with their assertion that 'computing has a special
responsibility to explain itself to the layman', nor with the
good intentions apparent in their attempts to demystify that
omnipresent bogey-thing the computer. Nevertheless,
misconceptions and prejudice are rarely removed by half-truths
and over-simplifications: on the contrary. It is quite true
(even obvious) that computer error should be blamed on the
programmer more often than the machine, that computer held
databanks present opportunities for abuse of civil liberties
unparalleled in history, that problems which cannot be stated
algorithmically (probably) cannot be solved by computation and
that machine translation is not as easy as it looks. Whether
such startling aper&#xE7;us form an adequate basis for a proper
understanding of what computer programming is and where it is
going is more debatable.
</p>
<p>Character manipulation in FORTRAN has been compared to
shelling peas in boxing gloves; a simile which nicely conveys
the frustrations of using the wrong tool for the job, if not
exactly the nature of the difficulties. Apologists for the
language fall into two irreconcileable camps - the
extensionists, who point to the existence of various dialects of
the language more or less well-suited to tasks for which FORTRAN
was never intended, and the classicists, whose argument is
essentially that ANSI FORTRAN is and always will be the nearest
thing we have to a programming lingua franca. Some apologists
(including, I fear, the present authors) try to have a foot in
both camps, which clearly will not do. The language used
throughout this book is a subset of FORTRAN IV, as implemented
by the WATFIV compiler. The merits and demerits of this
particular one apart, it seems somewhat unfair to present it to
the novice as at all typical of the sort of FORTRAN compiler he
is likely to encounter in his local computing centre. Some of
the characteristics of the subset used can be applauded -
explicit declaration of all variables, for example; others seem
eccentric - particularly the relegation of any consideration of
the nature and purpose of subroutines to a page in the appendix,
along with such essential elements of the programming art as how
to operate an IBM 026 keypunch (two pages).</p>
<p>The first section of the book takes the novice carefully through
such essential concepts as variables, loops, conditionals,
substrings, and arrays (here confusingly called lists). Because the goal
here is to explain everything, some distracting details peculiar to
WATFIV (e.g. the representation of quote characters quoted
literals) or to FORTRAN (e.g. the construction of DO loops) are
given prominence, but the basic pedagogic method is sound and
could profitably be used in any introductory programming course. The
remainder of the book, in which the more elaborate programs are
presented, is inevitably more sketchy. By choosing the simplest
algorithm, rather than the most appropriate, they run the risk of
setting budding programmers some very bad examples. The chapters on
concordance programs, statistical programs, and information
retrieval, for example, all use methods which no one would consider
appropriate in the real world. The authors are not unaware of this, and
caution the student to that effect, but this is no excuse for not
having done the job properly to start with. Typical of the
superficiality of this approach is the random number generator
presented without comment on page 196. Disregarding the misprint in the
code (which is guaranteed to blow up the most tolerant of compilers),
the function is presented with no explanation of how it works - in
particular with no warning that it will only work on machines which
store integers as 32-bit words!
</p>
<p>This book contains much that is praiseworthy. It covers a lot of
ground, is understandable and well written. It cannot however be
recommended to anyone who really wants to learn how to program, nor
even to anyone who wants to know what programming really is. The
preface glosses the use of the word 'poets' in the title as arising
from the practice of dedicating 'the least technical
introductory version of a subject "to poets" '. I hope someone more
gifted than I will be inspired to rebuff this slur on the intelligence
of the followers of Apollo.</p>
<trailer>L.D. Burnard
University of Oxford
</trailer></div>
      <div xmlns="http://www.tei-c.org/ns/1.0" source="#r8009TLS" facs="pages:1980-emerson.jpg" xml:id="emerson">
<head>Set authors</head>
<head>By L.D. Burnard</head>
<bibl>SALLY EMERSON :<title> Second Sight</title>240pp. Michael Joseph. &#xA3;6.50. 0 7181 1965 7 </bibl>
<p> Jennifer&#x2019;s mum Sarah, is no better then she should be. Her dad, Edward, tends to wear socks that
don&#x2019;t match. This is because he is a successful author and donnish, while she runs a fashionable
restaurant and is sluttish. They met, we are told twice, at Oxford, where she attended a secretarial
college and he (presumably) the other sort. Such miscegenation can obviously come to no good. The
child of their union. whose transition from O-levels to A-levels this novel documents is a
withdrawn, shy adolescent given to fantasy and equally hopeless at games and at chatting up boys.
Her fantasies centre on <q>dear Rebecca</q>, a somewhat Wordsworthian contemporary killed in a road
accident, on Aphra Behn, the subject of her father&#x2019;s latest book, here treated as a sort of <hi>Ur</hi>-
Germaine Greer, and on that well- known mainstay of female adolescence P. B. Shelley. These three
imagined presences serve Jennifer as faithfully as many lesser children are served by their teddy
bears; at first idolized, then used as touchstones (for self, female, male respectively), they
&#xFB01;nally dwindle in stature to be finally discarded at the appropriate stage in her developing
understanding of Life and Love. </p>
<p> With the aid of the friendly old spiritualist gentleman across the square and her own psychic
powers, Jennifer plans a grand display in which her private trinity will materialize before her
astounded father and demonstrate her power <q>to defeat time and decay and death</q>, but curiously
little of this design is achieved. In the sordid world of grown-ups two sub-plots are hatching&#x2014;one a
murder story, one a love story. Linking the two is a personable young architect called Paul,
apparently the same age as Jennifer but also her mother&#x2019;s lover. Paul has as disastrous an effect on
all the ladies with whom he comes into contact as did the young Shelley, and, not to labour the
analogies&#x2014; which with an author as intelligent as Ms Emerson are particularly demure - is also just
as much of a rotter. This novel is hardly a who-dunnit, but it would spoil what suspense does exist
to reveal either the details of his caddishness or its unexpected&#x2014;indeed barely credible
&#x2014;consequences in the lives of Jennifer and her parents. Suffice it to note the irony illustrated in
the resolution of the novel: that a fantasy fulfilled is no longer a fantasy. </p>
<p> The murder itself is somewhat gratuitous as murders go &#x2014;much discussed but little realised. It
does, however, serve as a means of introducing a pleasantly satirical portrait of a greasily
succcessful barrister and his oppressed wife, the Pumblechook pomposity of his dialogue as neatly
caught as her nervous glove-twitching and obsessive house-hunting. </p>
<p> The novel id set somewhere in fashinoable Westmister where taxis are taken as freely as lovers
and whisky and Chablis flow at dusk and dinner respectively. But more insistent than these details
are those of the city itself: the reality of trees, of historic buildings, of swans in public parks,
in comparison with the flotsam and jetsam of humanity swirling around them. Sally Emerson also has a
fondness for pithy similes: <q>He peered at Jennifer's glass of water with an air of trepidation, like
a car examining a saucer of milk laid down by a stranger.</q> Perhaps because so much of the novel is
seen through Jennifer's sharp gaze, it is unusually rich in observations of colour and costume
(almost obsessively in the case of costume) and of nuances of mood and emotion, particularly those
of adolescence. By comparison the introspections of the other characters seem rather lifeless. The
novel has not anything new to tell us about youth and age, love and selfishness, dreams and reality.
But it is done with such calm authority and delicate wit that one hopes Sally Emerson's next will
display her evident talent at work with more demanding subject matter. </p>
</div>
      <div xmlns="http://www.tei-c.org/ns/1.0" xml:id="farringdon">
    <head>FARRINGDON, MICHAEL AND FARRINGDON, JILL <title>A computer-aided study of the prose style
            of Henry Fielding and its support for his translation of the Military History of Charles
            XII.</title> [In Advances in computer-aided literacy and linguistic research D.E. Ager,
        F.E. Knowles, and Joan Smith (eds). Dept of Modern Languages, Univ of Aston, Birmingham. UK,
        1979, 95 -105. See main entry CR 20. 11 (Nov 1979), 35.349.]</head>
    <p> This modest study of some patterns of usage characteristic of Fielding&#x2018;s prose relies on no
        statistical model more esoteric than common sense and makes no claims to universal
        appllcnability. As such it is a welcome breath of fresh air in the somewhat heated
        atmosphere of statistical stylistics. In his own lifetime Fielding's style was thought
        eccentric enough to he worth parodying. and this study demonstrates that many of the
        features parodied are indeed more frequent in Fielding than in a small group of contemporary
        &#x201C;control" texts. Modern critics have been less successful in identifying the quintessentialy
        Fieldingesque usage when they have relied on impressionistic data rather than on
        computer-generated concordances. The Farringdons identify several phrases or constructions.
        not in themsleves particularly unusual or topic-dependent which are preferred to near
        synonymous phrases consistently more often throughout the Fielding texts than in the
        "control&#x201C; texts. With no underlying statistical model of word-distribution however. they
        make no attempt to assess the significance of these differences. The paper closes with an
        examination of an anonymous translation. attributed on external evidence to Fielding, in
        which the value of these &#x201C;Fielding-usages&#x201C; is further demonstrated. Different phrases in the
        original French are all translated by the same &#x201C;preferred usage." The authors conclude that
        the translation can con&#xFB01;dently be attributed to Fielding. However their argument would
        perhaps be more persuasive it some steps had been taken to compensate their observed
        frequencies for sample-bias. The genuine Fielding texts examined total over three times as
        many words as the "control&#x201C; sample. which must at least partially invalidate anv argument
        based on frequency alone. </p>
    <trailer> L. D. Burnard. Oxford. UK</trailer>
</div>
      <div xmlns="http://www.tei-c.org/ns/1.0" xml:id="hockey">
<head>
At home with the hardware</head>
<byline>By L. D. Burnard</byline>
<p><bibl>
ROBERT L.  OAKMAN:
<title>Computer Methods for Literary
Research</title>
235pp. University of South
Carolina Press. $14.95.
0 87249 381 4
</bibl></p><p><bibl>
SUSAN HOCKEY:
<title>A Guide to Computer Applications in the Humanities</title>
248pp Duckworth. &#xA3;28 (paperback
&#xA3;8.50 ).
0 7156 1315 4 (0 7156 1315 3)
</bibl>
</p>
<p>The first agreeable discovery which the literary scholar makes when
face-to-terminal with the computer of which popular culture makes so
much is just how stupid the brute actually is. If you want it to
search a text for words about colour, you have to tell it not only
that "blue" and "green" are both relevant but also that "blue" and
"Blue" are (probably) the same word, and indeed just what a "word" is.
Its literal-mindedness and its inubility to originate instructions are
perhaps a less agreeable discovery. If told to achieve your ends by
inappropriate means, it is incapable of improving on them. Programming
becomes the art of decking out the mindlessness of this automaton with
the semblance of sanity, a task requiring logic and ingenuity, but
also imagination. However, neither of these two books is directly
concerned with programming though Robert Oakman's does include a
somewhat unreliable chapter on the subject containing much that is
guaranteed to raise a sneer among computer scientists. As Susan Hockey
points out, programming is no longer necessary for the majority of
literary applications, given the existence of easily used utility
programs or packages at most major centres. Perhaps the third
discovery that the novice in the art of literary computing always
makes is that it all has been done before. The reinvention of the
wheel, and its proud presentation at international symposia, is a
familiar event.
</p><p>
	That two authors, both eminent in the field, shouid have
indepedently decided to publish what is substantialy the same book
at the same time is perhaps symptomatic of this. Of course, there are
differences in style and emphasis (and a gross disparity in cost)
between Mrs Hockey's book and Professor Oakman's, but by and large
they both cover the same ground, reach the same conclusions and balk
at the same hurdles. This is perhaps inevitable, because fifteen year
(roughly the period of activity covered by both) though an aeon in the
history of computing, is but the twinkling of an eye in the history
oif literary studies. The ground is a gentle introduction to what
computers are, not quite at the level of "See the Com-put-er. It has
flash-ing lights on" (but perilously near it), and a panoramic survey
of the uses to which they have been put in such fields as
lexicography, indexing, textual information retrieval, and stylistic
analysis. The conclusions are, predictably, that computers are
basically a Good Thing,, and getting better all the time. Tbe hurdles
balked at are, on the one hand, a proper critical perspective from
which to view achievements in this curious hybrid fied and on the
other a proper understanding of the wider areas of computing expertise
to which much of what is reported on here is tangential.
</p>
<p>Mrs Hockey's brisk pragmatism comes perhaps a little closer to
establishing a critical perspective than Professor Oakman's
comparatively discursive and historical narrative. Her book is
addressed more to the sorcerer's apprentice where his seems to be
aimed at a convention of sorcerers. Both include bibliographies (to
which all computer-related studies are much addicted) but Hockey's is
highly selective and closely topic-related where Oakman's, though
well-annotated, is too exhaustive and out of date to benefit tbe
apprentice. A cut-off date of 197G simply will not do for a volume
published in 1980. Hockey also includes useful glossaries of
computer-jargon and acronyms, in which a touch of dry humour is just
perceptible ("System" a general term almost everything connected with
the computer). On more serious matters she remains ruthlessly
pedagogic. When introducing punch cards, for instance, Oakman explains
that the code used for the holes punched in them is named after Herman
Hollerith, whereas Hockey gives us their dimensions and warns us that
different machines use different codes.
</p>
<p>A less revealing difference of emphasis (except perhaps of the
authors' respective eco-systems) is that for Oakman the chief problem
with the use of punch cards is that <q>they often become warped
because of moisture absorption</q> whereas for Hockey it is that
<q>they occupy a lot of room for storage</q>. Both authors cover the
same wellworn ground in their presentation of computer peripherals. It
is surely unnecessary in the 1980s to introduce a device consisting of
a keyboard and an electronic display screen as if it were some arcane
and exotic piece of machinery Of more use and conspicuous by its
absence from both texts, might have been some account of the perils
and privileges of "personal computing", as it is known in the
trade. The computing milieu which both authors assume is that of the
University Computing Centre where a large main frame is shared among
many different users, an assumption which becomes increasingly
unrealistic as computers get cheaper (or as cheap computers get more
powerful). The scholar curious to know whether his business friends'
PET or APLE will be of any use to him, thc Department buying its own
mini, wiII alike look in vain for guidance here. even as to the right
questions to ask. Neither book for instance explains such concepts as
virtual memory or why the fact that one computer is "smaller" than
another does not just mean that it takes up less room.
</p>
<p>These technical preliminaries disposed of, both authors settle down
to surveying the varyingly solid achievements of scholars who have
succumbed to the lure of the computer. On many topics there is little
to choose between them. Both begin by considering the making of
concordances (which has also recently been the subject of a valuable
monograph by Trevor Howard-Hill), both making the oft-repeated,
oft-ignored, plaint that a computer-generated concordance is as crude
or as subtle as its editor makes it. The fact that correct
lemmatization and disambiguation of homographs cannot be be formed
automatically should not absolve the editor from the responsibility of
attempting tbem manually, but neither Hockey nor Oakman (nor indeed
many of those wbo publish concordances today) appear to attach as much
importance to this as they do to detailing the various formats in
which contexts may be printed or to debating earnestly whether or not
high frequency words should be included. Oakman, following Raben,
speculates that the days of the pritited concordance are numbered --
when we all have our computer terminals we will perform ad hoc
inquiries of a computer-held text instead of browsing through fuddy-
duddy old-hat pieces of paper with ink on them. This is a consummation
to be wished for, if the alternative is the unusable mountain of badly
reproduced line printer output of some American university presses
</p>
<p>On the topic of textual editing techniques and collation algorithms
in particular, both our authors appear to be copying from the same
source. This is either simple parochialism or more probably a
reflection of the fact that textual scholars are less willing to admit
to using computers than they might be. If either of the present books
does no more than make such scholars realize that the computer can
actually save them time and trouble, by preserving a version of their
copy text which the cat cannot jump on nor milk be spilt over which is
infinitely flexible and editable and from which A Real Book can be
produced with no more exotic an intermediary than magnetic tape, then
both authors will have accomplished much. Like lexicography (vide such
projects as the Toronto Dictiorary of Old English) textual editing is
an excellent instance of a field where the computer can simplify and
improve the performance of a mindless chore on uhich human
intelligence would be wasted.
</p>
<p>Computer applications in the fields of stylistic and morphological
analysis are perhaps of more dubious value. Apart from a polite nod at
the end of one of Mrs Hockey's chapters the work of computational
linguists in narrowing the gap between natural and artificial
intelligence has been largely ignored by workers in the fields covered
by these books. Consequently "stylistic" analysis remains in practice
morphological only, despite sporadic outbursts in the literature
against the "New Criticism" mentality which computer-based stylistics
embody. 1n nearly all the work covered here (though again Mrs Hockey
makes a gesture in the direction of recognizing the value of the
collocation analyses which characterize some recent French work), the
mere counting of words, syntactic or phonetic items is regarded as an
adequate means of characterizing style. The evidence that (say) the
distribution of high frequency vocabulary tokens has a unique
relationship with some particular author remains debatable.  Such
evidence as exists is inevitably statistical and (humanists being even
more likely to be misled by statistics than by computers) Mrs Hockey
devotes the bulk of her chapter on this topic to a brisk introduction
to such mysteries as the arithmetic mean, the standard deviation and
the chi square test. She does not, in my view, lay enough stress on
the fact that even real statisticians are unable to agree on such
fundamentals as the nature of the frequency distribution of vocabulary
items within a text, which invalidates well over half of the studies
reported each year. One feels happier with mechanical analyses of
style which attempt only to quantify observed or suspected
characteristics of vocabulary though (as Oakman points out) one hardly
needs a computer to distinguish between romance and novel. Even here
there is a danger (typified in the uncritical way Oakman discusses the
uses made of the General Enquirer package of forgetting that the act
of criticism is about far more than the words on the page, which are
all that the computer has been used to analyse.
</p>
<p>To the vast majority of computer users the topics discussed so far
will appear extremely esoteric. When the terms "information retrieval"
and "database" crop up, however, they might be forgiven for thinking
they were on familiar ground. After all, computing literature proper
has been discussing little else for the past few years.  Alas, very
little of the discussion seems to have been noticed by literary
scholars. To do her justice, Mrs Hockey has heard of database
management systems, and even mentions one of the more successful as
being possibly useful. Nevertheless, like Professor Oakman, she still
clearly thinks of data as being held and processed within a computer
as if it were organized in large filing cabinets, through which
efficient electronic nymphs riffle to retrieve punched cards one at a
time. Without wishing to belittle such achievements as Schneider's
London Stage databank, the fact of the matter is that computing
technologoy has now moved beyond such a self-image. Literary computing
will not come of age until it recognizes this fact and adapts the new
tools of data analysis and data modelling to its own ends.
</p></div>
      <div xmlns="http://www.tei-c.org/ns/1.0" xml:id="tucker">
<head>Allen B. Tucker Jr,<title> Text Processing: Algorithms, Languages and
Applications</title>. (Academic Press, 1979; ISBN 0 12 702550 2. Price
[9.40).</head>

<p>As students of the humanities succumb to the lure of the computer
with increasing frequency, so the perennial chestnut 'Which
programming language is best suited to the job?' becomes ancillary to
the more fundamental 'What shall we teach the rising
generation?'. There is a growing tendency made manifest in the
proliferation of books like this one, to provide an answer in the
unlovely shape of 'Computer Appreciation' courses for humanities
students. Such courses inevitably do much good in familiarizing
students with the machine and its ways of 'thought', but also much
harm in that they generally teach just enough to get the student to
the point of being capable of making really significant blunders, and
then abandon him or her.  One should (perhaps) drink deep or taste not
the cybernetic spring. These dangers are particularly acute when what
is being purveyed are so-called 'state-of-the-art' techniques or
systems, such as text processing. Computer systems, like atomic
particles, have a bad habit of mutating whilst one is appreciating
them. Nevertheless, just as Literary Appreciation is frequently a
compulsory soft option for hard-boiled science students, so we may
expect Computer Appreciation to perform a symmetrical role for
humanities students. The need for a sound introductory text book for
such courses is evident, and the present volume is not the first to
try to meet it. Neither, I fear, is it the first to fail dismally in
the attempt. It covers very well-trodden ground in a perfunctory
manner and is chiefly distinguished by one of the worst bibliographies
I have ever seen on the subject.
</p>
<p>There is an introductory chapter on the basic concepts of the
algorithm (erroneously presented here as an essentially sequential
concept), a brief account of the sort of hardware likely to have
been encountered at a University computing centre four or five
years ago and its limitations, and an even briefer account of one,
rather eccentric, method of encoding text &#x2014; which is not made
any less eccentric by misprints in the examples. Chapters 2 and 3,
introducing the languages PL/1 and SNOBOL respectively, form the
heart of the book. Neither chapter adds anything to the author's
earlier and better treatment of them and of other matters in his
Programming Languages (McGraw Hill, 1977), apart from a
proliferation of prints a_nd syntactic solecisms, so great as to
render the book occasionally intelligible and invariably
illiterate. I remain unconvinced of the pedagogic soundness of
teaching programming as if the language used were an integral
part of task; students of the humanities in particular are well
accustomed to the 'compare and contrast' discipline and should
have correspondingly little difficulty learning about two or
more languages in parallel. Such a treatment would also ensure
that &#x2014; say &#x2014; the curiosities of PL/l's input/output system
(which take up a disproportionate amount of Chapter 3) would not
be seen by the student as an inescapable hurdle on the programming
path. Such concepts as the nature of a table or a subprogram
might actually become clearer if the different ways in which they
were realized were presented in parallel.
</p>
<p>As a PL/1 ignoramus I cannot say that the chapter on this
language did anything reduce my prejudice against the language,
while the SNOBOL chapter includes no mention of the TABLE
function, surely more important in SNOBOL than the ARRAY function,
no mention of the effect of the anchor in pattern matching, and
no discussion at all of user-defined datatypes &#x2014; surely at least
as important as PL/1's SQL-like record-structuring capabilities.
The SNOBOL chapter is also particularly rife with misprints in the
examples, a heinous crime in any text book. Angle brackets are
used for parentheses so consistently as to cast doubts on Mr
Tucker's knowledge of his subject; doubts reinforced by such
barbarisms as the following, which occurs as a model answer to one
of the exercises:
<eg>
NOUN DUPL(NOTANY('AEIOU'),2) RPOS(0) = NOUN 'ES'
</eg>
Spacing, which is notoriously crucial in SNOBOL code, is haphazard
in the examples throughout. A prize specimen is the example on
page 99, where 16 lines of SNOBOL code contain four different
syntactic errors.
</p>
<p>Chapter 4 is called an 'Overview of Text Processing Packages and
Applications'; the words 'currently available on the IBM 370 at
Georgetown University' should be added. Four packages are described
&#x2014; a primitive concordance generator called KWIC (so primitive it
considers the hyphen to be a word), the FAMULUS information retrieval
package, the CMS Editor, and a text formatter called SCRIPT. (I would
not class any Editor as a text processing package myself, but rather
as part of any reasonably-sized computer's operating system, but this
may be my own eccentricity.) The parochialism of this selection
further appears in the astounding error which Mr Tucker perpetrates at
least four times during the course of the book, (I quote) <q>this is
true of most current text processing packages; none is so widely used
that it has been implemented and supported on more than one computer
species.</q> What about COCOA (a package of which mirabile dictu Mr
Tucker knows nothing)? What about ROFF (another one, and surely a
better-known text formatter than SCRIPT)? What about FAMULUS itself,
which has to my knowledge been available on CDC 7600 and 6600, DEC 10,
ICL 1900 and 2900, as well as IBM 370 for at least the last three
years?
</p>
<p>But enough. It is embarrasingly clear from the bibliography
and final chapter of the book that its author has only a
perfunctory knowledge of his subject; enough perhaps for the
humanities students of Georgetown University, Washington. But
neither he nor they can expect to make much progress in the
field without at least nodding acquaintance with what the rest
of the world has been up to for the last few years.
</p><trailer>L.D. Burnard
Oxford University Computing Service
</trailer>
</div>
      <div xmlns="http://www.tei-c.org/ns/1.0" xml:id="urquhart">
<head>Trauchle in the but and ben </head>
<bibl>A diver in China seas by Fred Urquhart. (Quartet, 1980, #6.50)</bibl>
<p>
Fred Urquhart has produced four novels and countless short stories since 1936,
as well as having edited such volumes as Great True Escape Stories and a cartoon
biography of Winston Churchill. His tales, of which this is the latest volume,
have been broadcast on what I suspect he would call the wireless and translated
into several European languages. The former achievement, if not the latter,
seems peculiarly apposite for a writer so irretrievably wedded as he to the use
of Scots dialect. As a mere sassenach, I must confess that I find a trifle
indigestible such sentences as "Harry was unemployed at the time and there
wasn&#x2019;t room in our pokey wee but and ban for both him and me" or "Elspeth
McDOugall was finding life a bit of a trauchle" when encountered in cold print,
which indigestion is further compounded in this volume by an occasional confusion of
narrative voices. About half of the stories are told in the first person, where
dialect seems appropriate, but of the remainder only one or two employ a neutral
narrator. &#x2019;A camp follower&#x2019; for example is only sporadically told through the
eyes of its heroine, a luckless Scots lassie trammelled up in the Napoleonic
wars. Its diction wobbles uncertainly between Mrs Oliphant ("They would comfort
each other for youth&#x2019;s eternal longing and loneliness on the banks of springy
turf beneath Salisbury Crags") and Robbie Burns ("Willem was a big sonsy
fine-looking lad ... He was a likeable cratur&#x2019;, cheery and gallus"), before
plumping finally for Georgette Heyer with the arrival on the scene of one
Francisco Goya, just in time to save our heroine from the garotte. ("Even though
he&#x2019;d painted her without a stitch on, Senor Goya had aye behaved like a perfect
gentleman"). This sort of purple-streaked tartan is quite rare in the other
tales, I am glad to say, but there remain several in which a carefully
established dialect narrator fights for control of the tale with a bland
characterless one. The narrator of &#x2019;Like arrows in the hands of God&#x2019; for example
might initially be taken for one of the gossips at Mrs Rintoul&#x2019;s, but is also
allowed to be privy to Jessie&#x2019;s mind.
</p><p>
Several of the stories turn, like this one, on the relation between mistress and
maidservant and are concerned with that peculiarly concentrated spitefulness of
which elderly ladies are often both victim and agent. The main characters are
nearly all women of a certain age - Urquhart&#x2019;s men are invariably typecast as
bullying fathers, sentimental lovers or conceited boys - and the narratives
nearly all chronicle a remorseless decline. A favourite theme is a modern
variant on the Ubi sunt which might be called the Generation Gap Game.
&#x2019;Nostalgia for a Waltz Dream&#x2019;, for example, begins with a slanging match between
Mrs Nance McQueen and her daughter brought on by rock and roll, and then shifts
by way of flashback (a favourite Urquhart device) to slanging matches between
Nance and her father brought on by silent movies. Back in the present, Nance
muses "If only Jean would be a sensible lassie and give up all this rockin&#x2019; and
rollin&#x2019; dirt and go to the pictures instead". But the opposition is unbalanced,
for the tale ends with what the narrator describes as "the moaning of a glottal-stopped moron rendering
a rock &#x2019;n&#x2019; roll number". This partisanship spoils what is for me (perhaps
because I like rock and roll and silent movies) its point - the structurally
identical role the two play in the parent-child relation. But what should bring
Nance and her daughter together, Urquhart prefers to make separate them. A much
slighter tale, &#x2019;Auld Mother Claus&#x2019; similarily turns on the mutual dislike of
grandmother and grandchildren, but is more successful in its dialogue, while the
three generations in &#x2019;Dusty Springtime&#x2019; are juxtaposed in a more profitable way,
with the grand daughter&#x2019;s prim disapproval of her grandmother&#x2019;s eye for the
"laddies", one of whom happens to be the mother&#x2019;s current "beau".
</p><p>
Urquhart&#x2019;s real interest seems to be the passing of time, not surprisingly in an
octogenarian. "Now that I feel death&#x2019;s cauld hand crawling over me every night I
keep turning my mind to the corpses I&#x2019;ve seen in my day" the narrator of the
title story in this collection begins; there is a dour delight evident
throughout in the detailing of the little miseries of bourgeois family life as
its participants gradually decline from insistant respectability to unspeakable
senility. There are no Joycean epiphanies here. Instead life is a long process
of grim survival. Thus in the longest tale, &#x2019;Princess McDonald&#x2019;, the
inconsequential details - the hours and wages of the Princess&#x2019;s occupations, her
favourite songs and music hall stars, her fur coats- are the only relief in her
drab descent. Having married a Canadian she spends a long life moving restlessly
between Canada and Scotland, becoming that most pitiable of beings, a tourist.
The tale has a sureness of diction and a wealth of circumstantial detail that
carry conviction, if not enthusiasm; equally impressive, though in a different
way, is the first story in this collection &#x2019;Pilgrimages to the old Manse&#x2019;. This
I take to be mildly satirical of those notorious literary ladies of Auld Reekie,
whose tones are exactly mirrored in the narrator&#x2019;s. "My poor husband, who
practically never opens a book, except a third rate adventure story, and who
thinks Raymond Chandler is high brow". The poor husband is also a Baronet while
she is a "jumped-up wee keelie who&#x2019;d got too big for her boots". The idolised
author to whose Manse the pilgrimages are made, turns out to have been "a silly
woman without a thought in her head" and her admired works the product of her
maid servant. In a sense, I suppose, that is vindication enough for the use of
dialect throughout the book.

</p></div>
      <div xmlns="http://www.tei-c.org/ns/1.0" xml:id="malachi">
<head>
<title>Proceedings of the International Conference on
Literary and Linguistic Computing, Israel,</title> Ed. Zvi Malachi,
Tel Aviv, 1979.
</head>
<byline>By L.D. Burnard</byline>

<p>For one week at the end of April 1979 the Katz
Institute of the University of Tel Aviv was host to a special
conference organized under the aegis of the
ALLC. There were 38 participants, half of them from
Israel, with smaller delegations from the Netherlands, the
U.K. and the U.S.A. together with a handful of onlookers from
the rest of Europe and one solitary Australian. 48 papers
were presented, two-thirds of these being reproduced in the
present volume. The academic receptions, cocktail parties,
sight-seeing tours and other customary junketings are also
noted therein, which is perhaps not inappropriate. For the
function of this volume (I hope its editors will forgive me
for dwelling on the point) is more to recall an occasion than
to contribute overmuch to the state of knowledge. More
souvenir than symposium, it resembles nothing so much as the
contents of a particularily zealous delegate's briefcase on
his return home. Here is Professor X's abstract, promising
the philosopher's stone but not delivering it for lack of
space &#x2014; rather like Fermat's last theorem; here is Professor
Y's sheaf of smudgy printout, demonstrating his newfound
skill as a programmer; here is Professor Z's sales brochure
for some new piece of exotic computer software. Above all
here is evidence that the on-going preoccupations of the
University of Laputa's Special Research Section on Computer
Aided What Not are still going on, after a decade or more of
faithful attendance at international symposia.
</p>
<p>In the circumstances it is not surprising that it should be the
home team which fields the strongest side in this particular
international encounter. Few would disagree that the production of the
Historical Dictionary of the Hebrow Language is one of the most
ambitious projects of modern lexicographical scholarship,
computer-aided or otherwise. The sheer bulk of materials it is to
describe &#x2014; over two millennia of linguistic production &#x2014;
is awe-inspiring enough in it self, without the added complications
inherent in the Hebrew language. As Professor Ben-Hayyim, the
President of the Academy of the Hebrew Language, remarks in his
opening address, the project has had to find its own way towards
solving methodological problems "to which no solution had previously
been sought". This is not of course to deny the value of earlier
dictionaries (most notably Ben Yehuda's Thesaurus totius Hebraitatis)
but simply to demonstrate how rapidly a difference of quantity fades
into a difference of quality. Regrettably, not all of the Isracli
contributions describing work in this field
are here reproduced; particularly conspicuous by their
absence are any account of the Responsa Retrieval Project
(which has however been described in a recent report in
<title>Computers and the Humanities</title>) or of the Complete Yiddish
Dictionary Project. Of those which are reprinted, however,
three papers, those by Yeivin (on the methods used in pre-
editing ancient texts for lexiocographical analysis), Adler
(on the implementation of an online Hebrow thesaurus) and
Busharia (on methods of disambiguating unpointed Hebrew), are
particularily worthy of attention by anyone interested in the
problem of exactly how such tasks may be performed. It is also worth noting
hat the method evolved by Busharia and his co-workers for
disambiguating Hebrew has much in comon with the methods
used by the Norwegians for lemmatizing Ibsen; whether the same
method was used for lemmatizing the Belgian mediaeval Latin
corpus described in this volume by Tombeur is less clear. Of
course the method is hardly novel (it has been in use for ten
years or more) but it has the real merit of being over 9O%
accurate, unlike that presented in another paper on
lemmatisation by Oldberg and Choueka. They take a more
'information-theoretic' approach, presenting some highly
sophisticated algorithms and measures of performance based on
four rather dubious axioms concerning the relation between
wordform ("word") and wordroot ("meaning"), with satisfyingly
disappointing results.
</p>
<p>There is a great variety amongst the other contributions
to this volume, reflecting no doubt the diversity of membership
enjoyed by the ALLC. Thus we find cheek-by-jowl one article
enthusiastically describing how paper tape and Snobol may be used
to create Russian printout and another in which are ~unded the
pros and cons of a state-based representation of adverbial
modifers in an expanded sematic network. Such eclecticism,
however admirable runs the serious risk of becoming an uncritical acceptance
of any p-baked (O &lt; p &lt; 1) idea or proin which counting,
logical formalisms or algorithmic methods are involved. Selection is invidious but two
extreme examples of the type which I have in mind may be
cited: one by on the counting of suffixes as a test for ncol

	s in Rabelais and one by the perhaps bettern Melcuk on
the "Meaning (=) Text" Model

	of language. Tuttle is merely incomprehensible (a
prize of 5Op is hereby offered for the best explication of
the term "non-parallelicity of non-trivial total graph" which
appears throughout his article) while Melcuk transcends
comprehensibility in what I cannot resist dubbing a TBG
(Tantalizingly Brief Glimpse) of the new SemRs (Semantic
Representations) possible within "LMs (linguistic Models) of the Meaning (=) Text Type (MT models or MTMs)".
</p>
<p>Cercone and Slaby both report on projects already
described in the proceedings of the previous ALLC Conference
(<title>Advances in Computer Aided Literary and Linguistic Rescarch</title>,
ed. Ager, Knowles and Smith; Aston, 1979); Slaby's article is
more or less identical in the two publications, and Cercone's
differs only in the stress given different aspects of the Al
system described. The figures are better reproduced in the
Israeli version, but the bibliography is missing, which is a
pity, as it is a useful source of authoritative information
about expanded semantic nets. Using another well-beloved
formalism, Sallis attempts to represent formal argumentative
prose by a context-free grammar. This attempt must have
delighted Boot (whose paper here proposes such grammars as
the new lingua franca of text analysis software) as much as
it depresses this reviewer. A context-free argument is, I
contend, no argument at all but an algorithm; irony has no
place in such an analysis and is yet one of the most powerful
tools of argumentative writing (see Milton's prose,
passim). Marinov's paper seems to be SHRDLU revisited, but is
well written and clearly argued enough to demonstrate the
limitations of any system in which procedural knowledge
cannot be acquired but must be represented.
</p>
<p>Of more practical interest are two papers which
attempt to bridge the gap between data analysis and query
processor: King's evaluation of three well-known database
systems in terms of their using (or not using) the semantic 
information held within the database structure, which
opens up a promising line of rescarch; and Krause's account
of USL, a query-processing system with a powerful linguistic
component. As is often the case, however, more effort seems
to have gone into evaluating the system than designing it.
Tapper proposes another type of approach to the general
problem of rendering accessible the information held in those
glant databases of which the media have such horror: the use of
citations rather than keyword as primary index to legal
databases has much to attract the busy barrister but whether
it is a technique of general applicability is more debatable.
However, the popularity of such reference works as
ScienceCitationAbstracts perhaps indicates that my doubts are
ill-founded.
</p>
<p>Several papers deal with that branch of analysis which
has acquired the name of stylometry, after the assidnous
promulgation of the term by Morton and his followers. This
technique, as Bailey's magisterial recension of Kenny's work
on Aristotle correctly points out, remains a science in
search of a theory. We still have no adequate theoretical
model of linguistic behavior, and the process by which
variables are selected to make one up inevitably seems
somewhat arbitrary. Greatly needed is some method by which
the seemingly ad hoc collection of variables available for
statistical analysis of text can be clustered, discriminated
and filtered. In this connection, two papers, one by Weil and
another by Shore and Radday, touch upon the use of factor
analysis, the latter rather more thoroughly than the former.
Both papers deal with attempts to find quantitative evidence
to support the seemingly intuitive and qualitative groupings
of the books of the Bible which have been inherited from
generations of biblical scholars, but neither can be said to
be entirely successful.
</p>
<p>Shore &amp; Radday's case is perhaps more clearly argued,
as is their method, but neither paper tackles the fundamental
applicability of techniques which have been evolved for
parametric statistics to statistics such as word frequency.
It may be claimed that the relative frequencies of some
syntactic markers or of high-frequency function words are
(within the limits of experimental error) 'interval scaled',
but with no theoretical underpinning to explain why this
should be, the sceptic will remain unconverted. In
particular, he will assert that the difference between a word
frequency of O and a word frequency of 1 is not the same as
that between frequencies of say 1O and 11, and that therefore
word frequency is not strictly speaking a parametric
statistic at all. Hamesse's paper on an attempt to identify
an author's 'preferred language' independent of topos, by
inspection of the relative frequencies of non-function words,
is even more suspect. 'Least squares' analyses, where such
tests as Student's t are used to establish degrees
of significance, are worthless when applied to
variables which are not normally distributed. Whatever else
it may be, the distribution of lexical tokens in the works of
St. Bonaventura (or of anyone else yet analyzed),
particularly those of high semantic content, is not normal.
</p>
<p>The volume also contains a number of contributions
which might be dignified by the title of semisemiotic
analysis. Phelan on Chaucer's language is novel, if somewhat
too enthusiastic and inaccurate for my taste. Weiss on the
word "throne" in a short story by the Hebrew writer Agnon is
more restrained in his language, but is still evidently
reeling from culture shock. Bender on Conrad's vocabulary
promises to substantiate one of the commonplaces of Conrad
criticism, but does not deliver. This is a pity, because the
proper study on vocabulary and in particular of vocabulary
collocations should be an important adjunct to more purely
'literary' criticism. A recent work by Lakoff and Johnson
proposes metaphor as a fundamental part of perception; we
should expect to find its analysis at the heart of literary
studies instead of, on the present showing, its lunatic
fringes.
</p>
<p>The volume has been assembled with commendable speed
and a faithfulness to its authors' original typescripts,
which this reviewer perversely refuses to commend. Leaving
aside the somewhat pointless presence of a number of one-page
abstracts, the volume crawls with elementary typing errors
and linguistic solecisms which I believe it is the duty of an
editor tactfully to correct. Of course such corrections would
have delayed the publication of the work; they would also
completely preclude a comparative study of the typing or
linguistic skills of the distinguished contributors or a
survey of the levels of secretarial technology at different
universities. I count these small losses set against the
number of times in this volume that a fairly murky argument
is rendered utterly opaque by a simple error of transmission,
an inverted page, or reference to a missing footnote.
</p><trailer>
Lou Burnard is a consultant at Oxford University
Computing Service, where his chief responsibility is database
management. He read English at Balliol Collegc, Oxford and
has worked in computing since 1973.
</trailer>
</div>
      <div xmlns="http://www.tei-c.org/ns/1.0" xml:id="schafer">
<head>
Jurgen Schafer, <title>Documentation in the OED. Shakespeare and Nashe as
Test Cases</title> (Oxford: Clarendon Press, 1980).
</head>
<p>There is a deceptively modest air about this book which should not be
allowed to detract from its significance. Its chief aim is to
determine the reliability of the Oxford English Dictionary (hereafter
OED) as a description of the history of the English language. Its
method is to examine the factors causing the evident disparity between
the vocabularies of Shakespeare and Nashe as recorded in the OED and
as derived from more modern sources (Spevack's concordance in the case
of Shakespeare, McKerrow's edition in the case of Nashe). An attempt
is made to estimate the degree of error present in the dating of
lemmata in the OED and also the variation in the type of error
present. In this task, Professor Schafer sets an example which
scholars in quite different fields would do well to emulate, both in
the thoroughness of his analysis and the pellucid and undaunting
manner in which the numerical side of his case is presented.
</p>
<p>Rather more than half of the book is taken up with lists of
antedatings, (that is, words found in works searched by the OED's
editors which are dated earlier than the date assigned the words in
the published dictionary) found by Schafer's assiduous research team
in the works of Shakespeare, Nashe, Malory, and Wyatt. It is not clear
to me quite what audience these lists are intended for; of far more
use to the reader of the remainder of the book are the summary
statistical tables which precede them. The whole force of Schafer's
argument is, after all, that quantification of variability is more
significant than its specification.
</p>
<p>This study attempts to detect patterns of variability and to
postulate causes for them rather than simply to cite endless
variations (as is too often the tendency of historical
linguists). Thus, having established a measure of'authorial
reliability' (the ratio between the number of first usages recorded
for a given author in the OED and the sum of that number and the
number of antedatings for the same author), Schafer is careful to
discuss such related matters as the degree of antedating involved, the
alphabetic distribution of antedatings, their corpus frequency or even
their syntactic category. It seems that 30 percent of the QED's
lemmata are 50 or more years misdated, while 7 percent &#x2014; a mere 16,000
words &#x2014; are over a century out. It is fascinating to discover that over
two-thirds of these antedatings occur in the first half of the
Dictionary, reflecting the gradual improvements in techniques and
tools available during the course of its production. Schafer has no
explanation however for the OED's editors' apparent tendency to
overlook participial adjectives other than what in a pleasing phrase
he calls the 'morphological inconspicuousness' of such words.
</p>
<p>Most significant is the relationship between the reliability rate
and the size of the corpus available for searching.  The early editors
of the OED appear to have had a conscious policy of extracting every
word from major authors, but being more selective from minor
writers. It seems that the common wisdom, which attributes an
extraordinary linguistic creativity to the Shakespearian period, is at
least partly a consequence of the lemmatisation policy of the OED's
first editors, itself to some extent conditioned by the greater
availability of texts from that period.
</p>
<p>The purpose of this book is not of course to claim that
diachronic analyses based directly on the OED are worthless.  Rather
it sets out to establish sampling techniques and other methods of
analysis by which that great treasure house can be used more
effectively, to identify its limitations and correct its
deficiencies. As Schafer concludes, despite recent advances in
computer-based lexicography, <q>in the area of historical documentation
we can expect nothing in the foreseeable future as comprehensive or as
accurate</q>.
</p>
<trailer>L.D. Burnard
Oxford University Computing Service
</trailer></div>
      <div xmlns="http://www.tei-c.org/ns/1.0" facs="pages:1983-burnham.jpg" xml:id="burnham" source="#r8311TLS">
<head> Into the dreaded database </head>
<byline> L. D. Burnard </byline>
<bibl>DAVID BURNHAM The Rise of the Computer State: A Chilling Account of the Computer&#x2018;s Threat to
Society 273pp. Weidenfeld and Nicolson. &#xA3;10.95. 0 207 7x30: 5</bibl>
<p> Every time an American policeman makes an arrest, the luckless offender's details are
immortalized in the FBI&#x2018;s distributed database. Every time an American citizen makes a telephone
call, uses a credit card or even (soon) transfers money from one bank account to another. a new
record is added to some other database. Every time he or she becomes embroiled with a government
agency, be it the Inland Revenue Service, the Social Security Administration, the Department of
Labor or whoever, a database somewhere records the fact. Should this upset us? </p>
<p> Bureaucracies have always collected data obsessively. and no doubt always will. But the coming
of the computer has changed both the scope of the data collected and the ease with which they may be
processed almost beyond recognition. In Dickens&#x2018;s Circumlocution Office the grand principle was
<q>How not to do it</q>; now that magnetic tape and disc replace red tape and paper. satirists begin
to fear that it may in fact be done all too easily. The computer sold to us as a liberator from
toil. may become the instrument of oppression. giving the prying administrators and officious busy
bodies of the Welfare State rather more access to our private affairs than we anticipated. </p>
<p> But this threat needs either more careful analysis or more effective presentation than David
Burnham gives it here if it is to be perceived as a real threat and not just as the pro- duct of a
liberal paranoia. Even so. the book is a gold mine of anecdotal information guaranteed to break the
ice at Civil Rights gatherings. A company called TRW, for example, now offers a service to thousands
of American credit companies whereby, in return for contribut- ing records of their own customers&#x2018;
transactions, clients can check those of all TRW&#x2018;s other clients This seems a neat way of ensuring
that a bad debt incurred in Illinois can't disappear when its perpetrator moves to Arkan- sas, but
it is less impressive if the Illinois records are inaccurate or not up to date. Accord- ing to
Burnham. TRW receive annually over 350,000 complaints about the accuracy of the information they
supply; only a third of these complaints ever lead to a change in the database largely because of
the expense of validat- ing the huge amounts of data involved. Even the criminal records
administered by the FBI have a similar margin of error. for similar reasons. So inaccurate are they
in fact that apparently the police no longer use them - instead the dubious data are sold to
prospective employers checking up on candidates for interview. </p>
<p> Burnham, an investigative journalist who won his spurs uncovering police corruption for the
<title>New York Times</title>, is quick to stress the possibilities such databases offer for misuse.
He is particularly concerned about the surveillance activities of America&#x2018;s National Security
Agency. and with good cause now that executive orders from President Reagan have apparently extended
the purview of this institution to include the whole American people. Ever since the Enigma machine,
computers and military intelligence have existed in an uneasy symbiosis, the development of the one
both determined by and dependent upon the needs and capabilities of the other. The NSA apparently
uses computers on a grander scale than any other single institution on earth. One indication of the
range of its operations is the quantity of waste paper it produces. According to Burnham, anyone
hoping to sell the NSA a suitably secure shredder ten years ago had to be able to cope with over
thirty-six tons of classified waste every working day. </p>
<p> Aside from anecdote. the book attempts to discuss something called <q>Values</q>. It may or may
not be the case that the widespread use of systems analysis is detrimental to proper health care; it
certainly is the case that there is a parallel between a society's culture (its way of looking at
itself) and its technology (its way of looking at what is not itself) though this is not quite what
Burnham has in mind when he bemoans the narrowing of possibilities, the heart- lessness of machines
and what he considers the indecency of modelling human activity in mechanistic terms. There is
nothing here about the bene&#xFB01;ts that accrue from the availability of information to a benevolent and
skilled administration; perhaps. after Watergate, Amer- icans no longer believe in benevolent
Administrations. There is nothing here (apart from a token gesture to the importance of epidemiology
in identifying the causes of cancer) about the uses of computer databases in research. There is
nothing on computer fraud. which probably poses a far greater threat to society than any amount of
NSA cloak-and-dagger stuff. Equally, the book underplays the extent to which the computer is
becoming demythologized. </p>
<p> Burnham is at his best when sticking to the facts, and when, for example. he is charting the
tangled path by which this or that piece of legislation has succeeded or failed on Capitol Hill. his
prose has a grim fascination all of its own. We may not like it. but it rings true in a way that his
imaginative passages (mercifully confined to one acutely embarrassing chapter called <q>A
Future</q>) do not. </p>
</div>
      <div xmlns="http://www.tei-c.org/ns/1.0" xml:id="johansson">
<head>
<title>Computer corpora in English language research</title> 
ed. Stig Johansson. Bergen, 1982
</head>
 
<p>It is twenty years and more since the U.S. Office of Education first
funded the production of a standard corpus of American English "for
use with digital computers" (what a quaint ring that phrase now
has!). If to nothing else, this volume testifies to the continuing
value of that pioneering effort, both in its own right and as a model
and an inspiration to other projects, many of which would be barely
possible without it. The nine papers, mostly from a symposium held in
Bergen in 1981, here reprinted contain a great deal of importance to
anyone interested in the mechanical analysis of English 'performance',
not least some excellent bibliographies, but for this reviewer the
meat of the volume is contained in the four articles concerned with
methods of automatically parsing or tagging free text in a corpus.
The potatoes are represented by three very different papers on the
various applications Brown has found for itself in Amsterdam and in
Goteborg, while gravy is present in the shape of some more general
remarks by Professors Sinclair and Francis which begin the volume.
</p>
<p>The addition of syntactic or functional markers of some sort to any
computer-readable corpus immensely extends its usefulnes as a record
of language in use, provided that the markers employed are derived
from some consistent and objectively-verifiable linguistic model. In
this context, the work of the TOSCA project at Nijmegen (described
here by Aarts and van der Heuvel) deserves to be better known. This
project uses the mechanism of an extended affix grammar (not entirely
dissimilar to the augmented transition network popular in much AI work
of the last decade) as a means of detecting syntactic structure in
surface forms. The method has some far-reaching implications, notably
in the flexibility with which it may be adapted for different
grammars. At entirely the other end of the methodological spectrum are
the two papers on the tagging of the Lancaster-Oslo-Bergen corpus, one
from Garside and Leech at Lancaster, and the other by Johansson and
Jahr (Oslo).  The authors of these two papers are to be commended for
the clarity with which they review the somewhat murky workings of
Greene and Rubin's original TAGGIT program (avatar of so many other
so-called automatic parsers), expand on its weaknesses and detail the
improvements made to it.
</p>
<p>It is perhaps a pity that the editors could not find space to complement
the admirably full account of the suffix analysis part of their system
with more details of its 3000 'context frame rules'.
</p>
<p>The third project described here is the tagging of the London-Lund
corpus of Spoken English, described here by Svartvik and Eeg-Olofsson.
Here a pragmatic approach involving a dynamic lexicon and much
interaction between the linguist and a database of phrase rules has
proved the only practicable method of tagging word classes and
subequently identifying syntactic structures. A distinctive and
unusual feature of their analysis is the use of the tone unit as the
basic unit of analysis rather than the sentence which seems perhaps
unsurprisingly to make the parsing process rather simpler.
</p>
<p>The great virtue of corpus linguistics is surely that it enables
the linguist to make generalisations about 'normal' usage with some
degree of confidence. Consequently it is not surprising to find the
Brown Corpus being used increasingly in situations where linguists
have felt a little uneasy about relying on the intuitions of native
speakers or on the experience of other practitioners, notably in
teaching English as a foreign language. Typical of such studies is
that by Kjellmer (Goteborg) on the use of collocations, while Meijs
(Amsterdam) presents a more computer-oriented account of the
facilities available to linguists engaged in such research. Van der
Steen describes the inner workings of a query processor developed to
assist Dr Meijs and his colleagues in a paper which, while of obvious
interest to anyone wishing to write such a processor, is of rather
marginal use to computational linguists.
</p>
<p>This volume, together with the invaluable ICAME News, is a welcome
demonstration of the continued vitality of corpus linguistics in an
age where linguists seem to be becoming obsessed with paralinguistic
social nuances and literary scholars are too busily engaged in
deconstructing texts to relate them to their linguistic contexts.
</p></div>
      <div xmlns="http://www.tei-c.org/ns/1.0" xml:id="large">
<head><title>The devil's circuitry</title>
TLS <date>14 Dec 1984 </date>,p
1444 </head>
<bibl>Peter Large <title>The Micro Revolution Revisited </title> 0-8476-7361-8; </bibl>
<bibl>Michael Shallis <title>The Silicon Idol: the micro revolution and its social
implications</title> 0-19-215877-5</bibl>  
           
<p>In the last 25 years the computer has been transformed from a mysterious
piece of scientific apparatus attended by serious men in white coats
into a suitable gift for precocious eight year olds.  It has achieved
the status of a consumer durable, becoming a mark of social standing
without which no decent home is complete.  Unlike the washing machine or
the home video however it has also completely transformed the working
environment and even (for those who still have any) the nature of work
itself.  Its indiscriminate reorganisation of every aspect of economic
activity has not come about overnight, nor yet is it complete, but the
rate of change is increasing.
</p>
<p>For Peter Large who, as technology correspondent for the Guardian must
be -as it were- in touch with the leading edge, the home computer is
only the most conspicuous part of a grand transformation scene in which
the rewiring of the UK (the massive replacement of our communications
systems which all you punters in British Telecom have just funded), the
spread of automation, the growth of corporate databases and the
development of the cashless society all have starring roles.  It is not
just manufacturing industry which is presented with the simple choice
"automate or liquidate"; non-manufacturing industry is now undergoing
even greater turmoil, as word processors drain typing pools, databases
relocate filing clerks and expert systems render consultants redundant.
</p>
<p>Although the impetus behind these changes is primarily the same old
commercial imperative to do more of the same better and faster, their
combined effect, it is asserted, will together prove to be qualititative
rather than quantitative.  We are poised on the brink of a third
industrial revolution (if revolutions have brinks), in which the old
dialectics of labour and capital, of productivity and consumerism will
lose their significance.  A new utopianism, or an old despair, grips
apparently sane commentators as their imaginations attempt to grapple
with the microchip: it will bring about the end of civilisation as we
know it; it will bring about a land of milk and honey produced by small
collectives in which no one has to work unless they want to; it will
create a new disaffected peasantry only controllable by means of an
inhuman totalitarianism.  This rash of prediction is not a new
phenomenon; Marinetti and the futurists were making similarily wide of
the mark apocalyptic prophecies for technology before the Great War -
which proved to be a far more comprehensive agent of social change than
the motor car.
</p>
<p>Perhaps it is the headiness of little knowledge which encourages the
wilder extrapolations of the futurologists.  Sober reflection gives no
reason to suppose that the indisputably amazing advances in the
development of the micro processor will necessarily be parallelled by
teequivalent advances in its supporting technology.  Most computers still
communicate with us by means of an expensive and inefficient luminous
screen and a keyboard which was designed over a century ago to be
ergonomically inefficient, so that the "typewriter" (as its operator was
then called) could not go too fast for the machine's delicate innards.
More user-friendly methods of communication are still a long way off,
despite the current availability (at a price) of 'pucks', 'mice' and
touch-sensitive screens.  Speech recognition systems, into which vast
amounts of research money are currently being poured, may prove to be as
elusive in this decade as automatic translation of natural language was
in the last, and for similar reasons.  An allied object for scepticism,
at least amongst the currently computer literate, is the blithe
assumption that computer literacy can, or even should, be acquired as
simply and as easily as the ordinary sort.
</p>
<p>These two reservations do not appear to have occurred to either Michael
Shallis or Peter Large; their books have little else in common.  Large's
analysis is clearly aimed at a receptive audience which is like him,
decently agnostic, mildly satirical, technologically aware and sensibly
concerned about social issues.  Shallis, who teaches at Oxford's
Department of External Studies, is rather less sure of his audience.
The defensive note struck in his preface ("any critic of technology is
liable to be labelled a Luddite and I would not be surprised if the term
was used about myself") recurs.  The Luddites however were driven to
their futile activities by technological changes that affected their
lives directly and immediately; Mr Shallis appears to be motivated more
by an altruistic desire to save the rest of us from spiritual decay,
having already purified himself by banishing television, washing machine
and even electric toaster from his own home.
</p>
<p>There is more than a hint of the lay preacher in several of his
strictures: "To reduce intelligence to mere reason is ...  degrading and
unwholesome", "The computer was born with 'bad blood'", "Information
technology ...  is the invention of the devil". Not surprisingly, his
greatest scorn is reserved for those loonies of the artificial
intelligentsia who persist in finding (or modelling) fairies at the
bottom of the research lab: "It seems to me that before computers have
religious or ethical codes built into them, the computer scientists
might adopt some of their own to direct their purposes in a suitably
moral way."
</p>
<p>Ironically, Mr Shallis appears to earn his living by teaching people how
to use this diabolical device; indeed, doing it rather well, to judge by
the two chapters of straightforward technical explication included here.
Rather less satisfactory is his fondness for Large Truths (for example
that the history of technology is one of increasing abstraction from
reality) which tend to obscure as much as they illuminate.  The bogeyman
of scientific Reductionism takes a terrible pasting in these pages, as
does the principle of scientific neutrality and the idea that the
"so-called Protestant work ethic" (a favourite phrase) can be replaced
overnight.  For Shallis, because the telephone offers only a disembodied
voice, it can provide only an unsatisfactory surrogate for human
communication; systems which impose only such methods of "interfacing"
people are therefore "imperialistic".  Whether a messenger bearing a
cleft stick would be more acceptable is not clear. His doubts about
large computer systems are not without value: they do indeed malfunction
and do indeed require proper controls and careful validation; this does
not however make them intrinsically useless or evil.  
</p>
<p>Despite an engaging dottiness (there are apparently "many cases of
people's psychological state adversely affecting the performance of a
computer system") , despite the patent meretriciousness of some of his
targets (the remainder being straw men or lost causes), despite even his
earnestness, Shallis remains curiously unpersuasive, perhaps more
because of his wooden prose than his puritanism. 
</p>
<p>By contrast, Large, as a professional journalist is an adept of the new
technology.  His book is a composite of short paragraphs, pithy sayings,
useful statistics, non-technical explanation, occasional anecdote,
trends to note, memorabilia, objects of grave concern.  Its words were
clearly not composed but processed.  No important new development is
left un- summarised, no cause for concern deprived of equal time.  It is
bang up to date; the present volume being only the first of what will
presumably continue to be regular revisions.  And yet reading it as a
book is a remarkably unsatisfying activity.  It is meant to be "blipped"
&#x2014; Toffler's expressive coinage for what will become the characteristic
mode of human information processing once Shallis' nightmares have
become reality.  </p>
</div>
      <div xmlns="http://www.tei-c.org/ns/1.0" xml:id="turkle"><head>
Sherry Turkle <title>The second self: computers and the human 
spirit</title> (Simon &amp; Schuster, 1984, 0 671 46848 0)
</head>
<p>Just as the 'typewriter' was once its operator rather than a machine, 
so a 'computer' was once a person skilled in computation, rather than 
the mechanical or electronic contrivance used for that purpose.  The 
semantic shift whereby agency itself was transferred from the person 
to the person's tools perhaps typifies the unease evident in this and 
other books which attempt to assess roles assigned to the computer in 
society. For Professor Turkle, such assessments should be made with 
great caution. The characteristic role she identifies for the machine 
is as a "constructed object", a "Rorschach, a projective screen for 
other concerns". It presents us with a "second self", an electronic 
mirror by which we may determine exactly what we are about. Children, 
according to this thesis, when exposed to 'Speak and spell' or 
similar toys use them as a catspaw for the great question of childish 
metaphysics "What makes a thing alive?". Students, similarly, use 
computers primarily as tools in the great question of adolescent 
metaphysics "What am I?"
</p>
<p>Turkle identifies three paradigmatic ways of "relating to computers": 
the metaphysical ("Is this thing alive?"), the masterful ("How can I 
conquer this thing?") and the identifying ("This thing is like me"). 
She asserts, repeatedly, that the existence of these three modes of 
response is both an important discovery  in  its own right and also 
valuable as a diagnostic aid: an individual's ratings on this 
triple-pronged scale  provide a window into his or her personality 
and "cognitive style" (whatever that might be).
</p>
<p>Her book certainly demonstrates the usefulness of these three modes 
in analysing the individual cases she describes, though their 
generality is hardly thereby demonstrated. One might  anticipate 
these three modes of reaction to almost any stimulus on the part of 
children; perhaps computers differ from, say, rabbits, dolls or 
bicycles only in the complexities of obtaining (and maintaining) the 
stimulus in the first place. Children also wonder whether stuffed 
toys are alive, after all; they often use their toys to define a 
position in society and even identify with them in ways hard to 
distinguish from those which Turkle claims to be characteristic of 
their response to the machine. Equally, one can only speculate as to 
the extent to which her division of children into "hard" and "soft" 
masters on the basis of their programming styles would retain its 
validity in a learning environment where the declarative and 
non-procedural methods of logic programming had prevailed over the 
obscure functionalism of Logo and Basic.
</p>
<p>The obsessiveness with which afficianados play arcade style games is 
not perhaps quite the same as the obsessiveness of the collector of 
railway engine numbers, but while the first obsession seems 
qualitatively different from the second, it is hard to see in what 
exactly that difference consists; certainly Professor Turkle does not 
attempt to explain it. A lengthy methodological appendix to the book 
provides one possible explanation for this. As in her previous 
<hi>Psychoanalytic politics</hi>, Turkle sees herself as being  "like 
the anthropologist who lives in an isolated village in a far-off 
place". Her interpretations are presented with the patina of 
neutrality and objectivity, but this does not make them any the less 
subjective. Unlike the ethnologist however, Turkle's method of 
proceeding wilfully ignores the background knowledge of the cultural 
context within which her subjects live and move, a background 
knowledge which she (unlike "real" ethnologists such as Margaret 
Mead) cannot escape sharing with them. For one well-educated 
middle-class white American academic to regard the <hi>zeitgeist</hi> of 
another as totally alien, because it is expressed in cybernetic terms 
rather than (say) theological ones, is merely silly.
</p>
<p>This is a shame, given Turkle's evident skill and insight when 
pinpointing the beliefs and fears underlying individual responses to 
the machine. Substantial chunks of transcripted interview make up a 
significant part of the book, but these are orchestrated by skilful 
and (from my lay viewpoint) penetrating psychological analysis. What 
is lacking is any understanding of the broader cultural context 
within which these individual reactions are articulated. This is 
particularly evident when she turns to adult computer users.
</p>
<p>The section in which she describes the "first generation" of home 
computer users fails to convince precisely because of its failure to 
point out those attributes which these early computer "buffs" shared 
with other hobbyists; the following section on MIT hackers similarly 
fails because of its apparent lack of understanding for what it is 
that distinguishes computer science students from computing 
professionals on the one hand and what it is that they share with 
other students on the other.
</p>
<p>With the best will in the world, no one could describe the Computer 
Science Department of the Massachussetts Institute of Technology, 
august and inspiring though that institution indubitably is, as 
typical of the community of computer users, without doing gross 
violence to the normal sense of the words "typical" or "community". 
The ethnology of the "hacker" which Turkle appears to regard as 
typical of the computer culture really differs from the ethnology of 
the "beatnik" or the "hippy" only in such minor details as the type 
of narcotics preferred. As one reads her rather solemn description of 
MIT initiation rites and the mystique of the Hack (as revealed in an 
acutely embarassing parody of <hi>Star Wars</hi> once circulated over 
ARPANET), the figure of the anthropological fieldworker  earnestly 
noting ingeniously provided misinformation comes dangerously close to 
the surface of the subtext.
</p>
<p>There is an interesting book to be written about the way in which the 
sixties' obsession with Tolkienish fantasy was transmuted into sword, 
sorcery, dungeons and dragons, but regrettably this is not it. That 
same unwritten book might go a long way to explaining why the 
characteristic mode of imaginative expression for the dedicated 
computer user is science fiction fantasy, in which all the richly 
awkward rugosities of human personality are ironed out into the 
crudely fantastical. It is to Professor Turkle's credit that she 
notes the frame of mind in which so limited a view of artistic 
activity obtains, and relates it to the characteristic rule-oriented 
frame of reference currently needed for computer programming. As an 
exposition of the <hi>gestalt</hi> within which the computer user 
operates it is however less than satisfactory.
</p>
<p>Conspicuously not interviewed by Turkle is any representative of the 
burgeoning class of young upwardly-mobile professional computer 
users: the tribes of analysts and programmers and data management 
consultants who are actually (to mix some rather painful metaphors) 
in touch with the leading edge of the state of the art. This seems a 
curious omission, the rectification of which would have gone a long 
way to balancing the one-sidedness of this book. Concerning itself as 
it does exclusively with people for whom the computer is essentially 
a toy rather than a means of livelihood, it necessarily contain many 
rather shallow responses, made to bear more significance than they 
warrant. By contrast, a book such as Tracy Kidder's <title>Soul of a new 
machine</title> which concerns itself with events of real significance 
within the computing community, is consequently a much more effective 
nailing-down of the ethnology of Silicon Valley than any amount of 
scholary survey and interview.
</p>
<p>For her next book, I would recommend Professor Turkle to examine the 
opinions of some arbitrary collection of people who have never used 
computers. These opinions will almost certainly conform to one of 
three potent paradigms, represented mythologically in the tales of 
Prometheus, Pygmalion and Frankenstein respectively. According to the 
Promethean paradigm, the computer extends our intellectual faculties 
just as technology extends our physical capabilities. It makes us as 
gods, paradoxically without impugning our humanity. On the contrary, 
our mastery of the computer is seen as the apotheosis of what it is 
to be human. This school of thought finds depressingly few adherents, 
except perhaps in IBM. Far more seductive, in these Godless days, is 
the Pygmalion paradigm, according to which the computer is a type of 
perfection to which mere bundles of neurones and amino acids can 
never hope to aspire. This is of course merely the inverse of the 
Frankenstein view of the case, in which the wretched machine is 
everything that humanity is not and its ultimate triumph a denial of 
every human aspiration.
</p>
<p>Like the present book, this one would have an enviable 
unassailibility. As a panoramic record of the various sorts of 
nonsense people might use to delude themselves with, its objectivity 
will be unimpeachable and its descriptive validity self evident. 
Whether or not such a book tells us anything useful about society as 
a whole will also very much depend on the representativeness of the 
punters whose views have been canvassed.
</p>
 
<trailer> 
Lou Burnard, Oxford University Computing Service
</trailer></div>
      <!--
! 2 july
 
 
Dear Mike
 
There follows the first of my two book reviews for University Computing.
I hope it is neither too late nor too savage. The second will follow
within a week or so.
 
Best wishes,
Lou


--><div xmlns="http://www.teic.org/ns/1.0" xml:id="avison">
<!--Published in <title>University Computing</title> 8, p. 159 <idno
                  type="issn">0265-4385</idno><date>1986</date> -->
<head>
D.E. Avison <title>Information Systems Development: a database approach</title>
</head>
 
<p>This text provides an introduction to basic concepts in information
systems design. It is clearly aimed at students of business studies or
similarly non-technical courses where a nodding acquaintance with such
topics as SSADM, 3NF or 4GLs is deemed necessary. It might also be
useful in a fairly low-powered or introductory computer science course.
The heart of the book is an orthodox and competent presentation of data
analysis methods using both Entity-Relationship modelling and a
simplified version of Codd's relational calculus. Other chapters cover
related topics such as 'business analysis', logical schema design
according to hierarchic, network or relational dogma, and some rather
skimpy descriptions of how these are actually implemented by IMS, IDMS,
dB2, ADABAS et al. All of this is adequate for an introductory text, but
in no way superior to the more thorough expositions already available in
standard text books (e.g. Date, Robinson).   
</p>
<p>The book attempts a more mobile target in its discussion of current
fashions in structured systems analysis methodologies. This being a
field in which jargon is everything, much of the discussion is little
more than padding to surround explanations of such phrases as  <term>Integrated
Programming Support Environment</term>, <term>analyst's workbench</term>,
<term>report generator</term>,  <term>structured walk through</term>, 
<term>prototyping</term>,  <term>functional description</term>,
<term>stepwise refinement</term>, <term>dataflow diagrams</term> etc. 
A keen student might get as good an understanding of most of these
simply by reading the computer trade press for a week or two.
Conspicuous by its absence in the chapter on systems design is any
mention of Jackson structuring techniques: surely a more reliable (and
more productive) systems design tool than  <term>structured english</term>
which gets several references.   </p>
<p>
Superficial chapters on data storage techniques and on Data Dictionary
systems together with an astonishingly inadequate chapter on micro-based
DBMS, distributed databases and database machines are also included:
apparently with little purpose other than to bring the book up to the
required length. On database machines, Avison's description of ICL's
CAFS is simply wrong, while to describe the IBM System 38 as a database
machine because it has 'micro-coded database instructions' (unspecified)
seems a little dubious. Distributed database systems get four pages (two
of which are pictures) and micro dbms five. A text book should either
discuss these issues properly or remain silent on them. 
</p>
<p>  The book is well produced and illustrated and its basic content is
sound. There is also a good bibliography, which students might be better
advised to choose from than to read the book itself. Avison writes  with
a contempt for the English language which should not go unremarked,
particularly in a book aimed at the educational market.   <q>Like most
texts, however,</q>  he remarks
<q>IMS will be equated with the database system</q> (he means <q>As
do most texts, mine will treat IMS as a database system</q>  &#x2014; I
think). Or again,  <q>However there are severe limitations to retain
flexibility unless the database is not very complex.</q>  Publishers
surely have a duty to protect the public from sentences such as these,
even if their authors have abnegated it.   
</p>
</div>
      <div xmlns="http://www.tei-c.org/ns/1.0" source="#r8607TLS" xml:id="campbell" facs="pages:1986-campbell.jpg"> 
<head>More and more addressable</head>
<byline>Lou Burnard</byline>
<bibl>Duncan Campbell and Steve Connor: <title>On the Record. Surveillance, Computers and Privacy - The
Inside Story.</title> 348pp. Michael Joseph. Paperback. &#xA3;7.95 0719125762
</bibl><!--, published in the TLS, 25 July 1986, p. 813--> 
<p>If ever there was a ridiculous mouse born of a mountain it is the
Data  Protection Act, which finally became law in July 1984 after nearly
a quarter century of prevarication in Whitehall, although what is rather
 grandly termed the <soCalled>appointed day</soCalled> for implementation of all of its 
provisions is not due until next winter. It has taken Parliament one
departmental committee report, one Royal Commission, two draft bills,
three Private Members' bills, three White Papers and numerous official
consultations to reach the point of enacting principles close to the
letter if not the spirit of such contentious pronouncements as Article 8
of the European Convention on Human Rights. Indeed, it is widely
believed that only the signing of the Council of Europe's <title>Convention
for the Protection of Individuals with regard to Automatic Processing of
Data</title> in 1981 finally forced Whitehall's hand; a fact perhaps
worth putting into the balance the next time that Strasbourg's
Eurocracies are being belaboured in the public prints.   
</p>
<p>That the present government, not notorious for its concern for
liberal causes, may go down in history as the one under which the
concept of a right to privacy of information was finally enshrined in
legislation is almost as pleasing an irony as the fact that Mrs
Thatcher's administration was forced to espouse this wettest of causes
by simple commercial pressures. As the 1982 White Paper in which the
current legislation was first proposed makes clear, its primary purpose
is to protect the United Kingdom's trading interests in Europe, which
might well be compromised by the inability to transfer information
across European frontiers consequent on a failure to ratify the
Convention.   
</p>
<p>Whatever its parentage, perhaps the kindest judgement on the Act
with which we now have to live is that at least it means well. However
honourable its intentions, its weaknesses are rapidly becoming
notorious. Its provisions cover only data held in machine-readable form:
one simple way of evading the need to register a database you wish to
keep secret is thus simply to print it all out, delete the online copy
and invest in a good filing cabinet. The mechanism by which
<soCalled>subject access</soCalled> (that is, access to one's own
records) is to be implemented is vague in the extreme; there is no right
to have inaccurate data corrected or removed, even where action is taken
through the courts.  The powers of the Data Protection Registry itself
are scarcely less well defined: violations of the Act must be stumbled
upon to be detected, and what will happen when these are brought before
the courts is anyone's guess. So far the chief sufferers from the Act
appear to have been agencies such as Citizens' Advice Bureaux,
reportedly unable to cope with the extra work of registering all their
files of innocuous but protected information by the deadline earlier
this month.    
</p>
<p>The Act has also been much criticised for the number and variety of
exemptions from its provisions. An unconditional exemption applies to
data <q>required to be exempt for the purpose of safeguarding national
security.</q> Subject access is denied to a whole range of data,
including police or taxation records as might be predicted, but also
those of other statutory bodies more or less at the whim of the
Secretary of State, many of which are precisely those government
departments and agencies whose information gathering activities give all
upright citizens most cause for concern: - the security forces, the
Inland Revenue, the DHSS, the Customs and Immigration authorities, the
television and vehicle licensing authorities. But perhaps the most
alarming aspect of this <q>load of holes joined together</q> (as the
BMA is said to have called the bill) is that certain bodies, themselves
exempt from the Act's  regulatory provisions, are at liberty to force
other (regulated) data holders to disclose information for
non-registered purposes, without anyone being any the wiser. There is
thus a sense in which this Act requires all sheep to be ready to report
promptly to their local wolf.  
</p>
<p>This book is more than a detailed critique of the Data Protection
Act however. It documents, dispassionately and with every appearance of
accuracy, the sheer scope and intricacy of the vast accumulations of
information currently at the disposal of our masters, and makes some
informed speculations about their probable future. A web of databanks
lies concealed behind such unlovely acronyms as LOP, NICS, NUBS, JUVOS,
OCTA, COP, CODA, DCI, OPCS, IVAN, INDECS, MIRIAM, PNC  and CEDRIC; in
describing these and other systems, Campbell and Connor also provide
evidence of the extent to which their integration  into a gargantuan
apparatus of surveillance and control may soon be upon us. One does not
need to subscribe to any form of conspiracy theory to see a disturbing
inevitability in the process; it is a natural consequence of the
availability of better technology, in retrospect an almost Darwinian
process. The problems of integrating the disparate data files of a
commercial organisation is one which exercised the best brains of the
computing industry during the early sixties; the database concept, in
which a computer system deliberately aims to model the entire
information structure of an organisation, is the fruit of that exercise.
Within a commercial context, it is obviously appropriate. Whether the
state's information needs should be modelled in exactly the same way as
those of General Motors is more dubious.   
</p>
<p>Throughout the administrative infrastructure clerks and filing
cabinets have already been replaced by online terminals and
sophisticated database systems; what is recorded and for what purpose
necessarily change more slowly. It would be the height of naivety to
expect concern for the rights of the individual amongst the
administrators and guardians of our society if those rights are
threatened by a process which they perceive as simply helping them to do
their job better. On the contrary, we can expect whatever currently
characterises their mode of operation to be intensified. In whatever
sphere it is applied, the computer invariably brings the ability to do
more of the same thing, and faster; very rarely does it bring about a
radical reshaping of the enterprise to which it is applied.  
</p>
<p>Consequently, we should not be surprised to find the DHSS readier to
apply their new computers to the task of identifying  <soCalled>scroungers</soCalled>
than that of identifying those to whom benefits are due, though both are
technically equally feasible. Neither should we be surprised to find
that the Police National Computer (PNC), originally intended to hold
details of missing cars and persons, is also used as a repository for
suppositious gossip, circumstantial evidence  and speculation: (the
technical term is <term>intelligence</term>). What after all would we 
expect the Inland Revenue to do with a magnificent invention like ICL's
Content Addressable File Store (CAFS) but apply it to the task of
finding out people's National Insurance numbers, given only parts of
their names and addresses, the better to collate information from
sources (such as banks) which cannot specify a NINO with those (such as
their own records) which must?  Once the Home Office's list of
undesirable immigrants has grown to such a length as to prevent its
being memorized by even the most vigilant of immigration officers the
only effective way of policing our frontiers will be to introduce
machine readable passports (due in 1987) and use a computer (it is
called INDECS) to finger those flagged in the Home Office's database.  
</p>
<p>Moreover, once such systems are in place, their expansion seems
inevitable. Once INDECS is operational, the additional cost of logging
every entrance and every exit of any suitably interesting subject is
trivial. Once  the Inland Revenue has identified your National Insurance
number, (the closest thing currently to a Universal Personal Indicator
(UPI)), it would cost very little more to recover your employment
records, your health records, details of the area in which you live, and
of all the telephone calls you made to Dublin last month. Given that
such information will be held in machine readable form somewhere, given
that it is easier to keep it than to throw it away, given both the
tremendous increases in ease of communication between otherwise
unrelated computers, the technical means of implementing the omniscient
computer of science (and other) fiction are already present. The
database systems of the sixties already resemble the saurian manual
systems they replaced in their inflexibility and cumbersomeness. The
advent of really efficient free text searching systems and of deductive
or "expert" rule-based systems at the front end of the next generation
of computer systems will be complemented by developments such as CD-
ROM, which allows several gigabytes of data to be stored in a compact
disc accessible from a desk-top computer. Before technophoria overwhelms
us all, we would do well to remember that however morally neutral
informational technology itself may be, its applications (like those of
any other technology) need constant policing.</p>
<p>    Campbell and Connor, as good crusading journalists, are not
concerned with philosophical issues. The nature of technology is not on
their agenda, simply its effects and its applications. Their book is
primarily a pre-emptive register of data banks, including many which are
unlikely to come within the purview of the Data Registrar himself. Its
secondary purpose is to make the case against all such information
monoliths, a case that needs to be re-stated, perhaps a little more
explicitly for the benefit of those well-heeled citizens who feel they
have "nothing to hide". The case does not rest on anecdotes, though
there are plenty of these to be found, but on a recognition of the fact
that errors of judgement and observation, harmless in themselves, can
become lethal when given the spurious authenticity of a computer. The
authors also do the Data Protection Act the signal honour of taking it
seriously, itemising those few areas in which it may have some
significant effect on safeguarding civil liberties.   
</p>
<p>Despite its catchpenny title, this book is neither anecdotal nor
journalistically sensational. It is, as they say, solidly researched.
With its aid, the next time that you "come to notice" of the Police,
perhaps by reporting a crime, or by parking your car round the corner
from some political demonstration, you will at least know under what
category you are likely to achieve immortality, and in what company you
will be registered. 
</p>
</div>
      <!--
oucs friday
 
Dear Gordon
 
  Thank you for giving me the chance to tone down the following review
rather than bowdlerising it yourself!  Just to put the record
straight, I am fully aware of Coppen's status and reputation; my
reference to breakfast was not intended to be derogatory but simply to
point to the way in which such conferences yoke together the oddest of
bedfellows. However, as this point clearly failed to come across, I
have tried to do better below. Thank you for putting me right on the
Nebraskans: they certainly read more like recently fledged BAs than
Senior Lecturers. I have taken no notice of your attempt to change my
punctuation: the double quotes indicate words quoted from the original
I note you dont mind me being rude about Messrs Susskind & Strange, so
have continued so to be. I beg to differ as to whether ICON is ICON or
Icon: you as editor have the privilege of the last word on that as on
the rest.
 
ever yours
 
 
  
 
p.p. "Slasher" Burnard
 
 
--><div xmlns="http://www.tei-c.org/ns/1.0" xml:id="johnson"><head> <title>ICEBOL 85 (Proceedings of the 1985
International Conference on English Language and Literature
Applications of SNOBOL and SPITBOL</title>, edited by Eric Johnson, Dakota
State College) </head>
 
<p>This trim volume contains 11 papers revealing an extraordinary range
both in computing expertise and in application area. Of the more
widely interesting papers, it should be noted that one is not about
SNOBOL and another is not about English.  In his preface, Eric Johnson
assures us that he at least has "never attended a conference where the
conversations were so animated and engrossing". While warmly welcoming
the advent of this new international forum, this reviewer is tempted
to ascribe at least some of its animation to sheer mutual
astonishment. What common ground can exist between say Peter-Arno
Coppen (Catholic University of Nijmegen) and Wayne Tosh (St Cloud
State University)?  Coppen describes GRASP, a parser-generator for
context-free grammars which happens to be written in SNOBOL. Tosh's
describes a number of undergraduate exercises which also happen to be
written in SNOBOL. Both authors regard SNOBOL as an appropriate
teaching vehicle, but the disparity between what is being taught is so
extreme as to cast doubts as to the existence of any point of contact
other than the accidental identity of the chosen language.
</p>
<p>It is equally hard to imagine any common ground between Ronald
Susskind of the Antares Corporation in Minnesota, and William
C. Strange of the University of Oregon's English department. Without
undue disrespect, Mr Susskind's style leaves me gasping for breath,
but as I understand it, the tenor of his argument is that the English
language itself can be interpreted as if it contained only logical
statements, provided the notion of what constitues logic is "slightly"
extended (it could also be said to consist of purple anxieties,
similar licence permitted, but let that pass).  Mr Strange, writing at
a more even speed, contributes some good old fashioned practical
criticism of three concrete or, in Strange's phrase, "computable"
poems, leading him to the extraordinary and unsubstantiated assertion
that "Poems and programs are surprisingly alike".
</p>
<p>Surely more typical of SNOBOL's current user population are two
workmanlike contributions about real life applications of the
language, one from Paul Bantzer (Startext GmbH, Bonn) about its
usefulness in a photo-typesetting application, the other from Timothy
Montler (North Texas SU) describing a whole suite of programs
developed to assist in the linguistic analysis of a Northwest American
Indian language corpus. The latter is particularly impressive, and
deserves to be better known.  The book begins, as is only appropriate,
with two contributions from Ralph Griswold, one, tantalisingly short,
on the history of the language (which reveals that it was originally
called SEXI for String Expression Interpreter) and the other
describing ICON which, if people were as logical as their tools, will
surely replace SNOBOL as effectively as the transistor replaced the
thermionic valve.
</p>
<p> People being people, however, it seems probable that new
generations will continue to cut their programming teeth on this
magnificently eccentric and dangerously powerful language. Hard
evidence of this continuity is provided not just by two earnest
contributions from recent academic converts to SNOBOL at the
University of Nebraska but also in the new SNOBOL and SPITBOL systems
available for microcomputers. Only one of these (Catspaw's SNOBOL4+)
is mentioned here, in the context of some interactive debugging
utilities, the code for which is the most substantial part of Mark
Emmer's paper. It would be interesting to see some comparison between
the performance and facilities of the various SPITBOLs and SNOBOLs now
available on small machines; perhaps this will come in ICEBOL 86,
which we are assured will also take place in Madison, South Dakota.
</p>
</div><!--
ICEBOL 85 (Proceedings of the 1985 International Conference on English Language 
and Literature Applications of SNOBOL and SPITBOL, edited by Eric Johnson, 
Dakota State College)
 
This trim volume contains 11 papers, of which, without exerting excessive 
charity, only three could be regarded as of more than purely local interest. 
Moreover, one of these is not about Snobol and another is not about English 
language or literature. One can only assume that the event itself had 
compensatory  advantages; its organiser assures us that he at least has "never 
attended a conference where the conversations were so animated and engrossing". 
Judging by the enormous disparities in expertise and attitudes to computing 
revealed in the papers here published, some of this animation might be 
attributable to sheer mutual astonishment.  One can only speculate as to the 
breakfast-time  conversations between say Peter-Arno Coppen (Catholic 
University of Nijmegen) and Wayne Tosh  (St Cloud State University). Coppen's 
paper describes GRASP, a parser-generator for context-free grammars which 
happens to be written in Spitbol. Tosh's describes a number of undergraduate 
exercises which also happen to be written in Spitbol. Both authors regard 
Spitbol as an appropriate teaching vehicle, but the disparity between what is 
being taught is so extreme as to cast doubts as to the existence of any point 
of contect other than the accidental identity  of the chosen language.
In the same way, this reviewer would have liked to be privy to any conversation 
between Ronald Sussman of the Antares Corporation in Minnesota, and William C. 
Strange of the University of Oregon's English department. Without undue 
disrespect, Mr Sussman's style leaves me gasping for breath, but as I 
understand it, the tenor of his argument is that the English language itself 
can be interpreted as if it contained only logical statements, provided the 
notion of what constitues logic is "slightly" extended (it could also be said 
to consist of purple anxieties, similar licence permitted, but let that pass). 
Mr Strange, writing  at a more even speed, contributes some good old fashioned 
practical criticism  of three concrete or, in Strange's phrase, "computable" 
poems, leading him to the extraordinary and unsubstantiated assertion that 
"Poems and programs are surprisingly alike".
 
Surely more typical of SNOBOL's current user population are two workmanlike 
contributions about real life applications of the language, one from Paul 
Bantzer (Startext GmbH, Bonn) about its usefulness in a photo-typesetting 
application, the other from Timothy Montler (North Texas SU) describing a whole 
suite of programs developed to assist in the linguistic analysis of a Northwest 
American Indian language corpus. The latter is particularly impressive, and 
deserves to be better known.  The book begins, as is only appropriate, with two 
contributions from Ralph Griswold, one, tantalisingly short, on the history of 
the language (which reveals that it was originally called SEXI for String 
Expression Interpreter) and the other describing ICON which, if people were as 
logical as their tools, will surely replace SNOBOL as effectively as the 
transistor replaced the thermionic valve.
 
 People being people, however, it seems probable that new generations will 
continue to cut their programming teeth on this magnificently eccentric and 
dangerously powerful language. Hard evidence of this continuity is provided not 
just by two somewhat embarassingly earnest contributions by recent graduates of 
the University of Nebraska but also in the new SNOBOL and SPITBOL systems 
available for microcomputers. Only one of these (Catspaw's SNOBOL4+) is 
mentioned here, in the context of some interactive debugging utilities, the 
code for which is the most substantial part of Mark Emmer's paper. It would be 
interesting to see some comparison between the performance and facilities of 
the various SPITBOLs and SNOBOLs now available on small machines; perhaps this 
will come in ICEBOL 86, which we are assured will also take place in Madison, 
South Dakota.
 
 -->
      <!--
There follows the original text of my review of Mind Wheel, just to keep
you innerested. The thing itself, plus the review as published (which 
turns out to be subtly different) are in the post...
--><div xmlns="http://www.tei-c.org/ns/1.0" xml:id="pinsky">
<head>Byt and myth</head>
<bibl><title>MIND WHEEL An Electronic Novel (TM)</title>: by Robert Pinsky (Author) Steve Hales 
(Programmer) and William Mataga (Programmer). A Synapse and Broderbund 
Production. (Synapse Software Corporation, 1984) $39.00</bibl>

 
<p>On the face of things, the Electronic Novel (Trade Mark) should be
just the ticket for those practioners of the higher criticism who seek
to overthrow authority by re-building the sense of a text as it is
read. A significant amount of the Electronic Novel really is
constructed by the reader, who by typing at the computer keyboard
actually creates the story (for yes, oh dear yes, it does tell a
story) encounter by encounter, dialogue by dialogue. But there is
something illusory, something stanley-fishy, about this freedom. You
are free to disregard the overall purpose of the exercise, the
pre-text as it were; you can play it simply as a sight-seeing
exercise; you can marvel at the varyingly inept ways in which a
computer program can be made to torment the English language. But
these froward reactions will be rewarded only by boredom.
</p>
<p>Electronic Novel (Trade Mark) is but Adventure Game writ large. In
the adventure game, even this up-market version of it, plot is
everything, and the plot is that same old atavistic heroic quest for
something-or-other, liberally laced with backwards-chained
reasoning. To get the girl, Perseus had to conquer the Kraken; to
conquer the Kraken, he had to behead the Medusa; to behead the Medusa
he needed the advice of the three witches. In the same way, the
electronic adventurer, like some experimental rodent, must run a maze
of dark hints and false tracks, responding appropriately and in the
correct sequence to stochastically mutated stimuli.  To get up the
staircase, you must placate the weeping soldier. Nothing will do this
except a helmet full of rainwater from outside the castle gates; a
good joke, or even a Babe Ruth candy bar, will not suffice. If you set
foot outside the castle gates (having solved the riddle written over
them), you will be arrested by a lizard-like officer and rot away
behind barbed wire, unless you have previously acquired some suitable
magical talisman. To get the talisman, you must fight the thug instead
of sneaking out of the backstage door when he isn't looking. On the
other hand, if you climb the tree inside the prison camp and if you
still have your Babe Ruth candy bar, eating it might just cause you to
grow wings and escape.  If, having discovered all this, you decide
you'd rather not go up the staircase at all, you will rapidly exhaust
the possibilities of this electronic text, in rather the same way as
if early in Act I you had decided that on the whole Uncle Claudius was
quite a decent old stick.
</p>
<p>According to the lavishly illustrated printed prolegomena which
accompanies the floppy disk, "Mindwheel" came to its author during
fasting and meditation in Eastern Montana. Its original form, written
in a frenzy on the walls and ceiling of his cell, was reproduced and
edited from "large-format AccuLens photographs taken at the Greville
Meditative Order headquarters", (a photograph of the latter, though
not the former, is provided) but it took several years before suitably
innovative programmers could be found capable of implementing "the
grand scheme of those three days and nights of convulsive
inspiration". It certainly is several years since the Adventure Game
first became the favourite late night pastime of anyone with access to
MIT's computer systems, but its basic form has survived the
micro-computer revolution more or less unscathed.  The participant
still wanders around compulsively gathering up objects of dubious
utility, still encounters implausible conditions and still suffers no
worse fate than having to begin all over again. The sophistication
with which computers can be made to approximate realistic dialogue has
advanced a little, though this program is no more difficult to confuse
than any other. (Words not in its vocabulary are politely rejected
with a message apologising for their absence; as the adventure is
supposedly being performed via a computer terminal, such
computer-style 'error messages' as appear are in any case
appropriate).
</p>
<p>The originality of Mindwheel, such as it is, lies rather in the
richness and complexity of its imagery, a far cry from the usual
dungeons and dragons. The four "minds", passage through which forms
the plot of the adventure, are those of an assassinated rock star, a
poet, a military dictator and a mysterious female scientist. Each mind
is realised as a different environment, populated by varyingly helpful
or antagonistic characters and the usual collection of objects; there
is also a kind of background flow of randomly selected sentences
appropriate to each locale. Some objects and at least one character,
an endearingly helpful frog, can be carried from one environment to
another. Words and phrases have a magic of their own; riddles have to
be answered, and hints interpreted, while at one stage a poem (from
Pinsky's 1984 publication "History of My Heart") has to be
completed. Themes and images are combined to an unusual extent: the
name of the rock group is "Tyranny and the Senses"; the frozen wastes
of the Generalissimo's prison camp are behind a door which is opened
by the word "Tyranny"; escape from them involves freeing a frozen
fountain and a soldier turned to stone from the waist down, and so
forth. What little resonance remained in the sword and sorcery
sub-Tolkeinish world of most adventure games having long since been
hyped flat, Pinsky has wisely gone to the springs of his own
subconscious to create new mythical worlds. Whether the compulsion to
get to the goal, common to all adventure games, is so strong as to
blind the user to the richness of verbal texture, inventiveness of
incident and the overall unity of this adventure very much depends on
the individual adventurer.
</p>
</div>
      <div xmlns="http://www.tei-c.org/ns/1.0" xml:id="stein">
<head>A passion for control</head>
<bibl>Dorothy Stein: <title>Ada: A life and a legacy.</title> MIT Pr., 1986.</bibl>
 
<p>The set of persons after whom computer languages have been named is not a
large one and includes some very odd bedfellows: Blaise Pascal, William of
Occam and Ada Byron, Countess of Lovelace all have this, if nothing else,
in common. When the American Defense Department decided to honour the
memory of Byron's daughter by naming its newly-designed general purpose
programming language after her, it was presumably motivated more by the
myth that has grown up around this unfortunate lady than by the fact that
ADA actually sounds like an acronym, (as indeed do PASCAL and OCCAM). 
Even in her own lifetime, Ada was surrounded by myth. Her mother, Byron's
estranged wife and a woman of truly Dickensian awfulness, was obsessed by
the  manufacture and control of her public image; her daughter's
personality could not hope to escape manipulation as intense as that of
any modern politician or media personality. Consequently, Ada's chief
legacy is a dense tissue of intrigue, gossip and supposition,  which this
fascinating book patiently unweaves and explicates, using as its raw
material two main sources, firstly the archive of original letters and
papers so carefully preserved and, we must assume, largely doctored by
Lady Byron and her descendants to support her view of the case; secondly
the papers of Charles Babbage and those few others of Ada's friends and
confidants capable of resisting the demands of Ada's family lawyers. By
thus going back to primary sources, Stein is able to debunk more myths
than one. 
</p>
<p>Ada Byron was not quite two years old when first brought to public
attention in the third canto of <hi>Childe Harold's Pilgrimage</hi>, where
separation from her is presented as yet another instance of the terrible
cost of being Lord Byron, <hi>poete maudit</hi>. Her mother's reaction to this
celebrity was to shelter the child from all knowledge of her father for as
long as possible, and to treat her childhood ailments with a regime which
seems eccentric even for those leech-ridden days. Ada's childhood appears
to have been dominated by lengthy periods of debility; Stein argues in a
somewhat speculative appendix that she may have suffered from porphyria,
although the psychosomatic explanation also presented is surely more
persuasive. Needless to say, with adolescence the glamour of being Byron's
daughter could no longer be resisted, neither by Ada herself nor by others
(Disraeli's <title>Venetia</title> (1837)  for example is a wildly misleading novel
about this aspect of her personality); nor, with marriage, could her
mother's tutelage be maintained. Visiting Newstead Abbey, Ada expressed
the wish to be buried with her father there, and although it must have
annoyed her husband almost as much as her mother, in the event her request
was honoured. Stein uncovers much evidence that Ada had, within the limits
allowed, qualities as romantic and headstrong as might have been expected
from Byron's daughter. As well as an early infatuation with a tutor (no
more than might be expected from anyone similarly placed in time and
social position), Stein uncovers evidence of an adulterous affair with the
shadowy John Crosse, an experimental scientist of somewhat dubious social
standing. Equally, Ada's weakness for betting on horses, and the desperate
shifts to which her incompetence at it lead her, provide as scandalous a
backdrop to her painful death as any silver fork novellist might desire. 
</p>
<p>The Byronic mythos was also Ada's chief attraction for her first
biographer. Doris Langley Moore's book,  revealingly sub-titled <title>Byron's
Legitimate Daughter</title> (1977), continually moved Ada off centre stage in
favour of this or that Byronic association. Moore's (entirely just)
distaste for Lady Byron also lead her to give that unspeakable harridan a
misleadingly dominant role in Ada's life. For much of her short life, Ada
was barely on speaking terms with her mother, and although the latter
re-established her hegomony by influence over Ada's husband, Stein gives
ample evidence of Ada's independence of will, until, in her last year, she
was too weakened by disease to resist the domination of the woman whose
virtues Byron had compared (unfavourably) to "thine 'incomparable oil',
Macassar!". 
</p>
<p>Stein's Kings Charles's head, if she has one, is more contemporary. For
Moore, as surely for Ada's contemporaries, Lady Lovelace's apparently
passionate enthusiasm for mathematics and science was an interesting and
quirky, if un-lady-like, hobby; for Stein perhaps, for some of her readers
surely, it marks her as one of the great might-have-beens, denied the
fruition of her talents by a represssive patriarchy. For evidence of the
new Ada mythos, any popular introduction to the electronic computer may be
consulted: somewhere in chapter one there will be a paragraph or two on
the history of the subject, in which Charles Babbage's Engines will
naturally appear, closely followed by the ghost of Lady Lovelace, 
supposedly with Babbage one of the very few who understood the true nature
and purpose of the unbuilt Analytical Engine, occasionally reputed
Babbage's mistress, always billed as the world's first programmer. 
</p>
<p>Certainly, like her mother, Ada had a passion for mathematics.  (Stein
drily supposes that the subject was first recommended to her in
adolescence as an alternative to "objectionable thoughts"). However
Stein's close analysis of Ada's surviving correspondence with such
notables as Augustus de Morgan, Julia Sutherland and Babbage himself 
shows her to have had stunningly little aptitude for the subject, being
unable (for example) to understand the simple principle by which terms
from one of a pair of simultaneous equations may be substituted in the
other. In translating from French L.F. Menabrea's <title>Memoir</title> on Babbage's
Analytical Engine, she followed a printer's error in the original which
made mathematical nonsense, apparently blindly. The published <hi>Notes</hi> to
this translation, on which her reputation as computer programmer <hi>avant
la lettre</hi> must rest, were written very much under the guidance of
Babbage himself. For him, the publication was a golden opportunity to
renew his case for further funding, at a time when he was suffering from
government neglect remarkably similar to that suffered by Daniel Doyce at
the hand of the Circumlocution Office. As well as the occasion, the
general tenor of Ada's <hi>Notes</hi> is also often mis-represented: today, the
separation of the analytical engine into units supporting store, mill, and
input and output functions can only prefigure the programmability of the
modern computer. Ada however made this point only in passing and in order
to stress the difference between the proposed engine and the existing
difference engines. For Ada, and presumably also for Babbage, the truly
remarkable feature of the analytical engine was its separation of data
from process, and its consequent generality of applicability, . Stein
demonstrates that (in the <title>Notes</title> as a whole) the thought underlying
this distinction derives more from Manchester school economics translated
to mechanics than from the sort of imaginative leap which now seems so
remarkable in Ada's much quoted comparison "the Analytic Engine <hi>weaves
algebraic patterns</hi> just as the Jacquard-loom weaves flowers and leaves".
</p>
<p>Ada's brilliant mathematical career was launched (and terminated) with the
publication of her translation of Menabrea and its  <hi>Notes</hi> in 1843.
Babbage was to pay her generous tribute in his autobiography, but this was
written a decade after her death when the memory of the scandal that
surrounded it and of his subsequent quarrel with the Lovelace family was
presumably sharper than the truth of their working relationship some
twenty years earlier.  At that time, her vague offer of further
collaboration had been refused, firmly but politely. They had also
quarrelled over a proposed preface summarising Babbage's funding
difficulties, of which the Notes profess a somewhat ostentatious
ignorance; Ada's delusions of grandeur cannot have greatly endeared her to
the eminently practical Babbage either. In 1844, she abandoned mathematics
for music, then for science and mesmerism. As to her mother, so to Faraday
and others she wrote lengthy self-glorifying letters describing herself as
"the Deborah, the Elijah of Science", but no further prophecies were
vouchsafed her. 
</p>
<p>The last ten years of her life seem oddly empty. Her husband, the
long-suffering Count of Lovelace, took up a brief literary career; over
two years he published nine articles on suitably respectable issues of the
day, such as the Suffrage,  Malthus and  Animal Husbandry on all of which
he argued a solidly respectable point of view. In one of these, a review
of Gasparin's <title>Course of Agriculture</title>, the need to grapple with the
statistical problems of relating climate and crop size lead to a
collaboration with his wife, and further consultations with Babbage and
Sir John Herschel. Ada eventually contributed some Notes to the published
Review containing a number of unexplained and probably inexplicable
formulae; these form her only other published work.  Her disastrous
passion for the turf dates also from this last decade of her life; Stein
demonstrates persuasively both the absence of any involvement with Babbage
in this and also that the Countess was at least as often the duper as the
dupe in her gambling adventures. However, hard evidence for this, as for
her liaison with Crosse, is necessarily scarce, since these were precisely
the two subjects which lead to her mother's resuming control of Ada's
public image, during her needlessly protracted and painful death from a
cervical cancer in 1860. 
</p>
<p>Dorothy Stein is not primarily a  historian (at one point she remarks of
historians that they are "a sentimental lot"), but a psychologist with a
respectable understanding of physics and computer programming. As such she
is as much interested in explicating the people around Ada as determining
what their actions may or may not have been. Where Moore catalogues (say)
the interminable misdeeds of Lady Byron rather in the manner of the
village gossip, (pausing as it were only for the occasional "Well!" and
"Did you ever?" from the reader), Stein seeks motivation and psychological
patterns : "Her passion was for control, not care... Motherhood succeeded
wifehood as the name of her propensity for self-justification, only to be
replaced by grandmotherhood". To set against this sensitivity for the
human and the universal, it must be noted that Stein is far weaker than
Moore in her sense of period, occasionally and irrelevantly remarking in a
shocked tone on her protagonists' extraordinary insensitivity to the
plight of the working man in the 1840s, making very heavy weather of the
financial basis of the Byrons' marriage, and solemnly remarking on the
absence of antibiotics in the early 19th century.  Some rather more
aggressive sub-editing (which would also have removed several typos)
should have been invested in this altogether unusual and impressive book. 
</p>
</div>
      <div xmlns="http://www.tei-c.org/ns/1.0" xml:id="grundy">
<head>
<bibl><title>Proceedings of the Fourth British National Conference on Databases</title>,
edited by A. F. Grundy (Cambridge University Press, 1985). Price
&#xA3;25.00 h/b; ISBN 0 521 32020; 229 pp.
</bibl></head>
<p>The annual BNCOD conference is rapidly becoming an indispensable
fixture; it remains the only regular forum for presentation of British
research **ID the fundamentals of database systems, and as such is to be
valued. The papers in this volume reflect a tendency for the
conference to concentrate on theoretical issues at the expense of more
practical matters, though this is not in itself a criticism.
</p>
<p>Two invited papers, from Schmidt, on higher level relational objects,
and Pelagatti, on synthesizing relational and network models, typify
the growing awareness of the limitations of the relational model,
though Pelagatti's has little new to contribute. The BCS/CODASYL
Database Administration (DBAWG) presents a new and characteristically
thorough Access Control system, grasping a nettle which most
standardization bodies have been reluctant to consider. Borges reviews
existing discussions of the problems of concurrency control. Jiang and
Lavington breathe new life into the binary relationship model by
extending it to represent qualified knowledge, a necessary extension
for the long-awaited Manchester Intelligent File Store.  Feldman and
Fitzgerald present a novel graphic formalism for modelling actions, a
part of information systems analysis which has always tended to be
neglected. Atkinson and Stocker et al   report on the continued quest
for that holy grail, the Abstract Conceptual Schema, and its
associated mapping problems. Hitchcock et al. give a preliminary
report on the Aspect 'toolkit' approach to the definition of
*pses. Two short papers deal with user interface problems: Deen et
al. describe (yet another) new relational database management system
called PRECI/C, of interest perhaps in a teaching environment, while
Newman and Sethi report some fieldwork into the effectiveness of
various style of interactive help systems.
</p>
<p>Only two papers deal explicitly with particular application areas:
Moore on the problems of maintaining integrity in a large real-time
distributed database system for use in naval command and control
provides ample food for thought for anyone seriously contemplating
Star Wars research, while Jeffery attempts to synthesize some
traditional DBMS and office automation systems into a single
information system' using a fairly simple three level model.
</p>
<p>The book as a whole is well produced (conspicuously better than
earlier ones in the series) and, as a snapshot of the current
preoccupations of some British computer science departments, could not
be bettered. Whether it has any wider usefulness is more debatable.
</p>
<trailer>L. Burnard University of Oxford</trailer>
</div><!-- OCR errors-->
      <div xmlns="http://www.tei-c.org/ns/1.0" xml:id="date">
<head>
C.J. Date  (1)<title>Relational database : selected writings</title>
Addison-Wesley, 1986. 0-201-14196-5; (2) <title>A guide to INGRES</title>
Addison-Wesley, 1987. 0-201-06006-X; (3) <title>A guide to the SQL
standard</title> Addison-Wesley 1987. 0-201-05777-8.1-8 </head>
 
<p>Chris Date is, or should be, best known as author of both the
standard textbook on relational database systems (<title>An
Introduction to Database Systems</title>, volume 1 [1986] and volume
2  [1983]) and one of the best of the many introductory texts
on the same subject now available for the general reader
(<title>Database: a primer</title> [1984]). Not content with resting on
these laurels however he has now complemented them with the
three books under review, the production of any one of which
would have been more than adequate for most of us; to have
produced all three in such rapid succession says much for his
enthusiasm, his word processing skills and indeed his
publishers' confidence. There is perhaps inevitably a sense
of <foreign>d&#xE9;j&#xE0; vu</foreign> when reading through all three of these
volumes; several of the pieces in <title>Relational Database</title>,
for example, are unabashed reworkings of earlier papers, one
of which (the SQL critique) reappears again in <title>A guide to
SQL</title>, and acknowledged as such. There is some substance in
the charge that Date is continually re-writing the same book,
getting it wrong in different ways each time; a charge which
he attempts to defuse by making it himself in the preface to
the <title>Guide to SQL</title>. Nevertheless, it would be very
difficult to choose amongst these three if one were permitted
only one of them.
</p>
<p><title>Relational Database: selected writings</title> is an anthology of
seventeen papers most of which appeared in various varyingly
inaccessible professional journals or conference proceedings
during the past 15 years, together with two previously
unpublished pieces and a lengthy interview. The formality of
presentation naturally varies somewhat amongst these and
there is also some repetition; however Date's style, even at
its most formal, is never obscure and the complete tyro need
not be put off by the book's provenance. On the contrary,
Date has the rare ability to expound the most abstruse
notions in a way not only comprehensible but also exciting,
without compromising the technical complexity of his subject
matter or fudging the issues. In many of these papers there
is an urgency of polemic to which the least technically-
minded can respond; such topics as the role of null values
(somewhat of a King Charles's head for this author),
integrity constraints, the outer join or updateable views in
Date's hands cease to be arcane matters beyond the
comprehension of those without a master's degree in computer
science. And on some seemingly esoteric issues, such as the
importance of orthogonality in language design, the reader
with a humanistic background may find him or herself
agreeably surprised by the closeness of Date's position to
his or her own.  
</p>
<p>The book is divided into four major sections, on relational
database systems in general, on the differences between them
and non-relational systems, on the SQL language and on
database design; within each section, individual chapters
bear the full paraphernalia of abstract, preface,
conclusions, comments on republication, bibliography etc. The
effect of all this formal structuring is twofold: firstly it
serves to control Date's exuberance &#x2014; his willingness
(manifest in the reprinted interview) to have an opinion on
every database topic under the sun. Secondly, it makes the
book more usable as a reference work, containing as it does a
distillation of several of the debates (now largely settled
for better or worse) which have been animating both academic
and professional data processing communities over the last
decade. Members of other communities, to which these debates
have been as but distant rumblings, will derive
correspondingly more profit from their summaries here, while
some chapters (notably the chapter called "Some relational
myths exploded") should be made compulsory reading for all
those claiming to have any opinion on the subject of database
systems at all.
</p>
<p>Amongst the "relational myths" most in need of explosion is
the one that regards the academic (i.e. theoretical,
impractical etc.) parentage of relational database systems as
a weakness rather than a strength. Of the three or four such
systems currently clamouring for control of the marketplace,
it is therefore all the more pleasing that the system which
at the moment dominates the non-IBM world should be directly
descended from a prototype built at the University of
California at Berkeley during the early 1970s. The design and
construction of INGRES and its evolution into a highly
successful commercial product have been described in some
detail in a book  by Michael Stonebraker (<hi>The INGRES
papers</hi> 1986); Date's volume <hi>A guide to INGRES</hi> is
complementary, providing a fairly detailed and not entirely
uncritical description of that commercial product, as
currently marketed by Relational Technology Inc. for machines
of all sizes from the IBM PC upwards.
</p>
<p>As far as possible this volume eschews polemic and (even)
theory. Consigned to an appendix there are two chapters, one
on the advantages of relational systems in general and INGRES
in particular and the other a much condensed version of the
paper on relational concepts also reprinted in <hi>Relational
Database</hi>. Instead, the book is clearly intended as a
teaching text. The same well-worn supplier and parts database
is used as example throughout and there are useful exercises
at the end of each expository chapter. Not all components of
the INGRES system are treated with equal thoroughness
however. In particular, QUEL (the original INGRES relational
database language) gets very much more detailed treatment
than SQL (the standard relational database language), and
some features, most notably those peculiar to the current
version of RTI's INGRES such as the various application
development tools, are treated very patchily indeed. There is
of course considerable overlap between the facilities of SQL
and QUEL, and there are many who consider the latter to be a
better and more powerful language than the former;
nevertheless, if only for reasons of standardisation, it
seems likely that most newcomers to INGRES will begin with
SQL &#x2014; and consequently find this book less useful than it
might be. 
</p>
<p>Detailed descriptions of (say) the INGRES screen painting
utilities would clearly be out of place in a text book. All
the same it seems a pity that Date has not bothered to give,
say, OSL (the INGRES "fourth generation language") the
detailed critical attention it needs, both in its own right
and as a fairly representative exemplar of the sort of high
level glue to be found in most modern relational database
systems. Similar considerations apply to the INGRES report
writer and the EQUEL/FORMS interface. Just enough information
is provided about each of these to give some idea of their
capabilities, but for more detailed knowledge, the student or
practitioner must refer to the manufacturer's manuals, where
critical assessment is unlikely to be had, for all their
other merits. 
</p>
<p>In his <hi>Guide to the SQL Standard</hi>, Date amply compensates
for the lack of information on SQL in the INGRES volume. The
<hi>Guide</hi> is really two books in one: one is a relatively
informal but very thorough summary-cum-tutorial about the SQL
standard itself; the other, which runs below the surface
throughout, finally emerging in a 50 page appendix, describes
the shortcomings, infelicities, inconsistencies and downright
lacunae to be found in that standard &#x2014; some of them far from
trivial &#x2014; together with some suggestions as to how they might
be removed. The tutorial and expository parts are lucid,
jargon-free and reliable. As in his other books,a single
example database (yes, based on suppliers, parts and
projects) is used throughout. This has the virtue of
concentrating the reader's attention on the SQL feature being
illustrated rather than the quiddity of the application, but
the defect of requiring a certain imaginative effort on the
part of the reader wishing to apply that language feature to
a topic more relevant to his own concerns. There is also a
particularly useful set of exercises (with model answers),
the mastery of which may be taken as proficiency in the
language.
</p>
<p>Readers of this journal may perhaps wonder why, if the
standard is so flawed, it is worthy of their consideration at
all, or question the need for any sort of standardisation in
database languages in the first place. This is not perhaps
the place to tackle either of these fundamental
misconceptions. Some important differences between SQL and
other computer languages should however be stressed. In the
first place, SQL is based on a sound theoretical model (the
relational model first defined by Codd in 1970) and as such
demonstrably superior to other languages based on ad hoc
perceptions of the way problems are or might be solved.  In
the second place, SQL is a very high level language for
defining, re-defining and manipulating database structures in
terms which are entirely independent of their physical
storage  or realisation. It thus becomes possible for your
favourite database software package X running on machine A to
manipulate data held by your worst nightmare package Y
running on machine B, even to join it seamlessly with data
held by a package Z of which you have never heard running on
a machine C of the existence of which you are also ignorant.
Now that SQL has made the difficult transition from de facto
standard to official acceptance, more and more major database
software suppliers are adding SQL interfaces to their
existing systems to make this possibility a reality. It is
already possible to develop small (and not so small) database
applications on an off-the-shelf micro, secure in the
knowledge that they can be moved to larger mainframes with no
more effort than recompilation when they outgrow the micro's
capacity, or to integrate separate parts of a distributed
database held on a network of such micros. These are the real
benefits of standardisation, and, for all its shortcomings,
the SQL standard is a very important first step towards
reaping them. Acquiring a detailed knowledge of that standard
is of corresponding importance for all those wishing to make
use of database technology in the years to come, and Date's
works are amongst the best to be found for that purpose. </p>
</div>
      <!--
From:	VAX::ARCHIVE      10-FEB-1988 16:01
To:	LOU
Subj:	
--><div xmlns="http://www.tei-c.org/ns/1.0" xml:id="friedmann">
<head><title>The Little <hi>LISPer</hi></title>, by D.P. Friedmann and M Felleisen (MIT Press,
1987). Price &#xA3;9.95 p/b. ISBN 0-262-56038-0; 186 pp.
</head> 
<p>It is hard to believe that the first edition of this classic 
introduction to the wonders of recursive list processing first 
appeared over a decade ago. In this new and expanded version,
nothing of the freshness and wit of the original has been lost,
while  the  ingenuity with which exposition and drill are combined 
by its catechitical form is no less impressive.  Above all,  
the authors' enthusiasm for their subject remains no less infectious. 
What is new in this volume, is a broadening of purpose: where 
the first edition was primarily a self-teaching manual for the 
then-modish LISP language, this new edition (coming into a sadder 
and wiser world) has a larger purpose: to use the Ten Commandments 
and Five Laws of Lisp as a vehicle for the teaching of recursive 
techniques and their value in general-purpose problem-solving. 
As an aim, this cannot be faulted. The value of LISP as a programming 
language remains debatable (and hotly-debated); its elegance 
and simplicity as an embodiment of some fairly crucial notions 
of computer science surely much less so. 
</p>
<p>In its first incarnation, this book did a great deal to broaden 
the appeal of the language. It really was possible (as the original 
preface suggested you should) to sit down by the refrigerator,
slog your way through the book and feel your mind being modified. 
The importance of the refrigerator as mother / sustenance figure 
in American culture is not entirely irrelevant in the present 
context, for this book's curious obsession with food cannot be 
ignored, if only because of its effectiveness as a way of gaining 
that complicity between instructor and instructed on which a 
good teacher always depends. At the end of the first chapter,
for example, with the notions of ATOM, LIST, CAR, CDR, NULL and 
EQ firmly under our belt, we are advised to make a "peanut butter 
and jelly sandwich" - and provided with a blank page on which 
to spill the jelly (anglic&#xE9;, jam).  At the end of the seventh 
chapter, there is a (fairly reliable) recipe for making that 
staple of American cuisine, the chocolate chip cooky, unusual 
only in that it is written in LISP.  
</p>
<p>As the preface warns, this book should not be read by those seriously 
interested in dieting. Nor is it particularly suited as a classroom 
text in either logic or LISP programming. But  as a painless 
- even pleasant - form of mental exercise for those left feeling 
flabby and out of condition by the more formal ways of grappling 
with recursive principles, this book is a tonic heartily to be 
recommended. At least some of the healthy state of logic programming 
today must be due to the enthusiasm which the first Little Lisper 
kindled in a past generation of students; the effects of this 
new edition on a new generation will be all the more interesting 
to watch for.
</p></div>
      <div xmlns="http://www.tei-c.org/ns/1.0" xml:id="naylor">
<head>
<title>The PC Compendium. Volume 1</title>, by C. Naylor (Wiley for Sigma Press, 1987). Price &#xA3;12.95 p/b. ISBN 1-85058-097-1; 330 pp.
</head>

<p>This volume collects together 34 specimens of the sort of cheap 
and cheerful hack journalism with which regular browsers of the 
micro trade press will be all too familiar. It is hard to see 
any possible justification for the republication of such material 
in book form, other than that candidly avowed in the preface,
of selling it all over again &#x2014; repeatedly indeed, since there 
is a considerable amount of repetition in the volume. Naylor 
has done very little in the way of editing his PC-Week columns 
to avoid repetition and nothing at all about his style. The latter 
has a distinctive but rather irritating mid-Atlantic chattiness 
, a sort of determined chumminess  in which set phrases such 
as "Days of Yore" get capitalized, and British and American argots 
occasionally collide (as in "a right old collection of garbage"). 
</p>
<p>Sentences and paragraphs start abruptly, sometimes even lacking 
verbs. He has a mildly endearingly E.J.Thribb-like manner of 
starting paragraphs with the single word clause "So:" In small 
doses, and in weekly journalism, such an approach is pleasant 
enough, but in any quantity it becomes wearisome; this book (alas) 
is only volume one of a projected series.
</p>
<p>The subject matter includes some rather complicated ways of extending 
the (admittedly  primitive) facilities of MS-DOS by means of 
(God save the mark) a variety of BASIC programs, the sourcecode 
for which is reprinted at the end of the book and is also available 
on floppy diskette for the real enthusiast.   It is hard to see 
any long term interest in these, except perhaps in the nostalgia 
market, where they will provide vivid testimony  to the manner 
of nonsense endured during the early days of the micro-revolution. 
There are also a number of elementary introductions to various 
areas of micro computing which have become fashionable over the 
last few years, such as expert systems on which Naylor has also 
written a (comparatively) sensible book. Such articles at their 
best are, like an Oxford education, a very good way of acquiring 
just enough knowledge to sound as if you know what you are talking 
about.  
</p>
<p>The book also features one of those terminally unfunny joke glossaries 
which seem to wander around  large offices like lost souls and 
a minimal index which enabled me to check that Naylor really 
does think a decibel is so called because it contains ten bels,
but not to find again the page on which he talks of a "hyperbolus" 
&#x2014; presumably some sort of very efficient medecine.
</p></div>
      <div xmlns="http://www.tei-c.org/ns/1.0" xml:id="rudall"><head>
B.H. Rudall and T.N. Corns: <title>Computers and literature. A practical guide.
</title>Abacus Press. 0-85626-340-0</head>
 
<p>When computers perform literary criticism, they will be belle-lettriste,
pre-structuralist, post-structuralist, neo-Leavisite and Marxist. Despite
appearances, this is not one of those counterfactual sentences which so
delight the popularisers of computer science: the use of computers as
heavy artillery to back up the more speculative slingshot afforded by
linguistic intuition is rapidly becoming as routine in the various
branches of theoretical linguistics as (in the form of the word-
processor) it is already un-remarkable in the productions of traditional
humanistic scholarship. An Association for Literary and Linguistic
Computing (of which one of the present authors is Honorary Secretary) has
been in existence for nearly fifteen years now; more recently both
Historians and Art Historians have formed their own learned societies to
promulgate the digital gospel. At annual international conferences and in
the pages of learned journals, the joys and miseries of computer-aided
textual analysis have long been the subject of discussion, regret,
braggadocio and commiseration. Surely by now Computer Based Criticism
(CBC we could call it) must be a pursuit whose Time Has Come. 
</p>
<p>In the old days, when users had to write their own programs and
then get them transferred to paper tape or punch cards, there were
perhaps a few giants who would think nothing of tossing off 5000 lines
of KDF-9 ALGOL before lunch and an elegantly turned essay on the
provenance of some Shakespearean foul paper after it. But for the
majority, it must regretfully be said, it was labour enough to have
persuaded the local mainframe to pay them any attention at
all. Consequently, literary scholarship was not, despite early
optimism and a few promising eccentrics, radically transformed by the
cybernetic revolution, nor could it be, so long as the computer
remained an object of curiosity in the study, and research done with
its aid the subject of amused skepticism in the common room. The
emergence of the home computer has done much to accomplish the
demystification of the device itself; now there is the contrary danger
that its full potential will be overlooked, unheard for the swelling
demotic babble of the word processor. It remains depressingly
difficult for the historian of criticism point to many major studies
achieved, or even minor insights gained, by truly computational means.
</p>
<p>Corns, at least, may be presumed to be aware of this fact: it is
the only charitable explanation I can find for the skimpiness of such
few pointers to the available literature as exist in this remarkably
slim volume. The absence of any index is harder to excuse,
particularly when 17 pages are given over to a <soCalled>Glossary of
terms</soCalled> of remarkable silliness. It is always easy to nitpick
terminology (why should CHARACTER be present, but not BYTE, for
example) and there is a pleasing serendipity in finding ALGORITHM,
ALLITERATION and ANSI together, cheek by jowl. For the most part
however, this glossary contains too many near synonyms too vaguely
defined. Why gloss (for example) both PROBLEM ORIENTATED LANGUAGE and
PROBLEM-SOLVING LANGUAGE &#x2014; especially when the text for both is
identically uninformative (<q>a programming language designed for the
convenient expression of a problem</q>)?
</p>
<p>Those whose business is the study of language will find such things
risibly unhelpful; they will be equally unimpressed by the three
chapters of this book intended to provide some background information
on the nature of computing. To the post-Sinclair generations, Rudall's
description of how text is typed into a computer may have a certain
antiquarian charm; they will also perhaps read much into the flurry of
unnecessary quotation marks peppering the paragraph in which he
grudgingly concedes that not everyone is as convinced as he of the
old- fashioned virtues of <soCalled>flowcharting</soCalled>.  This is
harmless enough, if dull; rather more pernicious is a grossly
misleading chapter on programming.  This contains a list of
<soCalled>widely used high level languages</soCalled> in which
Fortran, Ada or Pascal are jumbled together with such nonce-tongues as
NELLIAC, and in which COBOL &#x2014; probably the most widely used
programming language in the world &#x2014; and SQL &#x2014; probably the
most useful &#x2014; are both equally conspicuous by their absence. It
was a good idea to "compare and contrast" a simple programming problem
in two different languages, but the languages chosen (BASIC and
PASCAL) are too similar for this to have been as instructive as a
comparison with (say) PROLOG, LISP and SNOBOL solutions might have
been. Along with an inexcusable number of misprints, this chapter also
contains the astounding assertion that <q>SNAP has the only natural
affinity to the (literary) problem area</q>.
</p>
<p>Fortunately, the rest of the book is less eccentric. One chapter is
given to the well-worn theme of concordance-generation, on which Corns
is non- controversial and practical, if a little uninspiring. It is
hard to see the virtue in describing both the Oxford Concordance
Program (OCP) and its predecessor COCOA, since the former has now
almost entirely replaced the latter.  The space might have been better
spent on OCP's major current rival: the Brigham Young University Micro
Concordance program.  Another chapter makes a gesture towards
providing guidance for those contemplating canonical investigation
(that hoariest of computer applications); it contains very little but
common sense and a short list of references.
</p>
<p>So long as topics such as these remain the norm for introducing the next
generation of scholars to the new tools of their trade, CBC will never
get out of the nineteenth century. The lexicographer and the historical
linguist may well be content to deal simply and solely with the words on
the page, but the critic  has a larger brief. Corns is also aware of this
and by far the best parts of this book are two chapters concerning the
rudiments of database design and usage. What is sadly lacking here is any
awareness of the essential techniques of data analysis which provide an
essential theoretical underpinning to the creation of structured
databases. In its absence, his eminently practical accounts of such
packages as SPSS or the ESTC online catalogue tend to be overwhelmed by
the discouraging obscurity of these particular bits of software. 
</p></div>
      <!--
Here's the one that went missing (JANET didnt lose it, I forgot to
transfer it from the MAC)
--><div xmlns="http://www.tei-c.org/ns/1.0" xml:id="simon">
<head>John J. Simon Jnr :<title> From sand to circuits and other inquiries. </title>
Harvard University Office for Information technology 1986  0-674-32573
</head>

<p>This book collects together and expands on a number of survey-type
articles published in the Harvard <title>Information Technology Newsletter</title>
over the last few years. Half the pieces are dated 1983, which means
that the art whose state they purport to capture has since moved on,
in some cases quite a considerable distance. The chapter on xerography
for example is almost entirely innocent of any reference to
laserprinters, while that entitled 'The author- computer-ttypesetter
interface' boasts one (1) sentence on generalised markup and nothing
at all on postscript. There is a chapter dealing with interactive
video ('The record with a view') but nothing at all on optical storage
systems (CD or WORM).
</p>
<p>There are introductions to the basic principles of most 'information
technologies', and to the mathematical principles underlying binary
logic and imaging systems, together with a discursive chapture on the
industrial and social effects of 'disvestiture' in the US
telecommunications industry (for those with short memories, this is a
polite term for the breaking up of Old Ma Bell's cartel), and several
gee-whizz pages on the growth of computing facilities in US colleges
and universities, the computerisation of library systems etc. The most
recent article (1986) is also the longest: it is a fifty page essay on
the history of artifical intelligence winsomely titled "A long and
winding road". It is derived almost entirely (as indeed is the rest of
the book) from secondary sources, by a process of digestion and
paraphrase, the end result of which is a sort of mindless pap somehow
contriving to be both bland and indigestible.
</p>
<p>This book contains little that could not more easily amd more reliably
be found in any reputable encyclopaedia or comparable reference
work. It might perhaps be useful for students who lack the self
confidence or skill to approach such books directly, or for those
content to operate exclusively at a 'Readers Digest' level of
intellectual activity.
</p>
<trailer>
L. Burnard, University of Oxford
</trailer>
</div>
      <!--
From:	VAX::ARCHIVE      "Oxford Text Archive" 12-DEC-1989 10:27:42.90
To:	LOU
CC:	ARCHIVE
Subj:	

--><div xmlns="http://www.tei-c.org/ns/1.0" xml:id="doheny">
<head><title>Effective Documentation: What we have learned from research</title>.
Edited by Stephen Doheny-Farina. MIT Press, 1988. 354 pp. 
33-75 </head>

<p>It's a comforting thought that something can be learned from
research, even if only how to produce more `effective'
documentation. The papers in this compendium come, for the
most part, from a twilight zone inhabited by `professional
communicators and teachers of communication', whose thankless
task it is to produce those unread manuals and unscanned
online help systems on which we all depend. It arises out of a
perceived gap between the communicators and those engaged in
`communication research', for the most part a hybrid crew of
social scientists, behaviourists and human factors
researchers. Communication is too important to be carried out
by rule of thumb, is the unstated message: its effectiveness
can and should be calibrated by the same sort of statistical
testing procedures as are used in market research. In this way
a new rhetoric, based on sound quantitative procedures, can be
defined. There are detailed discussions of experimental method
(how to set up double-blind experiments, the importance of
proper statistical controls etc.) which would be of use to
anyone embarking on any sort of behavioural research, and
several presentations of applications of this technique as a
means of establishing incontrovertibly in what circumstances a
particular method of presentation is better or worse for
certain purposes. If you want to find research results proving
(or disproving) that windows icons and mice provide better
ways of controlling screen editors than command lines, this
volume and its extensive bibliographies will help. If you are
concerned with stylistic issues, you will find supporting
arguments here for both the view that users of manuals want to
know about topics in depth, and its opposite. Less dubiously,
you will find much support for the (surely by now self
evident) view that people learn better by `doing' than by
rote, or that online and printed presentations of text require
different techniques. The unstated concern that
`communicators' should be regarded as professionals too also
finds expression in a number of papers devoted to the
effective management of the documentation process, and a plea
for its integration with the more glamorous side of software
development. The introduction claims that the book is "one of
a small but earnest group of research-based resources" for
these unsung heroes of the documentation section: small it
isn't, but earnest it certainly is.</p>
</div>
      <!--
From:	VAX::LOU          "Lou Burnard" 12-APR-1989 12:11:01.01
To:	CBS%UK.AC.UCL.CS.NSS::EDU.UCHICAGO.GIDE::mark
CC:	LOU
Subj:	new improved ize review
--><div xmlns="http://www.tei-c.org/ns/1.0" xml:id="ize">
<head>
Software Review: IZE version 1, March 1988. By Persoft Inc (

Lou Burnard, Oxford University Computing Service
</head>

<p>These days it's getting harder and harder to find a really
innovative idea in the creation and management of electronic texts.
Most players are happy to follow the tried and tested route of
stealing the best ideas from the competition and then lumping them
all together. This can turn out disastrously like trying to build
a racehorse by combining the wheels of a Maserati, the engine of
Concorde and the guidance system of a fruit bat. What you get when
you combine a word processor, a free form database system, a text
retrieval engine, an outliner and a hypertext authoring system is
arguably just such a chimera. But IZE has one good and original
idea which might just catch on and which makes it well worth a
second look: using keywords alone to structure, identify and store
texts within a text base.
</p>
<p>IZE comes in a smartly designed box, containing a white package
labelled "Opening your IZE" which has the installation disks etc.
in it, a red package called "FamiliarIZE" which is a tutorial
guide, a blue package called "Using your IZE" which is a reference
manual, and a yellow one called "UtilIZE" which lists available
printers and other mundane matters. (There is also the usual
booklet of last minute updates to the manual, which is headed,
groan groan, "ApologIZE"). I installed IZE on a Compaq DeskPro 386S
with as much memory as DOS 3.3 can handle, but not unfortunately
a colour monitor, so that I was unable to verify my suspicion that
red, blue, yellow and white blocks also cohere to form the logo at
the centre of the package's welcome screen. During the otherwise
unremarkable installation process I was only once perplexed (by
having to decide which 'hot link' drivers I wished to use, before
I had any idea what a hot link driver might be). The package is not
copy-protected and I was not asked to supply a licence number when
installing it. The software alone takes up around 1.5 Mb of disc
space, and needs 380 Kb of memory. It can be installed as a TSR
(RAM-resident) program, if your system has enough spare memory for
the purpose. 
</p>
<p>IZE looks as if it came from, or was designed for, the familiar
Macintosh/Windows interface. It has pull down menus on which
inaccessible choices are signalled in brackets. Arrow keys or
initial letters can be used to select options, and there are built-
in short cut keyboard commands (usually CONTROL plus an alphabetic)
as  alternatives for some, but not all, menu options. This version
of the software does not seem to provide any support for a mouse,
which is rather odd in a package so clearly designed for a WIMPish
environment. For those who dislike scanning through menus, it is
possible to define 'keyboard macros' which allow a single keystroke
to stand for a whole sequence of menu selections. Macro definitions
cannot be edited (you have to retype them until you get them right)
but can be named, grouped into named sets or exported to other IZE
databases. It is also possible to specify a macro to be executed
whenever a particular text is accessed. The richness of this set
of facilities perhaps reflects IZE's parentage in that Persoft is
best known for its extensive range of communications software.
</p>
<p>IZE is used to maintain one or more 'textbases', each composed of
many (up to 32,000) separate texts. Texts can be created using
IZE's own word-processor, or loaded in from other sources. They
need have no common structure, either internally or viewed as a
group: all that IZE requires is that each text have at least one
keyword or other way of linking it with the rest of the textbase.
</p>
<p>The word-processing part of IZE is competent, if unexciting. It has
all the usual features for moving through a text in terms of
characters, words, lines sentences or pages, forwards or backwards;
delete functions for similar units; block moves and deletes, cut
and paste, automatic word wrapping etc. It allows visual attributes
(emboldening, underlining or inverse) to be added to parts of the
text in a fairly simple way and has an adequate range of re-
formatting options to cope with changes in margins or page depths.
Running headers or footers can also be added. Support for non-Roman
alphabets is rudimentary and there is no spell checker or support
for multiple fonts or point sizes. One interesting alternative for
those too addicted to some other wordprocessor to contemplate using
IZE's is to use the 'hotlink' facility referred to above. This
stores in the IZE textbase a surrogate text which when accessed
loads a nominated program (the wordprocessor of your choice) into
memory, while retaining enough of IZE  for you to continue marking
keywords etc. This is a good example of a neat idea made more or
less useless by the hostility of an unfriendly operating system:
I found it impossible to squeeze enough memory out of my system to
load both WordPerfect version 5 and IZE without removing my RAM
disk and reconfiguring my system entirely. The HotLink facility
worked satisfactorily with the Norton Editor - which for all its
other merits is not the word processor of my choice.
</p>
<p>Explicit links between texts may be created independently of the
links implied by the fact that they share keywords: this enables
complex hypertextual structures to be built up quite simply. It is
less easy to view the whole of a structure as IZE provides no way
of browsing such structures short of following each link in turn;
instead you are expected you to browse the text in terms of a
taxonomic arrangement of its keywords, known as an outline.  Links
are point-to-point and typeless. If the text to which a link points
is subsequently deleted, the starting point of the link is
retained; attempting to follow it will generate a warning. If a
backup still exists for the missing text, you can recover it, but
its links will have been lost. If you try to delete a text which
points to another text, and that link is the only access path into
the second text, IZE will also warn you.
</p>
<p>Keywords are associated with a text in a number of different ways.
You can simply hit F4 when the cursor is over a word, or CTRL-F4
when over a highlighted block of words (but note that hyphenated
words are treated differently in these two situations). You can hit
ALT-F4 to open a scrolling window in which all the keywords so far
defined are displayed, which can be added to or modified ad lib.
You can use a keychanger (see below) to add keywords to a file as
it is imported and to control the keywords you add yourself.
Keywords can be up to 64 characters long, and there is no
documented limit on the number you can use for a single text. There
are no limitations on what counts as a valid keyword (other than
those mentioned above), so it is usually advisable to have a filter
(see below) in place to prevent high frequency words such as 'to'
or 'the' dominating the keyword list, when this is being created
on import. I found it a little surprising that no default stop word
list is provided.
</p>
<p>Because all IZE links are typeless, there is no way of categorising
the keywords used to index a text. I found this quite an
irritation: in a textbase of electronic mail messages (from
HUMANIST, of course) I wanted to add keywords relating to the
sender and date of the message and also its subject, and to
manipulate these independently. The only (and rather
unsatisfactory) way I found of doing this was to use a filter (see
below) to include or exclude specified  keywords. IZE is no
substitute for a database system, though it would provide an
interesting front end for one.
</p>
<p>A keychanger is a named process to be performed when a text is
saved, when it is first imported from elsewhere into IZE, or
optionally at any other time. A default keychanger can be nominated
for use on import and on saving. Each keychanger can add a
specified list of terms, unconditionally or only if they appear in
the text, or can remove a specified list of keys already associated
with the text. It can also mark as keywords any terms appearing
between a pair of specified delimiters. Fuzzy-matching  cannot be
used to specify the inclusion or exclusion list associated with a
keychanger, and only one set of delimiting strings can be used.
</p>
<p>A filter is a way of specifying that some keywords are equivalent
for searching purposes, or that others should be ignored when
keywords are grouped into outlines.  As with keychangers, the fact
that only one filter can be active at a time somewhat reduces the
usefulness of this facility. Fuzzy matching cannot be used to
specify a range of keywords to be ignored in this way.
</p>
<p>To search for texts, you hit F9 and then enter a search expression.
At its simplest, this is a single word, but it can be a complex
Boolean expression containing keywords,  free text strings in
quotation marks, or fuzzy-matching terms. The Boolean operators 
AND OR and NOT are used, with parentheses to resolve ambiguity. As,
however, operators are not distinguished from keywords, the
keywords 'and' 'or' and 'not' cannot be searched for correctly,
though they can all be stored. Whatever part of IZE parses search
expressions can easily be confused by requesting such things as
'and and and' (which finds anything) or 'and or or' (which is
syntactically invalid). Free text searching is a little slow, and
the parser makes no attempt to optimise it: each part of a search
expression containing a term in quotes requires a separate painful
trawl through the entire textbase. A search expression can also
contain pre-defined macros, identified by a dollar sign as their
first character. Search macros can be nested but not recursive
(though the program does not detect this until you attempt to
execute one). And finally, search expressions can refer to the date
a document was last modified. There is no way of restricting
searches to particular parts of a text, 'fields' as it were, or
restricting the range within text strings must co-occur. If some
keywords identify the place from where a mail message comes and
others its subject, there is no easy way of distinguishing texts
about London from texts from London. But IZE does not claim to be
a database management system nor a full-blown free text retrieval
system like BRS/Search or even WordCruncher. Its searching
facilities are there primarily to assist in the identification of
the keyword strings which provide the only organising component of
the textbase.
</p>
<p>Fuzzy-matching of keywords is done by prefixing the substring to
be searched for with an ampersand. The exact algorithm used is not
defined in the documentation, but seems to involve more than a
simple stem-match, with both left and right truncation. A search
for the keyword <code>&amp;ing</code> will be expanded to match not only all
keywords which either begin or end with the string 'ing' (thus
picking up both 'ingenious' and 'string') or which contain it (thus
picking up 'Seringapatam'), but also those which have only a few
characters distinguishing them from it, (thus also picking up
'again' and 'in'). There is apparently no way of saying that only
left (or only right) hand truncation is required, but the expansion
of a fuzzy match term is available for editing before the search
is performed. 
</p>
<p>String matching is always case-insensitive and also ignores some
non-alphabetic characters such as apostrophes or hyphens. Other
punctuation characters are treated as distinct alphabetics, so
keywords <code>tea-pot tea_pot tea&lt;po&gt;t</code> and <code>tea'pot</code> are all permitted
(and would be found by a search for <code>&amp;teapot</code> &#x2014; but not by a search
for <code>teapot</code>). These same characters are however ignored if they
occur at the start or end of keywords: '-teapot' is indexed under
'teapot'. Currency symbols are treated as punctuation of this kind,
but not (for no apparent reason) brackets or parentheses:
'Tea(po)t' or 'tea[po]t' are simply not permitted as keywords.
According to the manual, spaces can be included in keywords
('Desktop publishing' as a single term for instance) by typing
CTRL-SPACEBAR; I was unable to get either this, or the alternative
proposed by the manual, to work on my copy of the software.
Characters from the top end of the ASCII character set (notably
accented letters)  can be entered, using the ALT+numeric keypad
method for example, and they are preserved on import. Fuzzy-
matching will usually find both accented and unaccented forms of
a word, where these differ in only one or two characters.
</p>
<p>For most users, there can be little doubt that the convenience of
a single convention for fuzzy-matching which seems to meet most
requirements of the kind 'give me words like "Smith"' will entirely
outweigh the complaints of specialists who want to see all and only
texts with keywords ending with the string "ation". Nevertheless,
I feel that some precise description of the matching algorithm used
should have been included somewhere in the documentation, as also
of the character usage. It also seems rather strange that fuzzy
matching should be possible only on keywords: the free text
searching capabilities of IZE are severely constrained by the fact
that no sort of wild-card is permitted and all matching must be
exact. The other major limitation of the search facility is the
absence of any non-string searching options: there is a date search
but no means of comparing numerical values at all.
</p>
<p>Clearly, none of this is particularly novel. What distinguishes IZE
from many other systems is that the result of a search is always
structured not as a simple list of documents ordered alphabetically
or in some other mechanical way, but as an outline based on all the
keywords in the retrieved texts. For those unfamiliar with the
concept, an outline is a hierarchic arrangement of keywords, like
the following:
<eg><![CDATA[
	I.	Database software
		A.	Relational
			1. SQL/DS
			2. Ingres
			3. Oracle
			4. Others
		B.	Network
			1. IMS
			2. IDMS
			3. Others
		C.	Textual
			1. Indexed
			2. Unindexed
			3. Hypertextual
	II.  Database design
		...
]]></eg>	</p>
<p>By default, IZE constructs such hierarchies by looking for co-
occurring keywords. Texts using the keywords 'Database software',
'Network' and 'IDMS' will all be grouped together under I.B.2 in
the hierarchy.  If every text in the database uses some term 'A',
that term will be at the top of the hierarchy. At the bottom will
be all the terms which appear in only one text. In between will be
all the terms which appear in more than one text. The assumption
behind IZE (perhaps a debatable one) is that the more keywords
texts share, the more probable it is that they should be grouped
together. Each term in an outline can only appear at one point in
the hierarchy, though of course the number of hierarchies that can
be created is enormous.
</p>
<p>The outline returned by a query can be browsed using arrow keys in
an intuitively obvious way. As you move down the tree (from I to
A to 1 in my example above) new outlines are created automatically,
until you reach the level at which only text pointers (terminal
nodes) exist. Groups or subgroups of texts can also be browsed,
printed, locked, unlocked or deleted. As a retrieval tool, it must
be stressed, an IZE outline is only as effective as the keywords
used to create it. If the keywords assigned to documents are
ambiguous or incomplete (in that they don't correctly characterise
the text at every hierarchic level of description required for a
particular outline, for example) then clearly the outline produced
will be fairly useless.
</p>
<p>Two tools are provided to make the ways in which keywords are
assembled into outlines at once more flexible and more powerful:
guidelines and filters. A guideline can be thought of as a template
for creating an outline. It looks very like an outline, though it
cannot be directly created from one, which is a nuisance. It may
alternatively be thought of as a graphic representation for a
Boolean expression limiting both the range of a query and the
outline it is designed to return. Thus, a guideline like the one
above could also be thought of as a search expression
<eg><![CDATA[
	(Database Software or Database Design) and (relational or
network or textual) ...
]]></eg>
into which the texts that match are to be slotted. The cursor keys
are used to step through the tree structure selecting different
parts of this complex expression. Guidelines are named, created,
edited and removed in much the same way as other objects, though
there are some differences; for example, moving parts of a
guideline around within it is simpler than doing a cut-and-paste
operation, though that is also possible.
</p>
<p>A guideline can also be used as a template for new texts: by
selecting a part of the guideline and pressing ALT-F5, you can
create a new empty text with the keywords indicated by that part
of the hierarchy already attached. The content of new texts can
also be controlled by things called templates, which allow you to
require for example that names and addresses are always added to
texts at a certain point.
</p>
<p>Texts created in IZE can be exported to other systems, either in
straightforward ASCII format, with no formatting codes at all, or
in a special 'holding' format from which they can be loaded
directly into a different IZE database. Texts can also be printed,
using a variety of options to specify page headers, layout etc.
Unfortunately, when files are exported to ASCII (and even when they
are printed) the keywords associated with them are lost, which I
regard as a serious defect in the system. It is supposedly possible
to include keywords in the page heading when texts are printed out,
but I was unable to get even this facility to work. A utility
program is provided which unloads all the keywords in a textbase
to an ASCII file, which can then be used for example as the basis
of a keychanger for a new textbase, to enforce keyword consistency
between textbases. 
</p>
<p>Like many ambitious programs, IZE is a little fragile. I found it
quite easy to lock the keyboard up in a state where only a warm
boot would get things moving again. So far as I can tell, this was
usually caused by memory shortages, for example when trying to load
WordPerfect 5, or after repeatedly loading new images of DOS into
memory. Fortunately, the program does its best to protect you from
the consequences of its running into a brick wall by itself
automatically saving changed texts in a temporary file at regular
intervals while it is running, and also by keeping backup copies
of altered copies as they were before the most recent change. These
two features can be disabled, independently, for those who like to
live dangerously, or who find the performance penalty they imply
unacceptable. I was tempted myself to disable at least the backup
option when I noticed the difference in size between an IZE
textbase and the raw ASCII texts from which it had been made: a
factor of two or three seemed normal, especially if texts were
repeatedly changed. A Regenerate option is available, which
compacts the textbase by deleting deadwood and (presumably)
rebalancing index trees; its regular use would seem essential in
a live system. Performance can also be improved by using either
expanded memory or a RAM disk for the disk swap file IZE uses when
the number of texts currently held in memory gets too large. Even
on a Compaq 386S, no slouch when it comes to disk access, the
amount of time IZE takes to rewrite a section of its textbase is
noticeable, but I could find no obvious way of improving this, and
the manual gives no information on whether (for example) increasing
the number or size of I/O buffers available to DOS would help the
situation. 
</p>
<p>The inability of conventional record or relation based database
management systems to cope with large quantities of seemingly
unstructured text has long been recognised. In reacting against the
discipline imposed by such systems, text retrieval packages have
often been unable to offer satisfactory ways of representing the
structure of the information carried by text, other than by ever
more complex indexing schemes. Yet for many collections of text it
is both possible and natural to wish to create descriptive
taxonomies that enable that information to be accessed in a
structured way. IZE seems to be setting a new trend in information
management systems in its use of guidelines as an organising
principle, which makes it well worth a second look.
</p>
<list>
<item>  Title/Name of the product: IZE
  Version, update number 1.0b (Dec 1988)

  Copy protection (yes/no) No

  System requirements : Needs approx 380 Kb memory and at least 1
Mb free disk space

  Operating System : MS/DOS; OS/2

  Producer's name   (UK) 
				(US)Persoft 
          
   Address and phone number for placing orders
			(US) 465 Science Drive, Madison WI 53711
				tel. (608) 273-6000
			(UK)	Softsel House, Syon Gate Way, Great West Road,
                    Brentford TW8 9DD
				tel. 01-568 8866
   List Price  (UK) &#xA3;300 
			(US) ?

   Notes ( importance)
			Needs a hard disk.
   Equipment or software of particular importance used in
           the evaluation ()
	

   Biographical information ( function).

	Lou Burnard is a consultant at Oxford University Computing
Service, where his responsibilities include the Oxford Text Archive
and the development of textbase services. 
</item></list>




</div>
      <div xmlns="http://www.tei-c.org/ns/1.0" xml:id="ennals">
<head>RICHARD ENNALS and JEAN-CLAUDE GARDIN, eds. <title>Interpretation in the
humanities: perspectives from artificial intelligence</title>.
(Library and information research report 71, ISSN 02673-
1709; 71) Boston Spa: British Library Board, 1990. xv, 367
pp. &#xA3;25. ISBN 0-7123-3186-7.
</head>

<p>This is a very unusual book, deriving from a rather unusual
occasion, though of a type which, it is to be hoped, recent
successfully completed engineering works beneath the Channel
will increasingly facilitate. It represents a genuinely
collaborative effort, involving practitioners of disciplines
as seemingly far apart as literary theory, prosopography,
philosophy, computer science and law, and cultural habits as
distinct as Anglo-saxon empiricism and Gallic theory. Its
subject is what should be at the heart of all the
humanities: interpretation, that hermeneutical process which
distinguishes true scholarship from academic scribbling. As
a means of throwing some light on just what that process
might involve, each of the contributors to this volume
addresses the question of <q>computability</q>, that is, the
extent to which the process of interpretation in a given
field can be re-expressed in computational terms, without
losing sight of creativity, that quasi-mystical process by
which cognition and understanding come about. To what extent
can literary, historical, philosophical or legal analyses be
simulated in formal terms? Is there an inherent
contradiction between the attempts which expert systems make
to formalise the interpretative process and the stress which
most of the humanities have traditionally placed on
multiplicity of meaning, on the importance of ambiguity? </p>
<p>
The volume contains fifteen papers, four of them in French,
with an English abstract, and the remainder in English, with
a French abstract, together with a brief epilogue by the two
editors. The papers derive from a Workshop jointly funded by
the British Library and the French Government department
responsible for museums and libraries (the Direction des
Biblioth&#xE8;ques, des Mus&#xE9;es et de l'Information Scientifique),
organised by Dr May Katzen of the British Library's Office
for Humanities Communication and held at Cawthorpe House in
September 1988. Much of the keen intellectual pleasure the
invited participants must have derived from this latter day
symposium, as well as perhaps some of their occasional
mutual bafflement, is well preserved in the edited texts,
each of which deserves more careful and detailed analysis
than this brief review can give them.  </p>
<p>
The volume begins with an introductory section, in which
Margaret Boden presents an accessible but inevitably
somewhat skimpy presentation of the nature and goals of
Artificial Intelligence itself, Jean-Claude Gardin (whose
distinguished aegis hangs over much of the volume) considers
its applicability to the human sciences, and Richard Ennals
places its insights in a historiographical context. Gardin's
paper asks awkward questions in a characteristically
charming way:  what defensible basis can be found for
privileging (i.e. funding) some sorts of interpretation by
some kinds of people over others?  Referring to Shennan's
paper in the same volume, which argues that archaeological
interpretations would be better based on formal reasoning
than on the interpretive constructs of archaeologists, since
the latter are inevitably influenced by their own
culturally-determined interests, Gardin sketches an account
of his own <q>logicist</q> approach to this fundamental
epistemological problem, including several useful references
for further reading. He concludes with an incongruously
Thatcherite attack on the notion of <q>professionalism</q> as a
means of validating scientific interpretation -- perhaps a
result of the striking absence of the <q>cult of the amateur</q>
in his own culture. Ennals rehearses a similar argument in
favour of formal reasoning, which for him is what good
historians have already been doing unawares (rather as M.
Jourdain had been speaking French), if they have taken to
heart the lessons of Collingwood, Wittgenstein and other
theoreticians of language. His major concern is with <q>cross-
cultural</q> fertilisation of ideas, (not only across the
channel but also across disciplines), and with a view of
interpretation as code-breaking: as such, he risks
exemplifying the well-meaning amateur, as witness also his
quirky fascination with the ways of Westminster. </p> 
<p>
The second part of the book consists of a number of well-
focused papers on applications of Artificial Intelligence
techniques in three specific subject areas: archaeology,
history and letters. </p>
<p>
The papers of Stephen Shennan, Henri-Paul Francfort and John
Wilcock present good accounts of the achievements so far of
the <q>logicist</q> approach in archaeology, Wilcock's overview
being particularly accessible for the novice. The section on
history contains a bravura performance in which Jim Doran
assesses the ability of <q>distributed artificial
intelligence</q> to represent the complex semiotic models
developed by Todorov in his magisterial <q>Conquest of
America</q>; perhaps surprisingly, both semiotics and AI are
shown to profit from this unlikely encounter, which is also
certainly one of the most intellectually impressive in the
book. Kevin Schurer's piece on possible applications of AI
in historical demography seems initially rather pedestrian
and inconsequential by comparison. Nevertheless, it is
quietly subversive of some of the other articles in this
work in its opposition to logical positivism and its stress
on the practicalities of computing in history today, and
repays careful study. </p>
<p>
David Miall's paper discusses reader response theory, the
basic notions of which also seem somewhat removed from
logicist modelling. Noting that what characterizes a
literary text is its ability <q>simultaneously to establish a
norm and to subvert it</q>; Miall uses statistical techniques
borrowed from behavioural science to measure the <q>affect</q> of
such texts rather than any underlying formal model.  Some
more orthodox applications for AI are briefly described in
the same section by Danielle Bourcier, who considers their
very real potential in simplifying and making more effective
local administration and law.</p>
<p>
The final section of the book contains five comparatively
technical papers drawn from the disciplines which in some
sense underlie artificial intelligence -- linguistics,
logic, computer science and philosophy. Michel Charolles is
not well served by the three page summary of his 33 page
original, which includes a good summary of the emergence and
history of discourse analysis, even handed criticism of the
notions of text grammars and a discussion of why logicist
analysis has nothing to offer it. This must be read in the
original French, if only for the sake of the example
sentences; it provides a persuasive instance of a kind of
interpretation (the establishment of coherence in a
discourse) which does not seem to be susceptible to
formalisation by logicist rules. Jean Fargues fleshes out
the introduction to AI provided by Margaret Boden, with some
more specific discussion of its application in work towards
language-understanding systems. Peter Stockinger provides a
more detailed discussion of logicist analysis, from the
perspective of formal logic. Judith Richards argues,
briefly, for the importance of the so-called <q>strong</q> AI
approach, and the possibility of rediscovering its radical
potential which the direct modelling of human reasoning (or
<q>qualitative simulation</q>) offers. Finally, in one of the
longest, most thoughtful and thought provoking pieces of the
volume, Pierre Jacob gives a philosophical perspective on
the issue of what exactly <q>interpretation</q> is,
distinguishing at the linguistic level interpretation as
grammatical decoding from the kind of process defined by
Grice as <q>implicature</q>. This second kind of interpretation
is then related to cognition and intentionality, as a
prelude to a discussion of the distinction between
interpretation and explanation, specifically scientific
explanation.</p>
<p>
A useful concluding summary by the two editors underlines
the overall effect of this unusual collection: from whatever
theoretical or pragmatic standpoint one approaches it, both
computability and creativity are necessary <q>engines of
knowledge</q> in both the arts and the sciences. This volume
provides a vivid testimony to the effectiveness of
conjoining rather than opposing them. </p>
<trailer>
                                                 LOU BURNARD
                         Oxford University Computing Service
</trailer>
</div>
      <!--
From:	OXVAX::LOU          "Lou Burnard" 14-JAN-1991 10:10:31.68
To:	CBS%UK.AC.CARDIFF.COMPUTING-MATHS::RALPH
CC:	LOU
Subj:	RE: Book Review

Review to go under heading Software.
Programming Linguistics, by D. Gelernter, S. Jagannathan
(MIT Press, 1990). 
Price #33.75 h/b; ISBN 0262071274; xx+411 pp.
--><div xmlns="http://www.tei-c.org/ns/1.0" xml:id="gelernter">
<head>David Gelernter and Suresh Jagannathan <title>Programming Linguistics</title>.
Cambridge, Ma.: MIT Press 1990 </head>
<p>
The punning title of this book is only the first of the many
small pleasures to be found in it by anyone who maintains in
parallel interests in language, in programming and in programming
languages &#x2014; three very different topics, which only occasionally
overlap. For this is also that rarity, a readable textbook. It
can simply be used as the basis for a course on the historical
development of programming languages, from Fortran to Occam. It
can also be used as background reading for a course on compiler
construction, as a way of providing a richer  understanding of
the varieties of formal systems which modern compilers must
implement. But its strength lies more in the way in which these
essentially technological concerns are placed in a broader, more
humanistic, perspective. Its authors have chosen to tackle head
on the question of what <q>computer science</q> as an academic
discipline is fundamentally about, relating their analysis of the
evolution of modern programming systems to more fundamental
concerns about design and about what sort of a machine a computer
program is. Though presenting itself as history, the book has
clear ideological designs upon us &#x2014; which it is however honest
enough to declare. 
</p>
<p>The authors begin by defining an <q>ideal software machine</q>, an
abstraction from the reality of a running program, expressed as
an intersecting <q>space map</q> and <q>time map</q>, showing how the
constituent entities of any program can thus be described. Unlike
mathematical or declarative models of what a program is, the ISM
unifies in a simple way the opposition between program structures
and data structures. The discovery, erection into dogma, and
subsequent collapse of this opposition, termed here the <q>Algol
Wall</q> is described as possibly the major event in the evolution
of programming languages. </p>
<p>
To chart that evolution, the authors begin with a section dealing
in some detail with Fortran, Algol60, and Lisp, which they
characterise as the first of the <q>hero languages</q> which
significantly advanced the art or craft or science of
programming. In each case, enough of the language is described to
make clear its principle features without encumbering the reader
with over much detail. Useful pointers to further reading are
also provided, most of which are original research papers rather
than second or third hand digestions of them. Subsequent major
sections introduce three other heroes:  Pascal; Simula67 and
Scheme. Honourable mentions are accorded to other languages
insofaras they are perceived as having addressed key blind spots
in the Algol world-view: Cobol, PL/I and Algol68 for their
identification of data structures and user-defined types;
Smalltalk and Ada for their contributions to the object-oriented
juggernaut which finally overthrew the Algol Wall. The book
concludes with a section on two typical declarative languages
(Miranda and Prolog), and another on concurrent languages,
specifically Occam and Linda. For light relief, there are two
pleasantly idiosyncratic chapters, apparently intended as
starting points for general discussion: one addresses the
question of what <q>complexity</q> means in a design, but at a very
superficial level; the other, surely guaranteed to break the ice
at computer science departmental parties, suggests that some
aspects of software engineering have more to do with ideology
than engineering.</p>
<p>
Inevitably, a book with so clearly stated an agenda &#x2014; to present
a particular interpretation of the past as a way of justifying or
evaluating current concerns &#x2014; runs the risk of offending those
whose interpretations of the past (or the present) differ.
Certainly, those who feel that programming is best regarded as a
form of mathematics or logic will find the marginalisation of
declarative languages in this book at best ill-judged, at worst
pernicious. There are also some curious omissions: near-heroic
status is accorded to Cobol for its discovery of data structures,
but no consideration at all is given to subsequent developments
in database languages; Snobol's CODE function anticipated by
several years the ability of a Simula67 program to define
procedures and data structures isomorphically, while its stress
on pattern matching as a fundamental operation surely deserves
some mention, as does the generator concept of Icon. More curious
still is the presentation of Algol68 &#x2014; surely the apotheosis of
structural elegance &#x2014; as essentially equivalent in needless
complexity to PL/I.  And there is no doubt that the chattiness
and informality of some of this text will serve only to irritate
those teachers who do not wish to do their job wearing (as it
were) jeans and humorously inscribed sweatshirts. </p>
<p>
Nevertheless, the book can be confidently recommended as a means
of stimulating interest in an undeservedly neglected area of
computer science, as a reliable source of information (at least
for the topics it addresses) and as a thoroughly readable and
provocative analysis of what programming is, and where it is
going.</p>
<trailer>
Lou Burnard
Oxford University Computing Service
</trailer>
</div>
      <div xmlns="http://www.tei-c.org/ns/1.0" xml:id="lancs">
<head>I. Lancashire and W. McCarty <title>The Humanities Computing
Yearbook</title>. Clarendon Press, oxford, 1988.ISBN O-19-824442-
8.396pp.&#xA3;4O.
</head>

<p>In 1989 it is no longer possible to question the desirability,
still less the existence, of 'humanities computing' as an academic
discipline. At centres throughout the world new posts are being
funded, as new journals flourish and new generations of students find
new skills are expected of them. The computer wielding linguist,
historian or textual critic is no longer pardonably eccentric; his/her
method has been canonised. It was only a matter of time before that
canon should be authoritatively documented, and this volume represents
the first and, to date, most successful attempt at that process. True,
there have been many no less worthy attempts to summarise
comprehensively the state of this particular art (for example, Hockey
and Oakman in 198O or Hughes in 1987) but these were all deliberately
evangelical works, whose authors were perhaps a little parochial in
their enthusiasms, a little uncritical in their acceptance of all
things computational. As much hy its format as hy its provenance, this
volume asserts that 'humanities computing' no longer needs to
proselytise, is no 1onger a coterie pursuit, and hence need not fear
rigorous examination or selfcriticism. It also clearly marks out a
turf and an agenda which may not perhaps exactly coincide with others'
definitions either of 'humanities' or of 'computing'.
</p>
<p>'Computing' (asserts the preface) 'is the "how-to" of
any discipline made obvious', hefore going on to specify
three 'kinds of methodology' for which computing seems
appropriate. These are storage and information retrieval;
statistical description; and linguistico-semantic analysis.
The relevance and success of computing methods in at least
the first two categories is not controversial, though the
acceptability of (for example) computer-based content
analysis, propositional logic or even computer simulation as
a means of adequately describing the reading of texts is
rather more debatable. It could he argued that computing
methodologies in fact distort the true nature of a
discipline, hy focusing attention in only those areas
easily susceptible of computerisation, which are not
necessarily those central to it. That is not a concern of
the editors of this volume, for whom the discipline is only
and exactly those aspects of it in which available computing
methods can he shown to he productive. For the rest, one
suspects, they feel like Jowett that 'what I don't know
isn't knowledge'.
</p>
<p>Some readers may he surprised to discover that 'the
humanities' include art history hut not art or design;
philosophy, hut not law, politics or economics; and 'folklore
studies' hut not ethnology or anthropology. The amount of
space and the number of subdivisions allocated to what
remains is also indicative of a very strong bias to
linguistics, literature and history at the expense of music
or archaeology. The editors claim no more for their taxonomy
than that it follows from their 'fieldwork' in assessing
what 'our colleagues have done and are currently doing' and
from their own view that 'most computational work is
methodological'; one might reasonably ask how they define
their colleagues.
</p>
<p>The volume is large, containing nearly 4OO closely &#x2014;
and very well &#x2014; typeset pages. Most of its bulk is taken up hy
a mixture of helpfully annotated bibliographic citations and
brief details of available software or similar resources,
grouped by topic or discipline. For the most part (though by
no means exclusively) the bibliographic citations are to
articles published in interdisciplinary journals and
conference proceedings, rather than from mainstream
discipline-specific journals, which (presumably) the editors
assume are already accessible to this volume's likely
readership. Since one highly desirable effect of the maturity
of 'humanities computing' must be its acceptance by the mainstream,
one must question the wisdom (if not the practicality) of
this assumption. Brief notes are attached to about
threequarters of the citations &#x2014; an exceptionally helpful
feature of this volume. The software and other resources
cited form a very mixed bag, ranging all the way from 'a
generator of science fiction written in BASIC level 11 for a
Radio Shack TRS-8O' up to detailed reports on the INGRES
database management system. In most cases, developers,
suppliers and references to fuller published descriptions are
given. Inevitably many of these were out of date on the
book's publication, if not before, such is the rate of change
within the industry; nevertheless, the range and diversity of
software now available is well indicated, and has the
same sort of cumulatively dizzying effect as did the Software
Fair from which this part of the Yearbook had its origins.
If you want to know where you might find hieroglyphic fonts
for the Apple Macintosh, a Modula-2 compiler for MS-DOS, or
just a variety of Writers Tools for teaching 'composition
skills', this is the place to look.
</p>
<p>There are also about sixty extended descriptions of
software or resources felt to be of sufficiently general
interest or major importance within particular fields. In
such cases any editor must tread gingerly the boundary
between advertising hype and detailed evaluation; the
selection of which software deserves such trentment is also
invariably contentious. Thus, for example, in the Historical
Studies section the only product so singled out is the prize-
winning 'Would be Gentleman', a historical simulation of
seventeenth-century French social mobility; less
eccentrically, in the section on Classics, the Thesaurus
Linguae Graecae rates detailed attention as (however) do
three assorted micro-based systems for drilling Latin grammar
into computer-wise students.
</p>
<p>The book's twenty-eight unevenly-sized major sections
are arranged alphabetically. Some sections are highly
discipline-specific, while others are on topics of general
interest. By far the largest section (8Opp.) is that on
Languages and Literature, closely followed by two other
related sections Text Analysis (33pp.) and Computational
Linguistics (31pp.). Sections primarily of relevance to those
with a linguistic or literary bent amount to about over half
of the total. Sections of general interest (Hardware and
Software Resources, Database systems, Bibliographic
resources, People and Places, Programming Languages and
Statistics) together amount to about a quarter. That leaves a
total of about 5O pages for the also-rans, chief of which is
Historical Studies (15pp.), followed by Philosophy (lOpp.),
Music (6pp.) Archaeology (Spp.), Art History (3pp.), with
Drama and Folklore getting a page each. Where a
particular reference might be considered either of
general interest or discipline specific, the latter generally
takes priority. Thus Manfred Thaller's Kleio database system
is discussed in the Historical Studies section rather than
the database section; on the other hand, the Toronto old
English Dictionary is discussed in the general section on
lexicography rather than the section on old English.
</p>
<p>Medieval Studies get a section on their own, whereas
the study of all other historical periods is subdivided by
place, from I5.1 Austria to 15.15 Yugoslavia. Apart from
being of little use to one searching guidance on, say,
computer-aided work done in historiography, demography or
urban history per se, this method of division gives a false
impression of the coverage in this section, which is
reasonably thorough, if not as complete as more specialised
bibliographies such as those published hy the Cologne Centre
for Historical Social Rescarch; although the journal
Historical Social Research is cited in the reference
hibliography at the start of the volume, it does not seem to
have been as well abstracted as (for example) the
publications of the Association for History and Computing,
even in the section on German History. Equally conspicuous by
their absence are Historical Methods and the Journal of
Interdisciplinary Studies. Computational historians need to
confront the epistemological issues raised in such journals
and by such major works as Barzun's Clio and the Doctors or
Fogel &amp; Engelman's Road to the Past, neither of which is
mentioned here. Supplemented in this way, and preferably
reorganised chronologically rather than topologically,
this section would provide an excellent set of pointers to
the current state of historical comptlting, at once thorough-
going and non-partisan.
</p>
<p>The book's taxonomic eccentricities would not matter,
of course, it it were adequately cross-referenced or indexed.
Regrettably however there is no overall alphahetical index,
even to authors cited. There is a 16-page reference
bibliography, in itself an impressive catalogue of resources,
and an alphabetical index of software and software
developers, rather marred by some niggling inconsistencies in
such matters as abbreviations and cross-references. OCP is
indexed under OCP with a cross-reference from 'Oxford
Concordance Program', but TLG is indexed under 'Thesaurus
Linguae Graccae' only. There are two columns of references to
universities, alphabetised under'University of. . .' (which
is silly), and while Project Pallas is cross-referenced from
that for Exeter, the entry for DISH stands proud of any
association with the University of Glasgow.
</p>
<p>The most significant shortcoming of the Yearbook is
the lack of any single alphabetical list of authors or
projects cited. In its absence, the reader must guess into
which section likely citations will have been placed,
and then search the alphabetical lists of references and
resources in that section for useful items. This means that
it is easier to find, say, all the software published by
Microsoft than all the articles written by, say, Martindale
(except where he is cited as developer of software) &#x2014; which
surely cannot be right. It provides needless aggravation
for those who do not share the editors' view of the
structure of their field, or are simply unaware of the name
used for it. In this volume, for example, 'Second language
instruction' is a distinct field from 'English as a second
language', while 'Creative Writing' is concerned with
interactive fiction and poetry generation, not to be confused
with the 'Text Generation' which is a subclass of
Computational Linguistics, nor in any way associated with
'Editing and Publishing'. Of course, establishing a subject
classification for an emergent discipline is no trivial task
and the editors deserve every credit for having attempted it
(I find particularly felicitous their having reserved a
section called'Visionary Devices'under'Computing Resources');
however, until there is general agreement on the boundaries
and internal divisions of the Yearbook, the absence of a
proper index looks suspiciously like preemptive action,
fossilising the potential of this new discipline at the point
of take-off.
</p>
<p>Nevertheless, this volume is to be heartily recommended. There is no
comparable publication, either in depth and breadth of coverage or
overall accuracy. As a snapshot of the current ferment of activity in
scholarly applications of information technology, and within its
selfimposed limits, it has no equal and no research library is
complete without it.
</p><trailer>
Lou Burnard Oxford University Computing Service
</trailer></div>
      <!--
From:	OXVAX::LOU          "Lou Burnard" 14-JAN-1991 10:12:27.91
To:	CBS%UK.AC.CARDIFF.COMPUTING-MATHS::RALPH
CC:	LOU
Subj:	RE: Book Review

Review to go under heading Computing Milieux.
--><div xmlns="http://www.tei-c.org/ns/1.0" xml:id="tzonis">
<head>
Hermes and the Golden Thinking Machine, by A. Tzonis
(MIT Press, 1990). 
Price #14.95 h/b; ISBN 0262200767; ix+284 pp.
</head>
<p>This book is almost as irritating as it is possible for a first
novel to be, without actually being printed on poison ivy. It
purports to be a novel of ideas, purporting to be a detective
story, in which the protagonist, despite a state of the art
laptop and a conveniently eager protegee, fails apparently to
solve a mysterious crime. There is much atmospheric rendition of
various unsavoury and variably pretentious intellectual
characters drawn from the murky world of archaeology and
antiquarianism, and quite a bit of purple prose describing the
weather of contemporary Athens. The hero begins the novel
apparently suffering from jetlag, and never seems to fully
recover from it, which, together with his overwhelming world-
weariness and sense of his own intellectual superiority does
little to encourage the reader into making the imaginative leap
necessary for even a `wonderful send up of deconstructionism'
such as the blurb proclaims the work to be. There is a great deal
of indubitably clever talk about the nature of mental processes,
about epistemological concerns, about the capabilities and
limitations of artificial hermeneutics (the hero's name is
Hermes, geddit?), as well as a few bloody murders. Some useful
diagrams introduce us to various AI formalisms and there is a
bibliography, largely of secondary sources from which partially
digested ideas concerning computer models of cognition, vision
analysis, semantic nets and frames, analogical reasoning and
interpretation appear to have been taken. All this, however, is
not enough to make a novel, or even to poke fun at the notion of
the novel itself, in the absence of narrative. Plots there are,
in abundance, but they collapse into self contradiction as soon
as they are seized on.  At the centre of the book, both
physically and conceptually is a confrontation between two views
of literature - as a hermeneutic exercise or "zero-knowledge
game" and as a way of representing intelligence in action. The
writer would probably wish to be identified with the latter point
of view; regrettably this reader at least places him in the
former camp. </p>
<trailer>
Lou Burnard
Oxford University Computing Service
</trailer>
</div>
      <div xmlns="http://www.tei-c.org/ns/1.0" xml:id="hockeyIde"><head>Review: Research in Humanities Computing 4 &amp; 5
Lou Burnard
    Humanities Computing Unit
    University of Oxford
    lou.burnard@oucs.ox.ac.uk</head>
<head>
Selected papers from the ALLC/ACH Conferences 1992 (eds. S. Hockey &amp; N.
Ide) &amp; 1995 (ed. Giorgio Perissinotto). Clarendon Press, Oxford 1996.</head>   
<p>
These two volumes, the latest to appear in OUP's somewhat dilatory series of
publications on Research in Humanities Computing, under the general editorships
of Susan Hockey and Nancy Ide, offer the reader an interesting opportunity to
review activities in this field over the last five years, and thus to compare
progress on a rather longer time scale than usual. The first collects 14 papers
from the ACH-ALLC conference held in Oxford in 1992, while the second presents a
further 15, from the ACH-ALLC conference held in Santa Barbara, some three years
later in 1995. Both bear a publication date of 1996.
</p>
<p>
In 1992, the field was still dominated by literary statistics and quantitative
methods. The papers by Burrows, Lancashire, Lessard and Hamm, and Opas might
have been written at almost any time since the beginnings of humanities
computing in the seventies: their dense tables of statistics, graphs, and
diagrams bear witness to the continued vitality of a tradition that two decades
of scepticism have not yet succeeded in dislodging. Whether their topic is
language variation amongst writers of different nationalities (Burrows),
automatic identification of phrasal collocations within a specific historic
genre (Lancashire), or author (Lessard and Hamm), or multivariate statistical
analysis of an author's style (Opas), these authors have all worked hard at the
difficult problem of making mathematically-based reasoning accessible to a
largely innumerate readership, and at clarifying their own methodological stance
within an academically-respectable tradition. Whether any of the work can be
said to have produced impressive new ways of reading, or provided anything more
than confirmation of conclusions already reached by other methods, remain open
questions. 
</p>
<p>
In 1995 quantitative methods remain an important, almost an institutionalized,
part of the field. Impeccable papers by Baayen, Frischer et al., and Tweedie et
al. (the latter is cited as co-author in three of the papers in this volume:
surely some kind of record) focus on ways of making sense out of the reams of
statistical data which concordancers and the like throw up. Baayen and the
Frischer team make distinct, and equally good, cases for continuing to be deeply
sceptical about some of the claims of the stylometricians, while the papers from
Tweedie and her collaborators continue the tradition of bending the latest new
technology (here, multivariate analysis and neural nets) to the oldest problem
(authorship identification).
</p>
<p>
As this last example shows, the methods and techniques of humanities computing
have always been characterized by a wide ranging eclecticism. In the 1992
volume, for example, we find connectionism applied to poetic meter (Hayward);
parser generator systems applied to Greek metrics (de Jong and Laan); NLP
techniques developed for text understanding systems applied to reader response
theory (Snelgrove); data modelling techniques applied to lexicography (Ide and
Veronis) and a particularly productive application of the cladistic techniques
developed in the biological and natural sciences to the analysis of manuscript
variation (Robinson and O'Hara). In 1995, correspondence analysis is used to
define a typology of computer mediated narrative (Aarseth), and also as a means
for providing the raw data for visual presentation of a philosophical concept
(Bradley and Rockwell); distributed database systems grapple with the problem of
integrating text collections (Giordano, Goble and K&#xE4;llgren); and
object-oriented systems represent complex humanistic analyses (Simons).
</p>
<p>
Perhaps the most significant preoccupation for the field, to which these volumes
bear ample witness, is a focus on basic issues of text representation (or
encoding) and corpus construction. Methodological papers elaborating ever more
sophisticated techniques of statistical analysis remain of little use or
credibility if the data analysed derive from arbitrary or academically
indefensible language samples. In providing a standard method for the creators
of digital resources to document and make explicit their transcription and
encoding practices, the Text Encoding Initiative has laid the ground work for a
revolution in the field, as well as re-centering its attention on the
hermeneutic issues that characterise the humanities. It has also greatly
facilitated the transfer of techniques and methods between different groups of
researchers, working often in quite different disciplinary fields.
</p>
<p>
This 'cross-over effect' is particularly apparent in the way that the insights
of corpus linguistics and other areas of applied linguistics have changed our
notions of the enterprise of literary exegesis and analysis. The 1992 volume
contains two particularly useful articles by van Halteren, on part of speech
tagging, and Burr, on corpus design methods, both of which should be required
reading for anyone interested in the construction of lexical corpora for
whatever purpose. It also contains two nicely complementary articles on the
inter-relationship between tagging and interpretation, from McCarty and Renear
et al. In 1995, a year after publication of the TEI <title>Guidelines</title>, similar
theoretical encoding issues are articulated in the context of a growing number
of applications and realistically scaled projects. Examples include Hofland's
practical description of an algorithm for automatic alignment of parallel
corpora; Simons' description of an object-oriented implementation of the TEI's
feature structure markup; and Calzolari and Monachini's account of the EAGLES
proposals for standardization of morpho-syntactic analysis. The pragmatic force
of such contributions is a pleasing counter-balance to the more speculative
papers reviewing the limitations of the encoding enterprise, such as Neuman on
manuscript transcription, or K&#xE4;llgren on the problems of automatically
tagging Swedish.
</p>
<p>
These volumes show that humanities computing is more than a ragbag of techniques
and applied statistics. It is interdisciplinary and synergistic, in the best
senses of those rather overworked words, bringing insights from many fields to
bear on a common set of tasks. If these volumes are to be believed, it also has
a defined focus: the traditional humanistic concern with textual integrity and
explication. Those embarking on an exploration of how computer-based techniques
can support, hinder, or enrich that mission will find substantial guidance and
information in them, beyond the simple chronicling of continuity and change
within the research field they document.
</p>    
</div>
      <xi:include href="samways.xml"/> <!-- unpublished? -->
      <xi:include href="simmons.xml"/> <!-- Univ Computing. Date? -->
    </body>
  </text>
</TEI>
