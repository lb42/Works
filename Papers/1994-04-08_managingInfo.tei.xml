<?xml version="1.0" encoding="UTF-8"?>


<!-- Pam, here it is. Not quite what I expected it to be, and no -->
<!-- guarantees that the talk itself will be the same, but something -->
<!-- to go on the book anyway. Apologies for the delay! -->
<!-- It parsed last night, but I have made some mods since then... -->
<!-- Lou -->

<!--<!DOCTYPE article SYSTEM "article.dtd" [
<!-\- ENTITY plgfig1 SYSTEM "plgfig1.tif" NDATA tif -\->
]>-->
<TEI xmlns="http://www.tei-c.org/ns/1.0">
	<teiHeader>
		<fileDesc>
			<titleStmt>
				<title>Managing Information with SGML --- the next frontier</title>
			</titleStmt>
			<publicationStmt>
				<p/>
			</publicationStmt>
			<sourceDesc>
				<p/>
			</sourceDesc>
		</fileDesc>
	</teiHeader>
	<text>
		<front>
			<titlePage>
				<docTitle>
					<titlePart><title>Managing Information with SGML --- the next
						frontier</title></titlePart>
				</docTitle>
				<docAuthor><persName><forename>Lou</forename><surname>Burnard</surname></persName>
					<affiliation>MA MPhil Oxford University </affiliation><affiliation>European
						Editor, Text Encoding Initiative</affiliation><affiliation>Director, Oxford
						Text Archive</affiliation>
					<orgName>Oxford University Computing Services</orgName>
					<address>
						<addrLine>13 Banbury Road</addrLine>
						<addrLine>Oxford</addrLine>
						<addrLine>OX2 6NN</addrLine>
					</address>
					<email>lou@vax.ox.ac.uk</email>
				</docAuthor>
			</titlePage>
			<div type="abstract">
				<!-- The following heading should always be used for Abstracts. -->
				<head>Abstract</head>
				<p>SGML has proved its value as an interchange format for documents and other forms
					of textual data. But that is only half of the story.
					<!-- The abstract should be kept to approximatey 100 words. --> This
					presentation considers whether the ways of representing the kinds of information
					traditionally the preserve of the Management Information System or the
					relational database might not also be handled within the SGML paradigm. Whether
					your application is about producing readable documents as output from a nice
					clean set of facts, or about producing plausible facts from nice clean set of
					documents, SGML documents provide a conveniently neutral vehicle for the
					expression of information. </p>
			</div>
			<div type="keywords">
				<list>
					<item><term>Text retrieval </term></item>
					<item><term>Databases </term></item>
					<item><term>Data Analysis</term></item>
				</list>
			</div>
		</front>
		<!-- end the front matter -->
		<body>
			<!-- beginning of the body matter -->

			<!-- The following section should always be entitled "Keywords". Each 
paragraph in the section should consist of one  keyword term. -->

			<div xml:id="ldb1">
				<!-- 	Please do not number the sections. This will be done automatically 
	by the rendering software. 
	This synopsis section should be a minimum of about 1000 
	words.
-->
				<head>Retrospective</head>
				<p>In a paper published in 1987, I wrote: <quote> Every tagging system is intimately
						related to the software used to process it. Despite welcome efforts towards
						standardisation ... there is little sign of an emergent <q>text description
							language</q> analogous to the <q>data definition languages</q> now
						standardised for database management systems</quote><note place="end">In
							<title>CAFS: a new solution to an old problem</title> (Literary and
						Linguistic Computing vol 2, no 1, 1987, p 8)</note>
				</p>
				<p>Seven years later, can we say that SGML (the outcome of those <q>welcome
						efforts</q>) has now provided us with something like an adequate text
					description language? In this presentation, I'd like to get you to think about
					that question. </p>
				<p>We might profitably begin by reviewing a little history. Ten years ago, at least
					from where I was sitting, data modelling techniques looked pretty reliable. In
					an influential book of the period,<note place="end">J.F. Sowa: Conceptual
						Structures, p 17, 1984</note> John Sowa characterized it as <q>the work of
						philosophers, lawyers, lexicographers, systems analysts and database
						administrators</q> : today might we add DTD writers to the list? The
					techniques of conceptual analysis described by Sowa, Chen, Tschirtzis, and
					others had emerged out of a decade of computer science theorizing and were being
					repeatedly put to the task of designing enormously complex fault-tolerant
					systems for the management of huge flows of complex information in
					administrative databases, in business, industry and government. It seemed
					reasonable to ask why nothing analogous had yet appeared for the processing of
					text --- particularly because existing dbms were so conspicuously awful at
					handling text? What was so special about text that it couldnt be handled in the
					same way as other kinds of data? </p>
				<p>The 80s orthodoxy was that before attempting to build a database, you should
					first derive an underlying conceptual model. This model (often held in a
					specialised kind of database called a data dictionary) would define a
						<q>universe of discourse</q> -- a meta language with specified limits. The
					data dictionary for an airplane reservation system says all there is to say
					about airplane reservation independent of any processing you might want to do,
					within the constraints of a given universe of discourse. So, the data dictionary
					knows that there are such things as airplanes, and airports. It will hold
					information about some attributes of airplanes (such as their payload
					capabilities and other technical characteristics, their ownership, their usage
					history etc) but probably not others (such as their colour). It will also hold
					information about ways in which airplanes and airports can be meaningfully
					related: for example, that a given airplane is able to land at a specific
					airport. </p>
				<p>Separating out information about relationships from information about real world
					objects (or <q>entities</q> as they are confusingly known in this terminology)
					can be difficult: for example, an airplane ticket could be said to represent a
					relationship between a given airplane, a passenger, and a set of airports ---
					but has so much additional information (seat number, class, price paid etc.)
					that it would probably be defined as an entity in its own right. As with all
					other forms of model, there is no ultimate truth being striven for here: only a
					consistent set of definitions. The relationship between this model and a real
					world airplanes, tickets, flights and airports is thus defined once for all, if
					only by exclusion. The crucial point is that the goal of the exercise is to
					model the real world, not necessarily some selection of pre-existing artefacts
					designed to help you cope with the real world. </p>
				<p> This stress on abstracting the underlying system, rather than its outputs, may
					be explained in historical terms -- the integrated database systems of the
					seventies and eighties were designed to replace, integrate or enhance existing
					processing applications and thus needed to clearly establish a broader
					perspective (rather in the way that appeals to nationalism are often used to
					undermine regional or tribal loyalties). Or it may be regarded as simply a
					recent variant of an ontological dispute that can be traced back to Aristotle
					and Plato. Either way, it was conspicuously absent from the way most people
					processed text by computer in the period. </p>
				<p>In the seventies, electronic text was the province of a subdiscipline of library
					science called "information retrieval" (as opposed to "information management").
					It dealt not in abstractions or facts but with an unbounded, semi-chaotic world
					in which countless artefacts -- we will call them documents, though they may
					take no substantive form -- compete for attention. Somewhere in the texts of
					these documents information lies buried, but it is rarely explicit, generally
					requires human comprehension and is often ambiguous or misleading. A typical
					example might be the dossiers of evidence amassed by even a moderately competent
					police force during a criminal investigation, or the masses of experimental
					description associated with a patent application: many disparate types of
					document, all concerned with a related set of objects and events in the real
					world, but in each case refracted through a different set of distortions. </p>
				<p>Considerations of scale, as much as anything else, preclude the handling of these
					masses of textual data in terms of an underlying model of their subject matter:
					instead, they must be handled as masses of words. Sophisticated indexing
					techniques and pattern-matching algorithms, specialised hardware and storage
					methods are devised to overcome the inherent reluctance of human language to be
					treated as a formal system, to cater for its inherent redundancy and complexity.
					Instead of trying to model the real world, IR systems try to model a naive
					reading of texts about the real world, in which all they contain is words. </p>
				<p>Now, as any phenomenologist will tell you, meaning is an emergent property of
					language: that is, the meaning of a word is not strictly speaking a property of
					the word alone. Instead, it comes about in the mind of the perceiver during a
					complex interaction with the text, involving a host of contextual information,
					not explicitly present in the text. This perception also underlies the emphasis
					placed by more sophisticated IR systems upon user-interaction with the system:
					notions of <q>precision</q> and <q>recall</q>, of <q>relevance feedback</q>, and
					similar metrics, which foreground each users' view of the system's usefulness
					rather than attempting to derive the system from any agreed model. In extremis,
					some practitioners make a virtue out of this lack of design; claiming that to do
					otherwise is to prejudge the issue, to impose a structure which may impede true
					analysis. This seems at best defeatist; at worst anti-intellectual. <note
						place="end">I argued this case rather more vehemently in an article
						presented at an ASLIB conference in 1990; surprisingly, they did not invite
						me back</note>
				</p>
				<p>As the me-too eighties rolled on, two new approaches to dealing with textual
					information became fashionable, conveniently polarizing the very opposition I
					have been trying to identify during this rapid canter through recent history. </p>
				<p>Firstly: the rise of the <q>expert system</q> as the answer to all your
					information handling needs. This approach to information modelling placed all
					its emphasis on predication -- the abstract observation that planes fly between
					airports, or that if something flies it cannot be an airport -- using text only
					as a more or less convenient vehicle for articulating factual information. The
					claim was that huge amounts of detailed knowledge could be represented in this
					way, as collections of assertions and inferences. So far as I know, such systems
					have not yet been developed for anything other than highly restricted domains,
					such as lunar rock samples, medical diagnosis or oil prospecting; scaling up
					such systems to handle the real world often turns out to require quantitative
					changes approximating to qualitative metamorphosis. </p>
				<p>Secondly, the rise and rise of that most perniciously seductive of all the text
					handling technologies developed during the eighties: desktop publishing. Here,
					the emphasis is all on texts as physical objects, either on paper or on the
					screen, at the expense of their true function as information-bearing (or
					engendering) artefacts. In a landmark article published in 1987, Coombs, Renear
					and DeRose nicely characterized this tendency as the subversion of the scholarly
					intellect away from its true mission <note place="end">See <q>Markup systems and
							the future of scholarly text processing</q> Communications of the ACM,
						vol 30 no 11, pp 933-47</note> -- conveniently forgetting perhaps that much
					scholarship is very intimately concerned with the appearance of texts. Now that
					the SGML Revolutionaries have taken up high office, it may be difficult to
					remember what all the fuss was about. This audience will need little persuasion
					that a text should be considered independently of its rendition. Perhaps
					therefore, I should remind you how very difficult it can be to disentangle the
					two. In the case of many ancient texts, for example, it's not so much "What You
					See Is What You Get" as "What You See Is All There Is". </p>
			</div>
			<div xml:id="ldb2">

				<head>The Present</head>

				<p>The lesson I'd like to draw from this survey of trends over the last twenty years
					is a simple one: that we should stop trying to decide between abstraction and
					representation, and warmly embrace both. And I will add the rider that SGML is
					exactly the enabling technology that makes it possible for us to do so. </p>
				<p>The first step on the road to this conclusion is to accept that computer-held
					information is only the most recent of a variety of ways human intelligence has
					devised for its self-propagation and preservation. <note place="end">The
						distinguished linguist Tom McArthur argues plausibly for this continuum in
						his <q>Worlds of Reference: Lexicography, Learning and Language from the
							Clay-tablet to the Computer</q>, Cambridge 1986</note>. Written texts
					are not a special type of data: data is a special type of written text. </p>
				<p>This being granted, we should perhaps consider the nature of text a little more
					closely. In a presentation given at the Modern Language Association in 1990, my
					colleague Michael Sperberg McQueen listed the following important
					characteristics of texts: <list>
						<item>texts are linguistic objects: they cannot be fully understood without
							an understanding of linguistic constructs such as sentences and
							words.</item>
						<item>texts are also physical objects: the medium may not be the whole of
							the message, but it is often a very significant part of it.</item>
						<item>texts are both linear and hierarchic: they may be read in a linear
							manner, but are typically organized into at least one hierarchy, of
							nested structural components and often several.</item>
						<item>texts have cross-referring structures: there is nothing new about
							hypertext!</item>
						<item>texts refer to objects in a real or fictional universe: they provide a
							medium through which real world entities and relationships may be more
							or less clearly identified.</item>
						<item>texts have a history: as cultural objects, texts evolve and change,
							and our perceptions of their significance consequently shift. </item>
					</list>
				</p>
				<p>Parenthetically, note that we can express an analogous list for data: <list>
						<item>data may be expressed as attribute/value pairs</item>
						<item>data may be summarized or expressed in many forms</item>
						<item>data items may be aggregated into data structures</item>
						<item>data items may be copied or linked</item>
						<item>data relates to objects in the real world</item>
						<item>data may exist in multiple versions</item>
					</list> The key technology which enables us to support this range of textual
					features is <emph>markup</emph>. </p>
				<p>Textual markup is the way in which we make explicit our theory about a text. I
					hope no-one will be too much perturbed by my reminding you that there are very
					few atheoretical <q>objective</q> facts about how texts should be read, possibly
					none. Despite this, since the whole purpose of creating texts is to share them,
					it is crucially important to make explicit the conventions used in marking them
					up. The great gift which the designers of SGML gave the world was a markup
					language which was <emph>generalized</emph>: more exactly, a language for the
					description and definition of <emph>many</emph> markup languages -- as many as
					there are theories about a text. No markup language which is finite can also be
					complete, since no reading of a text can claim to be complete. However, all
					markup languages can -- and should -- claim to be describable. </p>
				<p>Theories about a text encompass the full range of features listed above and it is
					therefore reasonable to require of a markup scheme that it provide ways to make
					explicit: <list>
						<item>the linguistic organization of a text (its division into words or
							other linguistic units; its semantic organization; patterns of
							significance, topoi etc)</item>
						<item>the physical characteristics of a non-electronic version of a text,
							whether this is a precursor of the electgronic form, or generated from
							it</item>
						<item>the internal structural units of a text</item>
						<item>linkage and alignments between text units, both within single texts
							and across multiple text collections</item>
						<item>abstract representations of events, objects, etc. described in the
							text, linked to those descriptions </item>
						<item>the history of successive stages of the text, together with
							indications of uncertainty or ambiguity in its content or encoding
						</item>
					</list>
				</p>
				<p>You will not be surprised to hear me claim that the recently published <note
						place="end">Or so I devoutly hope</note>
					<q>Recommendations</q> of the Text Encoding Initiative include provisions for
					the representation of all of the above, and more besides, using a modular SGML
					document type definition; nor that I don't plan to substantiate the claim in the
					remainder of this presentation. Instead I would like to return to the issue of
					data modelling. </p>
				<p>A data model seems to have two distinct roles. Firstly, it serves to limit
					complexity, by defining legal combinations and organizations for all the data
					described by it, by providing a <emph>syntax</emph>. Secondly, it defines a
					framework within which the intended significance of the data described should be
					understood, by providing a <emph>semantics</emph>. The first role comes into
					play when the model tells us in which record types the field EMPLOYEE-NUM may
					appear, in which its value should be unique across the domain, and so on. The
					second role enables the model to specify not only that record type EMPLOYEE
					contains information about <q>employees</q> but also exactly what kind of an
					object an employee may be. </p>
				<p>It is often said of SGML that it provides for the first role very well, and for
					the second not at all. Like most things often said, this is both true and
					untrue. SGML in itself has no semantic component: that is the source of its
					strength. An SGML <emph>application</emph> however is all semantics: it defines
					a mapping between textual features and a given set of SGML elements and
					attributes. This mapping may be thought of as defining an interchange space in
					which there can be agreement about both syntax and semantics of a given set of
					documents. The challenge is to make this interchange space sufficiently large
					and flexible to accomodate many different SGML applications. </p>
				<p>One key notion in making this kind of information interchange possible is what
					the TEI terms a <q>tag set</q>: that is, a set of SGML elements, each of which
					is explicitly associated with some textual feature, as well as being
					syntactically defined in the normal SGML way. A collection of such tag sets
					functions in much the same way as an <q>object store</q> does in an
					object-oriented database: specific applications can pick and choose from this
					repository to build customized views which are overlapping and complementary. </p>
				<p>A second key notion, also derived from the object-oriented paradigm, is that of
					element classes. Any kind of classification scheme helps to control complexity:
					the assignment of individual components of a tag set to semantically meaningful
					classes also enables an additional part of the interchange space to be mapped
					out, orthogonal to that defined by the modularisation facilities provided by the
					use of tag sets. Allocation to a class is a convenient way also of allowing for
					controlled extensions to the information space, very similar to HyTime's
					architectural forms. </p>
				<p>Finally, we should regard as a strength the relative simplicity of the SGML
					formalism, which encourages us to encode isomorphically both the representation
					of information, and one or more interpretations of it. The ability to represent
					meta-information such as links enables this to be done with a high degree of
					complexity, even with a very simply syntax. To take a trivial example, consider
					today's date. There are any number of ways in which I might represent this,
					ranging from the pronominal ("today") to the periphrastic ("the last day I shall
					be in the office this week") as well as more obvious variations such as "8
					April", "1994-08-04" etc. Any one of these may be marked as a date simply by
					using a &lt;date> tag, thus alerting a processor interested in temporal
					information to the fact that this stretch of PCDATA may be relevant to it. The
					task of such a processor can be simplified by inspection of the context (a
					&lt;date> element appearing inside a &lt;birth> element probably means something
					different from one inside a &lt;death> element), or of any associated attributes
					(a common practice might be to include a normalised version of a date for
					example: &lt;date value='1994-04-08'>8th April&lt;/date> ). Or I may decide that
					the whole business of temporal information is too complex to handle in this
					simple way, choosing instead to define an abstract structure of feature value
					pairs, represented directly as analytic elements, and linked to the humble text
					phrases by pointers. SGML is not restricted to the representation of textual
					information. </p>
			</div>
			<div xml:id="ldb3">
				<head>The future</head>

				<p>Mechanisms such as those mentioned in the previous section point the way to the
					future development of SGML as a processing interlingua. In the beginning, the
					first sign that someone might be serious about using SGML was when they started
					to ask such questions as "Where can I get one of those ddt things? Do I really
					have to build one of my own?". At first, I am told, people did believe that it
					might be possible to define a few massive dtds adequate to the needs of whole
					sectors of the information processing industry. There's a persistent tendency in
					most human endeavours to start with the grand consolidated imperialist vision,
					and it rarely lasts: the sun sets on all empires, including IBM. What's
					interesting is to see whether what follows is balkanization and chaos or
					genuinely open communication between equal partners. I think most practioners
					will agree that an open market for element definitions, whose semantics are
					clearly and consistently specified, and which can be combined, modified or
					extended according to application-specific rules is the best way forward from
					here. After all, if no consensus can be reached on the semantics of an element,
					interchanging the information represented by it would be somewhat nugatory. All
					that we need is a mechanism for representing those semantics clearly and
					consistently. </p>
				<p>When we faced this problem in the TEI, there was some discussion about whether to
					adopt a traditional database approach for what was then called the "tags
					database" or whether to stick to our principles and document each element in
					SGML. Being principled academics, naturally we chose the latter route, and thus
					found ourselves having to design and implement an object oriented database
					system, document formatter and retrieval system all rolled into one.<note
						place="end">The ODD (One Document Does it all) System will be presented in a
						paper by Sperberg-McQueen and myself at the ALLC-ACH Interbnational
						Conference "Consensus ex machina" in Paris in April 1994</note> This
					quixotic task would have been impossible without the central notion of the SGML
					document as the basic entity on which independent processors can operate, to
					generate (in our case) variously formatted documentation or dtd fragments, and
					on which SGML-aware editors can operate to manipulate and modify a dynamic
					document database. </p>
				<p>Because all of our processors create, modify or read the same basic set of
					elements, we might have taken the database approach and defined record types or
					views appropriate to each processor. Instead we chose to define document types
					corresponding with the selection of such elements appropriate. When creating
					documentation for an element class, for example, we do not yet know what all of
					its members will be: the corresponding &lt;classdoc> element consequently has no
					&lt;members> subelement in the dtd used by the editing processor -- but such an
					element is present in the dtd used by the formatting processor and additionally
					in the processor which generates SGML dtd fragments from our document database. </p>
				<p>A system in which all inter-process communication is carried out exclusively by
					SGML-conformant documents turns out to be a surprisingly effective way of
					implementing EDI <note place="end">J P Gaspart gave an impressive presentation
						on this topic at an OII Workshop in Luxembourg last year</note>. The
					difficulties lie not in any theoretical inadequacy of SGML but in the difficulty
					of finding and integrating good modular SGML aware software tools: surely only a
					transient difficulty. We should not be surprised by news of this sort: data
					interchange and document interchange are two sides of the same coin. But SGML is
					the name milled round the edge. </p>
			</div>
		</body>
		<back>
			<!-- this is back matter information -->
			<!-- speaker vita -->
			<div type="vita">
				<head>Lou Burnard</head>
				<p>Lou Burnard has worked at Oxford University Computing Services since 1974. He has
					been director of the Oxford Text Archive since 1976 and active in the
					development of Humanities Computing facilities both at Oxford and nationally.
					His publications include papers on Snobol, database design, CAFS, text retrieval
					systems, research applications for IT and book reviews. His main interests are
					in database design and construction, free text retrieval systems, and the
					encoding of text for research purposes. He has been European Editor of the Text
					Encoding Initiative since 1989. </p>
			</div>
		</back>


	</text>
</TEI>
