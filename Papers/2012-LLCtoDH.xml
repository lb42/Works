<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" rend="jTEI">
  <teiHeader>
      <fileDesc>
         <titleStmt>
            <title type="main"> Du literary and linguistic computing aux digital humanities : retour sur 40 ans de relations entre sciences humaines et informatique
               2009
            </title>
        
            <author>
               <name>
                  <forename>Lou</forename> 
                  <surname>Burnard</surname>
               </name>
               <affiliation/>
               <email/>
            </author>
            <author><name><forename>Pierre</forename><surname>Mounier</surname></name> <affiliation/>
               <email/></author>
         </titleStmt>
         <publicationStmt>
            <publisher>TEI Consortium</publisher>
            <date/>
            <availability>
               <p>TEI Consortium 2015 (Creative Commons Attribution-NoDerivs 3.0 Unported License)</p>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <p>Saved from <ptr target="http://books.openedition.org/oep/242"/> and rehacked</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <projectDesc>
            <p>Revues.org -centre for open electronic publishing- is the platform for journals in the
          humanities and social sciences, open to quality periodicals looking to publish full-text
          articles online.</p>
         </projectDesc>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en">en</language>
         </langUsage>
         <textClass>
            <keywords xml:lang="en">
          
               <term/>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc>
         <change/>
      </revisionDesc>
  </teiHeader>
<text><front> <div type="abstract"> <head>Note de l’éditeur</head> <p>Ce texte est une transcription de l’intervention de Lou Burnard
  lors du séminaire « <ref target="http://www.ehess.fr/fr/enseignement/enseignements/2009/ue/928/"
    >Digital humanities : les transformations
      numériques du rapport aux savoirs</ref> » animé par Marin Dacos et Pierre Mounier.
  Cette intervention, qui s’est déroulée le 16 décembre 2009, est disponible dans son intégralité
  sur le blog d’OpenEdition,<title> L’Édition électronique ouverte, </title> à l’adresse suivante : <ref
    target="http://leo.hypotheses.org/3994">http://leo.hypotheses.org/3994</ref></p></div>
</front>
<body >
    <div>
      
 <head>Introduction</head>
  <p n="1">Je me suis fixé le but ambitieux de proposer un point de vue sur quarante ans d’histoire,
    sous la forme d’une synthèse, plutôt que de vous présenter des faits les uns à la suite des
    autres. J’ai essayé d’extraire de mon expérience quelques principes généraux.</p>
  <p n="2">Je commencerai en témoignant du fait que nous vivons de façon irréversible un âge
    numérique. J’expliquerai cela en présentant les trois périodes successives qui ont marqué le
    développement des relations entre sciences humaines et informatique, ainsi que les principes qui
    les ont portées. Enfin, pour parler de façon concrète, je présenterai le cas spécifique de mon
    expérience à Oxford, ce qui permettra d’approfondir l’état courant des recherches dans les
    sciences humaines et sociales (SHS) informatisées.</p>
  <p n="3">Il y a toujours eu une vraie tension entre les deux méthodes opposées dont on peut se
    servir dans les sciences humaines et sociales et l’informatique ; une confrontation entre le
    texte et les données. Certains diront qu’on peut se servir d’un ordinateur pour faire du
      <hi>data processing</hi>, pour gérer des données : des chiffres, des faits, des observations,
    des objets, des tendances statistiques. Le texte, en revanche, est composé d’autres choses : de
    mots, d’une langue, de paroles, qui ont une existence tout à fait indépendante de leur
    représentation dans un format numérique. Les données numériques, elles, n’existent que dans leur
    expression informatisée. C’est précisément là que réside la tension entre les deux. Pour réussir
    à la lever, certains informaticiens proposent de traiter le texte comme s’il s’agissait d’une
    donnée.</p>
  <p n="4">Pour revenir à mon expérience, lorsque j’ai commencé à travailler, on tapait sur des
    cartes qui comprenaient chacune 80 caractères. À cette époque-là, pour les informaticiens, le
    texte, c’était ces 80 caractères-là ; et à chaque texte correspondaient de grandes chaînes de
    caractères de 80 octets. Pourtant, le texte est évidemment d’une autre nature, puisqu’il possède
    sa propre structure, ses objets, ses idées, etc. Ce que les informaticiens trouvaient très
    difficile à comprendre, c’est le fait que les données elles-mêmes constituent une espèce de
    texte : elles ne sont pas objectives. Le chiffre 1 par exemple, possède une valeur mathématique,
    mais il ne veut rien dire s’il ne s’applique pas à quelque chose. Dans l’expression de son
    application, on effectue un acte d’interprétation, pour savoir si tel ou tel chiffre s’applique
    à tel ou tel événement, tel ou tel fait historique. Pour les méthodes les plus objectives, la
    difficulté réside donc précisément dans l’identification du phénomène d’interprétation. Un
    texte, ce n’est pas un ensemble d’octets, c’est une entité créée dans l’esprit de l’acteur – et
    c’est précisément cette question que je vais développer lors de ma présentation.</p>
  <p n="5">Nos infrastructures de connaissances sont devenues tout à fait numériques, sans que l’on
    puisse faire machine arrière, induisant des changements dans les modèles économiques et
    commerciaux. Inutile, par exemple, de continuer à penser que l’on évolue toujours dans un monde
    de l’édition papier : cette époque est révolue. Par ailleurs, les technologies numériques nous
    viennent en aide en nous permettant de faire des choses de façon plus simple, plus efficace.
    Elles favorisent surtout la gestion de données à une échelle plus grande. Tout ce processus,
    issu d’un changement quantitatif, prend finalement la forme d’un changement qualitatif.</p>
  <p n="6">Nous vivons dans une époque numérique. Pour preuve, les chercheurs qui entreprennent des
    recherches en littérature contemporaine par exemple, ou sur la communication quotidienne, ne
    peuvent pas se passer des données numériques. On pourrait toujours s’en passer, mais ce serait
    inepte. Les objets de nos sciences sont maintenant irrévocablement numériques même si les
    méthodes de travail n’en sont pas encore là. Cela dit, elles sont en perpétuelle évolution et on
    relève beaucoup de changements dans le genre de méthodes disponibles ainsi que dans leurs
    applications. C’est ainsi vrai du passage de l’hypertexte à l’<hi>hyperdata</hi>, d’un web de
    documents à un web de données, pour lequel la technologie est performante. Et même, s’il reste
    de nombreux problèmes à résoudre, ils ne sont pas d’ordre technologique mais d’ordre social,
    culturel et politique.</p></div>
<div>      
  <head>Le <hi>literacy and linguistic
        computing</hi> (LLC) (1960 à 1980)</head>
  <div>
  <head>L’Index
      Thomisticum</head>
 <p> <ref type="crossref" target="#ftn1">
    <hi>Corpus Thomisticum</hi>, http://www.corpusthomisticum.org/ (...)</ref></p>
  <p n="7">À la fin des années 1940 et au début des années 1950, le père Busa, précurseur dans le
    domaine, a eu le premier l’idée d’avoir recours aux <hi>IBM mainframes</hi> pour créer une
    nouvelle édition de Thomas d’Aquin<ref target="#ftn1"
      >1</ref>. Il a commencé par constituer une équipe de cinq personnes dont le travail consistait
    à taper sur des cartes perforées tout l’ouvrage de Thomas d’Aquin et ce, dans l’optique de
    générer de façon automatique un index de chaque mot du corpus. L<hi>’Index Thomisticum </hi>a
    été rendu public en 1972.</p></div><div>
  <head>Le Brown Corpus for Use on
        Digital Computers</head>
  <p><ref type="crossref" target="#ftn2">
    <hi>Brown Corpus Manual</hi>, http://icame.uib.no/brown/bcm.html </ref></p>
  <p n="8">Dix ans plus tard, à l’université de Brown, Henry Kurcea et Nelson Francis lançait le
    chantier d’un corpus linguistique pour la description de la langue courante anglaise. Pour ce
    faire, ils ont eu l’idée d’indexer, au moyen de machines, tous les mots de la langue, dans
    toutes les variétés de discours. C’est ainsi qu’ils ont mis au point le <hi>Brown Corpus for Use
      on Digital Computers</hi><ref target="#ftn2">2</ref>. La
    première version dénombrait un million de mots, intégralement tapés à la machine, en majuscules,
    sur des cartes perforées. L’intérêt de ce projet réside dans le fait qu’il a été construit de
    façon systématique afin de décrire au mieux l’objet étudié : l’anglais. Les chercheurs ont donc
    dessiné un corpus en commençant par définir la langue et ont constitué des échantillons de
    livres, basés sur la fréquence d’occurrences présentes dans la bibliothèque et rendant compte de
    la diversité des genres et des domaines traités. (C’est une démarche que l’on peut critiquer,
    dans la mesure où une bibliothèque ne conserve pas de discours oral, qui est pourtant l’un des
    paramètres nécessaires à la description d’une langue.) Malgré cela, il faut remarquer que
    c’était la première tentative de normaliser, de standardiser la construction d’un corpus selon
    des principes statistiquement défendables.</p></div>
  <div>
 <p> <ref target="#tocfrom2n3">Le <hi>British National
        Corpus</hi></ref></p>
  <p n="9">Quelques années plus tard, nous avons eu recours au même type de démarche pour le
      <hi>British National Corpus</hi>. Les enjeux étaient les mêmes, bien que la technologie, entre
    temps, ait quelque peu évolué. Notre corpus, constitué d’après des principes cohérents et bien
    exprimés, était sensiblement plus grand puisqu’il comptait cent millions de mots. J’en fais
    mention parce qu’il s’agit d’une des étapes très importantes de l’évolution de l’application des
    technologies aux recherches en linguistique – une recherche qu’il serait impossible de mener
    sans la technologie. La constitution des corpus linguistiques ne peut en effet se faire qu’avec
    un ordinateur, à l’inverse des travaux philologiques qui peuvent se poursuivre sans
    technologie.</p></div><div>
  <head>Le Thesaurus Linguae
        Graecae</head>
  <!--<ref type="crossref" target="#ftn3">
    <hi>Thesaurus Linguae Graecae</hi>, http://www.tlg.uci.edu/ <ref
      target="#ftn3">(...)</ref></ref>
-->  <p n="10">Le <hi>Thesaurus Linguae Graecae</hi> constitue mon troisième exemple<ref
      target="#ftn3">3</ref>. Le projet, fondé sur des éditions
    scientifiques, a d’ailleurs rencontré des problèmes de droit, vis-à-vis des maisons d’édition.
    Dans les années 1970, Theodor Brunner, professeur de grec aux États-Unis, donnait des cours du
    soir sur la littérature grecque, à l’occasion desquels il évoquait notamment les possibilités de
    recherche sur les fonds littéraires grecs, pour y étudier le vocabulaire, les usages, etc. Lors
    de l’un de ses cours, une personne de l’auditoire, David Hewlett (fils du fondateur de Hewlett
    Packard), lui demanda combien coûterait la création d’une base de données qui rassemblerait tous
    les textes de la littérature grecque. Et c’est ainsi que David Hewlett a financé les travaux de
    Theodor Brunner. Celui-ci a constitué une équipe qui a commencé à taper tous les textes de
    littérature grecque disponibles, sans prendre tout à fait conscience des problèmes de droit
    posés par l’utilisation des éditions scientifiques déjà publiées – le problème était assez
    inédit à l’époque.</p>
 <!-- <ref type="crossref" target="#ftn4">
    <hi>TLG Beta Code Quick Refrence Guide</hi>, http://www.tlg.uci.edu/encoding/quickbeta.pdf,
    consulté le 17 a <ref target="#ftn4">(...)</ref></ref>
  <ref type="crossref" target="#ftn5">
    <hi>ASCII Table and Description</hi>, http://www.asciitable.com/ <ref
      target="#ftn5">(...)</ref></ref>
-->  <p n="11">Ils se sont également retrouvés confrontés à des problèmes d’encodage. En effet, les
    cartes perforées ne permettaient que l’enregistrement des 26 lettres de l’alphabet latin, en
    plus des chiffres et de quelques signes de ponctuation, pas celui de l’alphabet grec, ni des
    accents et esprits. Ils ont donc dû créer leur propre système d’encodage : le <hi>Beta
      Code</hi><ref target="#ftn4">4</ref>, une façon d’encoder
    les caractères grecs, sous format déguisé, en ASCII (le premier système d’encodage, inventé en
    même temps que les ordinateurs<ref target="#ftn5">5</ref>).
    L’encodage en <hi>Beta Code</hi> a naturellement posé des difficultés par la suite, au moment de
    la conversion des textes en <hi>Unicode</hi>.</p>
  <p n="12">Ces projets, bien que très importants pour le sujet qui nous intéresse ici, ne sont pas
    les seuls. J’aurais ainsi pu vous en présenter d’autres.</p></div><div>
  <head>Quantifier les signes, établir
      des concordances</head>
  <ref type="crossref" target="#ftn6"> La concordance est un procédé philologique hérité du Moyen Âge
    qui permet d’identifier toutes les f <ref target="#ftn6"
      >(...)</ref></ref>
  <p n="13">Par ailleurs, c’est une chose que de vouloir créer une base de données textuelle, encore
    faut-il savoir ce que l’on souhaite en faire. Quels sont les enjeux d’une recherche fondée sur
    ce genre de données ? Pour la plupart des scientifiques de cette époque, il s’agissait de
    quantifier les signes, d’identifier des régularités statistiques dans les textes, d’établir des
      <hi>concordances</hi><ref target="#ftn6"
    >6</ref><hi>.</hi></p>
  <p n="14">En 1959, les Cornell University Press ont commencé la publication sur papier d’une série
    de concordances, sur tous les auteurs de la littérature anglaise, établies par ordinateur
    (recherche des occurrences en contexte pour chaque mot et tri). Cela a représenté un travail
    considérable, de même qu’un très grand nombre de pages, puisqu’à chaque occurrence d’un mot
    devait correspondre une ligne. L’équipe de Cornell University Press s’est rapidement aperçue que
    la consultation de l’outil informatique était largement préférable à la manipulation de tels
    volumes.</p>
  <ref type="crossref" target="#ftn7"> CETEDOC, http://bcs.fltr.ucl.ac.be/dicland.html <ref
      target="#ftn7">(...)</ref></ref>
  <p n="15">La dernière date significative pour cette période correspond à la création du Centre de
    traitement électronique des documents (CETEDOC)<ref
      target="#ftn7">7</ref> à Louvain-la-Neuve en 1968. C’est
    un des premiers centres de recherche dédiés aux applications de l’informatique sur les études
    médiévales et classiques.</p></div><div>
  <head>L’étude de distribution de
      fréquences, un moyen de définir la paternité d’un texte ?</head>
  <p n="16">L’identification automatique de l’auteur d’un texte a fait partie des applications très
    en vogue à la fin des années 1960 et au début des années 1970. Et je dois dire que c’est une
    problématique qui soulève plus de questions qu’elle ne donne de réponses.</p>
  <p n="17">Des études par ordinateur ont été conduites pour tâcher de déterminer si saint Paul
    était bien l’auteur de tel ou tel texte. Il s’agissait alors, en ayant recours aux propriétés
    statistiques et à partir du vocabulaire, d’identifier la paternité d’un texte. Les recherches
    ont été menées sur les mots les plus fréquents, car ce sont ceux dont l’usage et la répétition
    sont les plus significatifs : il est plus facile d’observer des variances entre plusieurs textes
    à propos des mots les plus employés.</p>
  <p n="18">L’étude des distributions de fréquence appliquées aux mots les plus employés, a permis,
    selon les spécialistes, d’identifier des modèles de fréquence tout à fait caractéristiques de
    tel ou tel auteur. Ils auraient ainsi mis au jour l’existence d’une « empreinte digitale » qui
    caractériserait la manière dont chacun emploie les mots les plus courants d’une même langue. On
    serait donc en mesure, par l’analyse des distributions de fréquences et en analysant l’ensemble
    de ses œuvres, de définir l’empreinte digitale de tel ou tel auteur, saint Paul inclus.</p>
  <p n="19">D’autres études de distribution de fréquence ont été menées, notamment par un Australien
    qui s’est intéressé à l’écriture de Jane Austen. Personnellement, je n’ai pas été convaincu par
    les conclusions auxquelles a abouti l’étude. Plus largement, il faut noter que ce type d’études
    repose sur une démarche fondée sur des évaluations statistiques qui ne semblent pas tenir compte
    de la diversité des modèles statistiques de distribution de fréquences (la distribution Poisson
    par exemple, qui ne ressemble pas aux distributions normales).</p>
  <p n="20">Les outils les plus importants en <hi>literacy and linguistic computing</hi> sont la
    concordance – encore aujourd’hui d’ailleurs –, et les statistiques complexes. On dit
    implicitement que le style peut être quantifiable, peut être mesuré.</p></div><div>
  <head>L’application de la
      technologie aux études historiques</head>
  
  <p n="21">À cette époque-là, de nombreuses études très intéressantes voient le jour. Par ces
    études, il s’agit d’améliorer les modèles statistiques de distribution, même si cela concerne un
    domaine de recherche qui n’est pas vraiment compris. On peut notamment faire mention de l’étude
    initiée en 1974 par Robert W. Fogel et Stanley L. Engerman, intitulée <hi>Time on the Cross :
      The Economics of American Negro Slavery</hi><ref type="crossref"
      target="#ftn8">8</ref>. Ces deux chercheurs ont conduit
    une analyse strictement numérique des enregistrements du système d’esclavage afin de déterminer
    si l’esclavage était un modèle économique plus rentable qu’un autre. Historiens de l’économie,
    leur démarche n’était pas idéologique mais purement informatique : ils ne s’intéressaient qu’aux
    faits et aux chiffres. Ils ont ainsi établi qu’un esclave était fouetté 0,7 fois par an, en
    moyenne. Ce faisant, ils ont perdu de vue que, comme je le disais au début, les nombres
    constituent un texte, une histoire. Dire qu’un esclave a été fouetté 0,7 fois par an n’enlève
    rien au fait qu’il a été fouetté. Cette étude, qui est fondamentale pour l’histoire économique
    et l’histoire de la traite des Noirs, était, à l’époque, complètement innovante du point de vue
    méthodologique. Les conclusions de cette enquête ont provoqué une avalanche de critiques et
    suscité de nombreux débats. L’application de la technologie aux études historiques en a tout de
    même sévèrement pâti, si bien qu’il faudra attendre pratiquement vingt ans pour pouvoir
    revendiquer une activité numérique appliquée à l’histoire sans être considéré comme un
      <hi>cliometrician</hi> du point de vue méthodologique.</p>
  <ref type="crossref" target="#ftn9">
    <hi/>http://llc.oxfordjournals.org/ <ref target="#ftn9"
      >(...)</ref></ref>
  <p n="22">En 1976, les chercheurs travaillant en <hi>literary and linguistic computing</hi>
    fondent une revue, qui existe toujours : <hi>Literary and Linguistic Computing</hi><ref
      target="#ftn9">9</ref>.</p>
  <ref type="crossref" target="#ftn10">
    <hi>Textométrie</hi>, http://textometrie.ens-lyon.fr/ <ref
      target="#ftn10">(...)</ref></ref>
  <p n="23">Depuis les années 1980, le champ des <hi>literary and linguistic computing </hi>est
    toujours en activité et tend à considérer le texte comme un phénomène statistique. C’est le cas
    notamment à l’ENS de Lyon avec le projet <hi>Textométrie</hi><ref
      target="#ftn10">10</ref>. Il existe cependant d’autres
    équipes en Italie et aux Pays-Bas, mais les projets les plus intéressants sont développés en
    France, notamment sur les rapports entre fréquence des mots et classes sociales.</p></div></div><div>
  <head>L’<hi>humanities
        computing</hi> (1980-1994)</head>
  <p n="24">Le paysage informatique a évidemment beaucoup évolué dans les années 1980-1990, avec des
    effets très signifiants sur les sciences humaines et sociales.</p><div>
  <head>La procédure
      d’institutionnalisation</head>
  <ref type="crossref" target="#ftn11">
    <hi>NdE</hi> : le BBC Microcomputer System, ou BBC Micro, est une série de micro-ordinateurs
    construits par <ref target="#ftn11">(...)</ref></ref>
  <ref type="crossref" target="#ftn12"> En programmation, BASIC est un acronyme pour <hi>Beginner’s
      All-purpose Symbolic Instruction Code</hi>, qui <ref
      target="#ftn12">(...)</ref></ref>
  <ref type="crossref" target="#ftn13">
    <hi>Arts and Humanities Data Service</hi>, http://www.ahds.ac.uk/ <ref
      target="#ftn13">(...)</ref></ref>
  <p n="25">Les années 1980 ont notamment été marquées par une abondance de gadgets en tout genre.
    Ce phénomène s’est également appliqué aux ordinateurs, qui ont connu beaucoup d’évolutions d’un
    point de vue technologique. Le monde scientifique a voulu se rapprocher de ces évolutions,
    convaincu que ces machines offriraient de nouvelles possibilités, quant à savoir lesquelles...
    Entraînant la nécessité d’apprendre et de former. On s’est alors mis à équiper tous les lycées
    d’ordinateurs. En Angleterre, par exemple, des BBC Micro<ref
      target="#ftn11">11</ref> ont été mis à la disposition des
    établissements pour apprendre la programmation BASIC<ref
      target="#ftn12">12</ref> aux lycéens. À une autre échelle,
    le gouvernement a financé la <hi>Computers in Teaching Initiative</hi> qui visait à apprendre
    aux enseignants les possibilités technologiques des ordinateurs. C’est également à cette époque
    qu’a été fondé, dans le domaine de la recherche, l’<hi>Arts and Humanities Data Service</hi><ref
      target="#ftn13">13</ref> conçu comme une infrastructure
    pour les centres numériques et les petites équipes déjà existants et animés par une volonté de
    questionner ces nouvelles pratiques.</p>
  <p n="26">Voici quelques exemples des technologies créées à cette époque.</p>
  <ref type="crossref" target="#ftn14">
    <hi>Humanist Discussion Group</hi>, http://www.digitalhumanities.org/humanist/ <ref
      target="#ftn14">(...)</ref></ref>
  <p n="27">La plus grande avancée technologique, c’est évidemment l’<hi>e-mail</hi>. N’oublions pas
    qu’à cette époque, le Web n’existe pas encore. La création des <hi>e-mails</hi> permet le
    développement des listes de diffusion. C’est ainsi qu’est fondée en 1987 la liste
      <hi>Humanist</hi><ref target="#ftn14">14</ref>, dont le
    grand succès est dû à la propension du milieu scientifique à l’échange et au partage des idées.
    Du reste, on retrouve là le principe de la correspondance que s’échangeaient les savants du e
    siècle.</p>
  <ref type="crossref" target="#ftn15">
    <hi>The University of Oxford Text Archive</hi>, http://ota.ahds.ac.uk/ <ref
      target="#ftn15">(...)</ref></ref>
  <ref type="crossref" target="#ftn16">
    <hi>NdE</hi> : OCR soit <hi>optical character recognition</hi>, ou reconnaissance optique de
    caractères. Pour une déf <ref target="#ftn16"
    >(...)</ref></ref>
  <p n="28">En 1976, à une époque où le questionnement portait sur l’établissement, la diffusion et
    la publication des textes numériques, j’ai fondé l’<hi>Oxford Text Archive</hi><ref
      target="#ftn15">15</ref> sur une idée de partage des
    textes numérisés, de mutualisation des données numériques. De cette façon, le travail de
    numérisation fourni par l’un devenait profitable à l’autre, souhaitant travailler sur les mêmes
    données mais avec une approche différente. Nous disposions alors d’une des nouvelles machines
      OCR<ref target="#ftn16">16</ref>, qui pouvait reconnaître
    l’alphabet latin, mais également les lettres grecques, turques, russes, mais non les lettres
    arabes (lettres conjointes). Nous avons simplement fait le lien entre les possibilités offertes
    par la machine et le travail d’archive. On proposait ainsi de numériser des textes pour les
    chercheurs des autres universités, à condition que la version numérique du texte soit versée
    dans notre fond d’archive.</p>
  <ref type="crossref" target="#ftn17">
    <hi>Oxford English Dictionary, University of Waterloo</hi>,
    http://www.cs.uwaterloo.ca/~fwtompa/newoed-proje <ref
      target="#ftn17">(...)</ref></ref>
  <p n="29">À cette même période, des maisons d’édition se sont intéressées à la technologie du ,
    puisqu’elles y ont vu la possibilité de vendre de nouveau des livres, notamment des
    dictionnaires. En 1988, l’<hi>Oxford English Dictionary</hi> a organisé une grande conférence à
    l’université de Waterloo (Canada), qui a réuni pour la première fois des experts lexicographes
    et des représentants de maisons d’édition pour discuter notamment de la lexicographie assistée
    par ordinateur<ref target="#ftn17">17</ref>. Une tension a
    alors émergé entre les partisans de l’apparence du texte et les défenseurs des faits
    linguistiques.</p>
  <p n="30">Dans le même temps, les pouvoirs publics ont commencé à financer des infrastructures de
    support pour l’application des technologies, à l’échelle nationale comme au niveau européen. Le
    domaine privé s’est quant à lui intéressé à la publication et à la commercialisation de textes
    sur supports numériques (à ce propos, tous les débats suscités par la création de Kindle se
    tenaient déjà il y a plus de vingt ans).</p>
  <p n="31">De mon point de vue de non-expert, il me semble qu’une communauté s’établit de plusieurs
    manières. Dans le cas de l’<hi>humanities computing</hi>, cela passe par la création d’une liste
    de diffusion, reconnue des autres centres.</p>
  <p n="32">La dimension institutionnelle de l’<hi>humanities computing</hi> s’illustre par la
    création de formations universitaires (qui permettent de recruter de nouveaux chercheurs), et
    l’évolution du domaine des <hi>humanities</hi>
    <hi>computing </hi>se trouve bien résumée dans cette phrase : <hi>« I used to read texts, but
      now I’m learning the tools to play with them. »</hi></p></div><div>
  <head><ref target="#tocfrom2n9">Le retour de l’histoire dans
      le champ des études de computing</ref></head>
  <p n="33">Si j’ai mentionné les historiens tout à l’heure, c’est parce qu’ils n’ont pas connu le
    même sort en Europe qu’aux États-Unis. En France notamment, quelques grands historiens, au
    nombre desquels Jean-Philippe Genet, pensaient que la numérisation des textes pourrait permettre
    d’identifier des faits susceptibles de soutenir ou de réfuter une théorie ou une hypothèse
    historique.</p>
  <ref type="crossref" target="#ftn18">
    <hi>Kleio</hi>, http://www.hki.uni-koeln.de/kleio/old.website/tutorial/intro.htm <ref
      target="#ftn18">(...)</ref></ref>
  <p n="34">Du côté allemand, Manfred Thaller a développé un logiciel fonctionnant comme une base de
    données textuelle avant l’heure, <hi>Kleio</hi><ref
      target="#ftn18">18</ref> – que je ne suis jamais arrivé à
    faire fonctionner.</p></div><div>
  <head>Le grand débat mené sur la
      définition de l’<hi>humanities computing</hi></head>
  <p n="35">Est-ce que l’<hi>humanities computing</hi> fonctionne comme une communauté établie sans
    raison formelle, sans théorie ? Qu’est-ce qui distingue un texte numérisé d’un texte non
    numérisé ? Les possibilités intrinsèques de l’un et de l’autre sont-elles vraiment différentes ?
    Au final, l’<hi>humanities computing</hi>, ne serait-ce qu’un ensemble de méthodes et de
    pratiques, fonctionnant comme une ingénierie ? Quels sont les éléments qui assurent la cohésion
    du domaine ?</p>
  <p n="36">Pour répondre à cet ensemble de questions, nous avons commencé par nous interroger sur
    l’existence de <hi>scholarly primitives. </hi>On espérait ainsi lister les activités
    fondamentales de la science (la possibilité de rechercher des exemples, de faire des analyses de
    caractéristiques émergeant d’un ensemble de sources, de faire des liens des perceptions communes
    à deux auteurs, deux critiques, deux textes, etc.) pour voir s’il était possible de les adapter
    sur un support informatique. Notre questionnement portait sur l’existence d’outils et de
    méthodes capables d’optimiser les activités fondamentales de la science.</p>
  <p n="37">Pour traiter par ordinateur les ressources du monde réel (livres, objets, œuvres d’art,
    etc.), il faut les encoder, les transformer dans un autre format. De mon point de vue,
    l’utilisation de l’OCR, l’utilisation d’un appareil photo, le fait de taper à la machine sont
    des formes d’encodage. Parce que dans chacun des cas, on part de quelque chose qui existe, pour
    en produire une représentation numérique qui peut ainsi être traitée, analysée automatiquement
    et enrichie. On met ainsi en place un cercle herméneutique virtuel, qui favorise
    l’enrichissement des ressources.</p>
  <p n="38">Évidemment, l’encodage repose sur des modèles abstraits. Pour être plus concret, prenons
    l’exemple de la lecture. Lorsqu’on tient un livre, qu’on lit une page et que l’on identifie
    trois paragraphes sur cette page, cette identification est rendue possible parce qu’on dispose
    déjà d’un modèle abstrait qui correspond au concept de paragraphe. Un concept que l’on reconnaît
    en lisant la page en question. On peut donc créer un code qui signifie la présence des trois
    paragraphes. Peu importe la manière dont on s’exprime, ce qui compte, c’est le fait de
    reconnaître ces faits, à partir d’un modèle abstrait, dans lequel existent des choses telles
    qu’un paragraphe.</p>
  <p n="39">On comprend donc la nécessité de pouvoir transmettre ces correspondances, ces
    interprétations : le monde scientifique a besoin d’une continuité. Dans le cas de la
    numérisation des textes, il s’agit de pérenniser les bits dont est composé un texte numérisé,
    sans pour autant avoir besoin de savoir ce que ces bits représentent. Et au-delà des bits, il
    faut préserver des métadonnées, qui expriment la portée de l’encodage, c’est-à-dire la portée
    des choses dans ce modèle abstrait que j’ai présenté. Tel est l’enjeu fondamental de la <hi>Text
      Encoding Initiative</hi>.</p>
  <ref type="crossref" target="#ftn19"> Le terme de <hi>« link rot » </hi>s’emploie pour
      désigner<hi> </hi>la non-permanence des URL.</ref>
  <p n="40">Mais avant, je préfère évoquer l’ensemble des problèmes rencontrés avec des ressources
    numérisées : les ressources qui ne fonctionnent que sur une machine, la difficile localisation
    des données disponibles, la durée de vie des données, le <hi>link rot</hi><ref
      target="#ftn19">19</ref>.</p></div><div>
  <head>La <hi>Text Encoding
        Initiative</hi> (TEI)</head>
  <ref type="crossref" target="#ftn20">
    <hi>TEI, Text Encoding Initiative</hi>, http://www.tei-c.org/index.xml <ref
      target="#ftn20">(...)</ref></ref>
  <p n="41">Un mot maintenant sur la TEI<ref target="#ftn20"
      >20</ref> qui est, à mon avis, le projet le plus important en <hi>humanities computing</hi>. À
    l’origine, il s’agissait de rendre possible la mutualisation des données en apportant une
    solution à l’immense variété des manières d’encoder – une véritable « tour de Babel ». En dépit
    du fait que tous les scientifiques tombaient d’accord sur les modèles conceptuels de
    représentation, chacun avait sa manière d’encoder et ses propres outils.</p>
  <p n="42">La TEI a donc pris la forme d’un modèle unique, encyclopédique, de représentation des
    éléments signifiants d’un texte. Ce modèle a fait consensus car il a semblé évident à toute la
    communauté des chercheurs qu’il était nécessaire de marquer les paragraphes, les noms de
    personne – encore faut-il s’accorder sur ce que c’est qu’une personne : si saint Paul est une
    personne, peut-on en dire autant de Dieu ? – et d’une manière générale, de marquer tous les
    éléments devant être distingués dans un texte. La TEI a évolué pour adopter une architecture
    plastique, qui peut s’adapter aux nouveaux besoins et enjeux de la recherche.</p>
  <ref type="crossref" target="#ftn21">
    <hi>Vindolanda Tablets Online</hi>, http://vindolanda.csad.ox.ac.uk/ <ref
      target="#ftn21">(...)</ref></ref>
  <p n="43">Pour illustrer mon propos, prenons l’exemple des tablettes de Vindolanda, ces tablettes
    de cire retrouvées dans les années 1980, lors de la fouille d’un fort romain du mur d’Hadrien,
    au nord de l’Angleterre<ref target="#ftn21">21</ref>. Le
    principe est simple : il s’agissait d’écrire son texte sur la cire, puis une fois que le message
    était parvenu au destinataire, la cire pouvait être fondue pour écrire un autre message.
    Toutefois, des traces de texte subsistent sur le support en bois. On s’est alors interrogé sur
    la manière de transcrire ces écritures.</p>
  <ref type="crossref" target="#ftn22">
    <hi>NdE</hi> : cet exemple concerne encore les tablettes Vindolanda.</ref>
  <p n="44">Ici, par exemple<ref target="#ftn22">22</ref>, le
    mot <hi>salutem</hi>. Les lettres <hi>s </hi>et <hi>a</hi> sont absentes, le <hi>l</hi> est
    présent, le <hi>u </hi>ajouté, la séquence <hi>t-e</hi> incertaine tandis que le <hi>m</hi> est
    visible. Dans la TEI, on a recours à un langage à balises pour représenter des formes de texte
    complexes comme celle-ci. Il s’agit d’ailleurs d’un projet collaboratif : on commence par se
    mettre d’accord sur la nature des faits à représenter, puis on cherche un moyen d’exprimer ce
    consensus ; tel est l’enjeu du balisage. Dans le cas des tablettes de Vindolanda, les
    métadonnées peuvent ajouter des précisions sur le lieu et les conditions de découverte, etc.</p></div></div><div>
  <head>Les <hi>digital
        humanities</hi> (1994-…)</head>
  <p n="45">En 1994, en pleine période de discussions théoriques sur l’encodage, le Web est arrivé,
    entraînant un développement exponentiel des capacités informatiques et générant une explosion du
    nombre de bibliothèques numériques et de projets de numérisation en masse. À tel point que
    l’informatique compliquée des années 1980 devient, dans les années 1990, un outil pratique et
    commode.</p>
  <ref type="crossref" target="#ftn23">
    <hi>Grid Computing</hi>, http://en.wikipedia.org/wiki/Grid_computing, consulté le 17 août 2012.
      <ref target="#ftn23">(...)</ref></ref>
  <p n="46">C’est également l’époque de l’émergence du <hi>grid computing</hi><ref
      target="#ftn23">23</ref><hi>,</hi> qui favorise le partage
    et la mise à disposition des informations, où que l’on se trouve – l’enjeu étant de pouvoir
    centraliser tous les processus dans une même machine, quelle qu’elle soit.</p>
  <ref type="crossref" target="#ftn24">
    <hi>Folksonomie</hi>, http://fr.wikipedia.org/wiki/Folksonomie, consulté le 17 août 2012. <ref
      target="#ftn24">(...)</ref></ref>
  <p n="47">La part d’implication des utilisateurs augmente également ; c’est l’émergence de la
      <hi>folksonomy</hi><ref target="#ftn24">24</ref><hi>.
    </hi>À la différence d’une taxinomie, qui repose sur un principe de classement artificiel,
    consensuel et officiel des données, la <hi>folksonomy </hi>consiste en une classification portée
    par le plus grand nombre et effectuée sans médiation scientifique.</p>
  <ref type="crossref" target="#ftn25"> « Galerie de photos de <hi>The Library of Gongress</hi> »,
      <hi>Flickr</hi>, http://www.flickr.com/photos/library_of_c <ref
      target="#ftn25">(...)</ref></ref>
  <p n="48">La <hi>Library of Congress</hi> a lancé une expérience – et publié un rapport très
    intéressant à ce sujet –, en mettant en ligne un grand nombre de photos historiques sur Flickr,
    laissant ainsi la possibilité aux centaines de milliers d’utilisateurs de parcourir le fonds et
    d’en identifier les contenus<ref target="#ftn25">25</ref>.
    Tous ces gens, reconnaissant ici un quartier de leur enfance, là un personnage, ont ainsi
    contribué à l’indexation précise du fond ; c’est ce qu’on appelle une <hi>folksonomy.</hi></p>
  <p n="49">C’est dans ce contexte-là que le <hi>cloud computing</hi> voit le jour : plutôt que de
    stocker mes photos sur mon ordinateur par exemple – et pour ne pas risquer de les perdre –, je
    les mets en ligne sur des serveurs distants.</p>
  <p n="50">Il y aurait toute une histoire à réaliser sur l’édition scientifique, mais je n’en ai
    pas le temps.</p>
  <ref type="crossref" target="#ftn26">
    <hi>TGE-ADONIS</hi>, http://www.tge-adonis.fr/ <ref
      target="#ftn26">(...)</ref></ref>
  <ref type="crossref" target="#ftn27">
    <hi>Europeana</hi>, http://www.europeana.eu/portal/ <ref
      target="#ftn27">(...)</ref></ref>
  <p n="51">Un mot toutefois sur l’émergence d’infrastructures telles qu’ADONIS<ref
      target="#ftn26">26</ref> qui illustre l’intégration des
    projets numériques aux politiques de conservation du patrimoine, portées aussi bien au niveau
    national qu’à l’échelle européenne. La bibliothèque numérique <hi>Europeana</hi><ref
      target="#ftn27">27</ref>, financée par les États membres
    du l’Union européenne, en est un autre exemple. De même qu’en France, le projet
      <hi>Gallica,</hi> et en Angleterre, le programme élaboré par le <hi>Joint Information Systems
      Committee</hi> (JISC). On voit comment les ressources sont à présent mises à la disposition du
    grand public et non plus seulement accessibles par les scientifiques.</p><div>
  <head>La distinction entre les
      textes numériques et les textes imprimés</head>
  <p n="52">Lorsque je remets à quelqu’un un livre imprimé, la personne en question le possède alors
    que moi, je ne l’ai plus. En revanche, si je donne une copie d’un texte numérisé, nous sommes à
    présent deux à l’avoir. C’est une différence évidente, mais non moins fondamentale, dans la
    mesure où elle rend les modèles commerciaux de distribution des textes numériques difficiles à
    concevoir. Qu’est-ce qu’on achète vraiment quand on achète un contenu numérique sur iTunes, par
    exemple ? Achète-t-on la permission d’écouter une chanson ? La question de savoir si cela
    correspond aux possibilités offertes par les ressources numériques reste entière.</p></div><div>
  <head>Le manifeste <hi>d’Open
        Source Development</hi><ref target="#ftn28"
      >28</ref></head>
  <ref type="crossref" target="#ftn28">
    <hi>Open Source Software Development</hi>,
    http://en.wikipedia.org/wiki/Open_source_software_development <ref
      target="#ftn28">(...)</ref></ref>
  <p n="53">L<hi>’open source</hi>, repose sur un principe de cession des droits d’auteur sur un
    logiciel. En effet, il est difficile pour un développeur de revendiquer la paternité de telle ou
    telle partie d’un logiciel, dans la mesure où le travail de l’un s’effectue dans la continuité
    des prédécesseurs et prépare le travail des suivants. Dans ce cas, autant céder ses droits sur
    le logiciel, et viser la rentabilité sur les activités de support, d’application. De plus, la
    mise à disposition gratuite du logiciel permet à chacun de le modifier, de l’améliorer.</p>
  <p n="54">Les logiciels <hi>open source </hi>font naturellement débat dans l’industrie
    informatique, d’autant que la plupart des logiciels que l’on utilise aujourd’hui sont <hi>open
      source</hi>.</p>
  <p n="55">On peut alors se demander s’il ne serait pas possible de transposer ce modèle
    opérationnel en informatique, dans le domaine des livres. Ne pourrait-on pas l’appliquer aux
    ressources numériques ? C’est dans cette optique que certains projets ont vu le jour. Cet état
    d’esprit est ainsi à l’origine du <hi>Digital Humanities Manifesto 2.0 </hi>rédigé par une
    équipe de l’université de Californie (UCLA) et qui comporte des idées très intéressantes. Les
      <hi>digital humanities</hi> n’y sont pas définies comme un champ unifié mais comme un ensemble
    de pratiques qui convergent vers l’exploration d’un univers où l’imprimé n’est plus le seul
    moyen de diffusion de la connaissance, de la science. Il se trouve absorbé par de nouvelles
    configurations multimédia. D’autre part, les outils numériques ont modifié la façon de produire
    et de diffuser les connaissances en sciences humaines et sociales.</p>
  <p n="56">Deuxième élément important, l’économie des <hi>digital humanities</hi> est basée sur
    l’abondance et non sur la rareté.</p></div><div>
  <head><ref target="#tocfrom2n15"><hi>Quid </hi>des millions de
      livres numérisés par Google ?</ref></head>
  <p n="57">En réponse à la question du traitement de la profusion des ressources numériques, John
    Unsworth développe une théorie fondée sur la non-lecture<hi>, </hi>selon laquelle dans toutes
    les situations de lecture, nous passons par des moments de non-lecture, d’absence de lecture. Il
    faut ainsi créer des outils qui permettent de balayer des millions de pages, d’entrer dans ces
    fonds considérables pour les décomposer et les recomposer à l’envi, d’en extraire les
    informations pertinentes en fonction des besoins, et d’en proposer la visualisation – le
      <hi>data mining</hi>. C’est ce que nous avions déjà mis en œuvre avec le <hi>Brown Corpus</hi>
    cité précédemment, en développant des outils statistiques favorisant les <hi>pattern
      recognition</hi>.</p></div><div>
  <head>L’évolution de ces trente
      dernières années</head>
  <p n="58">L’environnement de travail est passé du <hi>mainframe</hi> au <hi>network</hi>. Par
      ailleurs,<hi> </hi>l’outillage a évolué de la concordance vers le concept d’<hi>e-text</hi>,
    et donc finalement de ressource digitale, qui est beaucoup plus qu’un texte.</p>
  <p n="59">Les problématiques liées aux travaux statistiques ont cédé la place aux questions
    d’analyse textuelle ou de description des ressources, afin d’être en mesure de les assembler
    entre elles. Les modalités de communication ont également évolué puisque les <hi>e-mails</hi>
    ont été relayés par des listes de diffusion, puis des sites internet et des blogs.</p></div><div>
  <head>CLAROS<ref
      target="#ftn29">29</ref>, un grand projet européen de
      mutualisation de données numériques</head>
  <p>
    <hi>CLAROS</hi>, http://explore.clarosnet.org/XDB/ASP/clarosHome/ <ref
      target="#ftn29">(...)</ref>
  <ref type="crossref" target="#ftn30">
    <hi>Lexicon of Greek Personal Names</hi>, http://www.lgpn.ox.ac.uk/ <ref
      target="#ftn30">(...)</ref></ref>
  <ref type="crossref" target="#ftn31">
    <hi>Arachne</hi>, http://arachne.uni-koeln.de/drupal/ <ref
      target="#ftn31">(...)</ref></ref>
  <ref type="crossref" target="#ftn32">
    <hi>Lexicon Iconographicum Mythologiae Classicae</hi>, http://www.limc-france.fr/ <ref
      target="#ftn32">(...)</ref></ref></p>
  <p n="60">L’université d’Oxford a développé le <hi>Lexicon of Greek Personal Names</hi><ref
      target="#ftn30">30</ref>, qui regroupe tous les noms
    attestés par les inscriptions antiques. L’université de Cologne a, quant à elle, fondé
      l’<hi>Arachne Archive</hi><ref target="#ftn31">31</ref>,
    une grande base de données consacrée à la sculpture. L’université de Paris 10, enfin, a mis au
    point un dictionnaire d’iconographie classique : <hi>Lexicon Iconographicum Mythologiae
      Classicae</hi><ref target="#ftn32">32</ref><hi>.</hi>
    Toutes ces données ont donc été versées dans CLAROS, une base commune constituant un fonds de
    deux millions d’enregistrements traitant des mêmes objets. L’intérêt de cette démarche réside
    précisément dans la mutualisation de ces projets isolés, de façon à permettre aux antiquisants
    de brasser l’ensemble des données, d’étudier les objets selon les perspectives personnelles de
    chacun, en naviguant sans difficulté d’un document à l’autre.</p>
  <p n="61">Tous ces projets reposent sur le même standard de représentation : XML TEI. C’est-à-dire
    qu’ils partagent tous un même modèle conceptuel, ayant recours à des technologies comme
      l’<hi>Unicode</hi>, le <hi>Javascript</hi>, et partagent une même volonté politique
    d’ouverture, de mutualisation et d’enrichissement des données.</p>
  <p n="62">De cette manière, on peut repenser les caractéristiques d’une édition numérique.</p>
  <p n="63">En conclusion, j’insisterai sur le fait que la spécificité des sciences humaines et
    sociales consiste en l’étude du texte, de l’objet textuel. Nous travaillons sur le texte, qui
    représente un discours, raconte une histoire, et tâchons d’expliquer ces histoires, ces contes,
    ces représentations. Nous sommes des experts de la maïeutique du texte, et c’est précisément ce
    qui définit la contribution des sciences humaines et sociales à l’élaboration du Web
    sémantique.</p></div></div></body>
  <back><div> <head>Notes</head>
    <note xml:id="ftn1">
    <hi>Corpus Thomisticum</hi>, <ref target="http://www.corpusthomisticum.org/"
      >http://www.corpusthomisticum.org/</ref></note>
  <note xml:id="ftn2">
    <hi>Brown Corpus Manual</hi>, <ref target="http://icame.uib.no/brown/bcm.html"
      >http://icame.uib.no/brown/bcm.html</ref></note>
  <note xml:id="ftn3">
    <hi>Thesaurus Linguae Graecae</hi>, <ref target="http://www.tlg.uci.edu/"
      >http://www.tlg.uci.edu/</ref></note>
  <note xml:id="ftn4">
    <hi>TLG Beta Code Quick Refrence Guide</hi>, <ref
      target="http://www.tlg.uci.edu/encoding/quickbeta.pdf"
      >http://www.tlg.uci.edu/encoding/quickbeta.pdf</ref>, consulté le 17 août 2012.</note>
  <note xml:id="ftn5">
    <hi>ASCII Table and Description</hi>, <ref target="http://www.asciitable.com/"
      >http://www.asciitable.com/</ref></note>
  <note xml:id="ftn6"> La concordance est un procédé philologique hérité du Moyen Âge qui permet
    d’identifier toutes les façons d’utiliser un même mot dans un corpus donné.</note>
  <note xml:id="ftn7">
    <ref
      target="http://books.openedition.org/H:/AppData/Roaming/Microsoft/Word/CETEDOC,%20http:/bcs.fltr.ucl.ac.be/dicland.html"
    /></note>
 
  
  <note xml:id="ftn9">
    <hi/><ref target="http://llc.oxfordjournals.org/"/></note>
  
  <note xml:id="ftn10">
    <hi>Textométrie</hi>, <ref target="http://textometrie.ens-lyon.fr/"
      >http://textometrie.ens-lyon.fr/</ref></note>
  <note xml:id="ftn11">
    <hi>NdE</hi> : le BBC Microcomputer System, ou BBC Micro, est une série de micro-ordinateurs
    construits par Acorn Computers Ltd pour le <hi>BBC Computer Literacy Project</hi> initié par la
      <hi>British Broadcasting Corporation</hi>. Créés pour une utilisation pédagogique, les
    ordinateurs de la gamme BBC Micro sont réputés pour leur modularité et la qualité de leurs
    systèmes d’exploitation. (Source : Wikipedia, <ref
      target="http://fr.wikipedia.org/wiki/BBC_Micro">http://fr.wikipedia.org/wiki/BBC_Micro</ref>,
    consulté le 17 août 2012).</note>
  <note xml:id="ftn12"> En <ref target="http://fr.wikipedia.org/wiki/Programmation"
      >programmation</ref>, BASIC est un <ref target="http://fr.wikipedia.org/wiki/Acronymie"
      >acronyme</ref> pour <hi>Beginner’s All-purpose Symbolic Instruction Code</hi>, qui désigne
    une famille de <ref target="http://fr.wikipedia.org/wiki/Langage_informatique">langages de
      programmations</ref> de <ref target="http://fr.wikipedia.org/wiki/Langage_de_haut_niveau">haut
      niveau</ref>. (Source : Wikipedia, <ref target="http://fr.wikipedia.org/wiki/BASIC"
      >http://fr.wikipedia.org/wiki/BASIC</ref>, consulté le 17 août 2012).</note>
  <note xml:id="ftn13">
    <hi>Arts and Humanities Data Service</hi>, <ref target="http://www.ahds.ac.uk/"
      >http://www.ahds.ac.uk/</ref></note>
  <note xml:id="ftn14">
    <hi>Humanist Discussion Group</hi>, <ref target="http://www.digitalhumanities.org/humanist/"
      >http://www.digitalhumanities.org/humanist/</ref></note>
  <note xml:id="ftn15">
    <hi>The University of Oxford Text Archive</hi>, <ref target="http://ota.ahds.ac.uk/"
      >http://ota.ahds.ac.uk/</ref></note>
  <note xml:id="ftn16">
    <hi>NdE</hi> : OCR soit <hi>optical character recognition</hi>, ou reconnaissance optique de
    caractères. Pour une définition, consulter la page dédiée sur Wikipédia, <ref
      target="http://fr.wikipedia.org/wiki/Reconnaissance_optique_de_caract%C2%A0%C3%C2%A0%A8res"
      >http://fr.wikipedia.org/wiki/Reconnaissance_optique_de_caract %C3 %A8res</ref></note>
  <note xml:id="ftn17">
    <hi>Oxford English Dictionary, University of Waterloo</hi>, <ref
      target="http://www.cs.uwaterloo.ca/~fwtompa/newoed-project.html"
      >http://www.cs.uwaterloo.ca/~fwtompa/newoed-project.html</ref></note>
  <note xml:id="ftn18">
    <hi>Kleio</hi>, <ref target="http://www.hki.uni-koeln.de/kleio/old.website/tutorial/intro.htm"
      >http://www.hki.uni-koeln.de/kleio/old.website/tutorial/intro.htm</ref></note>
  <note xml:id="ftn19"> Le terme de <hi>« link rot » </hi>s’emploie pour désigner<hi> </hi>la
    non-permanence des URL.</note>
  <note xml:id="ftn20">
    <hi>TEI, Text Encoding Initiative</hi>, <ref target="http://www.tei-c.org/index.xml"
      >http://www.tei-c.org/index.xml</ref></note>
  <note xml:id="ftn21">
    <hi>Vindolanda Tablets Online</hi>, <ref target="http://vindolanda.csad.ox.ac.uk/"
      >http://vindolanda.csad.ox.ac.uk/</ref></note>
  <note xml:id="ftn22">
    <hi>NdE</hi> : cet exemple concerne encore les tablettes Vindolanda.</note>
  <note xml:id="ftn23">
    <hi>Grid Computing</hi>, <ref target="http://en.wikipedia.org/wiki/Grid_computing"
      >http://en.wikipedia.org/wiki/Grid_computing</ref>, consulté le 17 août 2012.</note>
  <note xml:id="ftn24">
    <hi>Folksonomie</hi>, <ref target="http://fr.wikipedia.org/wiki/Folksonomie"
      >http://fr.wikipedia.org/wiki/Folksonomie</ref>, consulté le 17 août 2012.</note>
  <note xml:id="ftn25"> « Galerie de photos de <hi>The Library of Gongress</hi> », <hi>Flickr</hi>,
      <ref target="http://www.flickr.com/photos/library_of_congress/"
      >http://www.flickr.com/photos/library_of_congress/</ref></note>
  <note xml:id="ftn26">
    <hi>TGE-ADONIS</hi>, <ref target="http://www.tge-adonis.fr/"
    >http://www.tge-adonis.fr/</ref></note>
  <note xml:id="ftn27">
    <hi>Europeana</hi>, <ref target="http://www.europeana.eu/portal/"
      >http://www.europeana.eu/portal/</ref></note>
  <note xml:id="ftn28">
    <hi>Open Source Software Development</hi>, <ref
      target="http://en.wikipedia.org/wiki/Open_source_software_development"
      >http://en.wikipedia.org/wiki/Open_source_software_development</ref></note>
  <note xml:id="ftn29">
    <hi>CLAROS</hi>, <ref target="http://explore.clarosnet.org/XDB/ASP/clarosHome/"
      >http://explore.clarosnet.org/XDB/ASP/clarosHome/</ref></note>
  <note xml:id="ftn30">
    <hi>Lexicon of Greek Personal Names</hi>, <ref target="http://www.lgpn.ox.ac.uk/"
      >http://www.lgpn.ox.ac.uk/</ref></note>
  <note xml:id="ftn31">
    <hi>Arachne</hi>, <ref target="http://arachne.uni-koeln.de/drupal/"
      >http://arachne.uni-koeln.de/drupal/</ref></note>
  <note xml:id="ftn32">
    <hi>Lexicon Iconographicum Mythologiae Classicae</hi>, <ref target="http://www.limc-france.fr/"
      >http://www.limc-france.fr/</ref></note></div><div type="bibliography"><listBibl><bibl/></listBibl></div>
     <div>
  <head>Auteur</head> <p>Lou Burnard L’un des pères fondateurs de la Text Encoding Initiative (TEI), du
  Arts and Humanities Data Service et du British National Corpus. Actuellement chargé de mission
  auprès du TGE Adonis.</p> </div></back>
</text>

</TEI>
